{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tX)\n",
    "print(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(tX!=-999.00,tX,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_=np.eye(5)\n",
    "id_[:,np.where(id_[:,2]!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a,b)=np.shape(tX)\n",
    "for i in range(b):\n",
    "    ind=np.where(tX[:,i]==-999)\n",
    "    tX_del=np.delete(tX[:,i],ind)\n",
    "    mean=np.mean(tX_del)\n",
    "    tX[ind,i]=mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38470000e+02  5.16550000e+01  9.78270000e+01 ...  1.24000000e+00\n",
      "  -2.47500000e+00  1.13497000e+02]\n",
      " [ 1.60937000e+02  6.87680000e+01  1.03235000e+02 ... -1.18452642e-02\n",
      "  -1.58228913e-03  4.62260000e+01]\n",
      " [ 1.21858528e+02  1.62172000e+02  1.25953000e+02 ... -1.18452642e-02\n",
      "  -1.58228913e-03  4.42510000e+01]\n",
      " ...\n",
      " [ 1.05457000e+02  6.05260000e+01  7.58390000e+01 ... -1.18452642e-02\n",
      "  -1.58228913e-03  4.19920000e+01]\n",
      " [ 9.49510000e+01  1.93620000e+01  6.88120000e+01 ... -1.18452642e-02\n",
      "  -1.58228913e-03  0.00000000e+00]\n",
      " [ 1.21858528e+02  7.27560000e+01  7.08310000e+01 ... -1.18452642e-02\n",
      "  -1.58228913e-03  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(0/1499): loss=1162110.2029357175, w0=0.9431245479744506, w1=0.9782839864310771\n",
      "(250000,)\n",
      "Gradient Descent(1/1499): loss=957402.4009938582, w0=0.8914339779630271, w1=0.9585420641502702\n",
      "(250000,)\n",
      "Gradient Descent(2/1499): loss=789651.0723763205, w0=0.8444363967112438, w1=0.9405866078679731\n",
      "(250000,)\n",
      "Gradient Descent(3/1499): loss=652166.79535596, w0=0.8016866619910006, w1=0.9242478292067627\n",
      "(250000,)\n",
      "Gradient Descent(4/1499): loss=539471.3858074611, w0=0.762781938694116, w1=0.909372081165042\n",
      "(250000,)\n",
      "Gradient Descent(5/1499): loss=447078.5720029288, w0=0.72735767734536, w1=0.8958203237520423\n",
      "(250000,)\n",
      "Gradient Descent(6/1499): loss=371314.3840026659, w0=0.6950839748814682, w1=0.8834667354738698\n",
      "(250000,)\n",
      "Gradient Descent(7/1499): loss=309170.066264235, w0=0.6656622813594514, w1=0.8721974568065656\n",
      "(250000,)\n",
      "Gradient Descent(8/1499): loss=258181.62428185472, w0=0.6388224197115361, w1=0.8619094531100132\n",
      "(250000,)\n",
      "Gradient Descent(9/1499): loss=216331.18246136885, w0=0.6143198887897693, w1=0.852509485629113\n",
      "(250000,)\n",
      "Gradient Descent(10/1499): loss=181966.20373053238, w0=0.5919334227718976, w1=0.8439131803078712\n",
      "(250000,)\n",
      "Gradient Descent(11/1499): loss=153733.33654581133, w0=0.5714627825598406, w1=0.8360441851186897\n",
      "(250000,)\n",
      "Gradient Descent(12/1499): loss=130524.24061943628, w0=0.5527267571184643, w1=0.8288334074929481\n",
      "(250000,)\n",
      "Gradient Descent(13/1499): loss=111431.22230331908, w0=0.5355613547985691, w1=0.8222183242387604\n",
      "(250000,)\n",
      "Gradient Descent(14/1499): loss=95710.90333270749, w0=0.5198181665849498, w1=0.8161423570555594\n",
      "(250000,)\n",
      "Gradient Descent(15/1499): loss=82754.46827800592, w0=0.5053628849270164, w1=0.8105543074101278\n",
      "(250000,)\n",
      "Gradient Descent(16/1499): loss=72063.29945630865, w0=0.492073963362921, w1=0.805407845131409\n",
      "(250000,)\n",
      "Gradient Descent(17/1499): loss=63229.023761210476, w0=0.47984140355392174, w1=0.8006610456177976\n",
      "(250000,)\n",
      "Gradient Descent(18/1499): loss=55917.1725170202, w0=0.468565657617879, w1=0.7962759710359967\n",
      "(250000,)\n",
      "Gradient Descent(19/1499): loss=49853.80012434947, w0=0.4581566348020089, w1=0.7922182913297718\n",
      "(250000,)\n",
      "Gradient Descent(20/1499): loss=44814.52573022614, w0=0.44853280257682504, w1=0.7884569412544286\n",
      "(250000,)\n",
      "Gradient Descent(21/1499): loss=40615.55917062415, w0=0.43962037317596936, w1=0.7849638100125449\n",
      "(250000,)\n",
      "Gradient Descent(22/1499): loss=37106.35188095971, w0=0.43135256745979017, w1=0.78171346039201\n",
      "(250000,)\n",
      "Gradient Descent(23/1499): loss=34163.57853166385, w0=0.42366894875258476, w1=0.7786828746019916\n",
      "(250000,)\n",
      "Gradient Descent(24/1499): loss=31686.208426384033, w0=0.4165148200020921, w1=0.7758512242690303\n",
      "(250000,)\n",
      "Gradient Descent(25/1499): loss=29591.46933297902, w0=0.4098406782420798, w1=0.7731996622966893\n",
      "(250000,)\n",
      "Gradient Descent(26/1499): loss=27811.54214924406, w0=0.40360172091102603, w1=0.7707111345104942\n",
      "(250000,)\n",
      "Gradient Descent(27/1499): loss=26290.85406689424, w0=0.3977573990976666, w1=0.768370209207447\n",
      "(250000,)\n",
      "Gradient Descent(28/1499): loss=24983.86186034037, w0=0.3922710132527316, w1=0.7661629229081738\n",
      "(250000,)\n",
      "Gradient Descent(29/1499): loss=23853.236550662463, w0=0.38710934733020935, w1=0.7640766407715431\n",
      "(250000,)\n",
      "Gradient Descent(30/1499): loss=22868.376765635905, w0=0.3822423377051847, w1=0.7620999302779962\n",
      "(250000,)\n",
      "Gradient Descent(31/1499): loss=22004.19127713614, w0=0.3776427735625336, w1=0.7602224469203134\n",
      "(250000,)\n",
      "Gradient Descent(32/1499): loss=21240.101974666934, w0=0.37328602576498554, w1=0.7584348307604354\n",
      "(250000,)\n",
      "Gradient Descent(33/1499): loss=20559.227359640594, w0=0.36914980149342197, w1=0.7567286128194504\n",
      "(250000,)\n",
      "Gradient Descent(34/1499): loss=19947.71387276202, w0=0.3652139222096113, w1=0.7550961303660428\n",
      "(250000,)\n",
      "Gradient Descent(35/1499): loss=19394.188285821885, w0=0.36146012272444616, w1=0.7535304502575478\n",
      "(250000,)\n",
      "Gradient Descent(36/1499): loss=18889.30923637506, w0=0.3578718693654835, w1=0.7520252995681593\n",
      "(250000,)\n",
      "Gradient Descent(37/1499): loss=18425.399953242602, w0=0.35443419542828875, w1=0.7505750028116005\n",
      "(250000,)\n",
      "Gradient Descent(38/1499): loss=17996.14747146227, w0=0.35113355226865944, w1=0.7491744251314104\n",
      "(250000,)\n",
      "Gradient Descent(39/1499): loss=17596.35629737649, w0=0.34795767454897336, w1=0.747818920891585\n",
      "(250000,)\n",
      "Gradient Descent(40/1499): loss=17221.74666457369, w0=0.34489545829323154, w1=0.746504287154234\n",
      "(250000,)\n",
      "Gradient Descent(41/1499): loss=16868.789306674746, w0=0.3419368505332575, w1=0.7452267215797101\n",
      "(250000,)\n",
      "Gradient Descent(42/1499): loss=16534.570134960897, w0=0.3390727494442486, w1=0.743982784328824\n",
      "(250000,)\n",
      "Gradient Descent(43/1499): loss=16216.679406109432, w0=0.3362949139726077, w1=0.7427693635867211\n",
      "(250000,)\n",
      "Gradient Descent(44/1499): loss=15913.120945776625, w0=0.3335958820537625, w1=0.7415836443641552\n",
      "(250000,)\n",
      "Gradient Descent(45/1499): loss=15622.237796699821, w0=0.3309688966034467, w1=0.7404230802646211\n",
      "(250000,)\n",
      "Gradient Descent(46/1499): loss=15342.651317531196, w0=0.3284078385435333, w1=0.7392853679354184\n",
      "(250000,)\n",
      "Gradient Descent(47/1499): loss=15073.211297091219, w0=0.3259071661937487, w1=0.7381684239475228\n",
      "(250000,)\n",
      "Gradient Descent(48/1499): loss=14812.95508970065, w0=0.3234618604241571, w1=0.7370703638733868\n",
      "(250000,)\n",
      "Gradient Descent(49/1499): loss=14561.074138371008, w0=0.3210673750208228, w1=0.7359894833537415\n",
      "(250000,)\n",
      "Gradient Descent(50/1499): loss=14316.88654836443, w0=0.3187195917691127, w1=0.7349242409643311\n",
      "(250000,)\n",
      "Gradient Descent(51/1499): loss=14079.814615815174, w0=0.3164147798062025, w1=0.7338732427114814\n",
      "(250000,)\n",
      "Gradient Descent(52/1499): loss=13849.366414432854, w0=0.31414955883697726, w1=0.7328352280016702\n",
      "(250000,)\n",
      "Gradient Descent(53/1499): loss=13625.12070572329, w0=0.3119208658460932, w1=0.731809056944985\n",
      "(250000,)\n",
      "Gradient Descent(54/1499): loss=13406.714571168899, w0=0.3097259249738725, w1=0.7307936988656685\n",
      "(250000,)\n",
      "Gradient Descent(55/1499): loss=13193.833273732684, w0=0.30756222025529556, w1=0.7297882219050121\n",
      "(250000,)\n",
      "Gradient Descent(56/1499): loss=12986.201945248737, w0=0.30542747094993894, w1=0.7287917836127588\n",
      "(250000,)\n",
      "Gradient Descent(57/1499): loss=12783.578769309406, w0=0.30331960921657963, w1=0.7278036224330475\n",
      "(250000,)\n",
      "Gradient Descent(58/1499): loss=12585.749389079658, w0=0.3012367599095945, w1=0.7268230499998695\n",
      "(250000,)\n",
      "Gradient Descent(59/1499): loss=12392.52231845734, w0=0.2991772222954698, w1=0.7258494441650799\n",
      "(250000,)\n",
      "Gradient Descent(60/1499): loss=12203.725175116037, w0=0.29713945350690724, w1=0.7248822426893331\n",
      "(250000,)\n",
      "Gradient Descent(61/1499): loss=12019.20158682067, w0=0.2951220535693613, w1=0.7239209375329213\n",
      "(250000,)\n",
      "Gradient Descent(62/1499): loss=11838.808649310778, w0=0.29312375185054285, w1=0.7229650696894913\n",
      "(250000,)\n",
      "Gradient Descent(63/1499): loss=11662.414836079264, w0=0.29114339479763224, w1=0.7220142245110327\n",
      "(250000,)\n",
      "Gradient Descent(64/1499): loss=11489.89827841805, w0=0.28917993483980026, w1=0.7210680274774376\n",
      "(250000,)\n",
      "Gradient Descent(65/1499): loss=11321.14534887821, w0=0.2872324203452723, w1=0.7201261403683692\n",
      "(250000,)\n",
      "Gradient Descent(66/1499): loss=11156.049493393111, w0=0.28529998653269834, w1=0.7191882577991966\n",
      "(250000,)\n",
      "Gradient Descent(67/1499): loss=10994.510267222682, w0=0.28338184724612053, w1=0.7182541040863856\n",
      "(250000,)\n",
      "Gradient Descent(68/1499): loss=10836.432537992458, w0=0.2814772875114518, w1=0.7173234304110279\n",
      "(250000,)\n",
      "Gradient Descent(69/1499): loss=10681.725825747008, w0=0.27958565680018177, w1=0.7163960122521656\n",
      "(250000,)\n",
      "Gradient Descent(70/1499): loss=10530.303755380035, w0=0.277706362933088, w1=0.7154716470642642\n",
      "(250000,)\n",
      "Gradient Descent(71/1499): loss=10382.083601260696, w0=0.2758388665631194, w1=0.714550152175623\n",
      "(250000,)\n",
      "Gradient Descent(72/1499): loss=10236.985907525863, w0=0.2739826761824019, w1=0.7136313628867205\n",
      "(250000,)\n",
      "Gradient Descent(73/1499): loss=10094.934170497487, w0=0.27213734360354896, w1=0.7127151307494879\n",
      "(250000,)\n",
      "Gradient Descent(74/1499): loss=9955.85457213228, w0=0.27030245987019474, w1=0.7118013220103093\n",
      "(250000,)\n",
      "Gradient Descent(75/1499): loss=9819.675755415929, w0=0.2684776515559538, w1=0.7108898162011844\n",
      "(250000,)\n",
      "Gradient Descent(76/1499): loss=9686.328634256015, w0=0.2666625774148879, w1=0.7099805048649671\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(77/1499): loss=9555.746231772502, w0=0.2648569253500709, w1=0.7090732904019349\n",
      "(250000,)\n",
      "Gradient Descent(78/1499): loss=9427.863541986046, w0=0.2630604096700176, w1=0.7081680850261518\n",
      "(250000,)\n",
      "Gradient Descent(79/1499): loss=9302.617410806324, w0=0.2612727686056175, w1=0.7072648098211886\n",
      "(250000,)\n",
      "Gradient Descent(80/1499): loss=9179.946432961291, w0=0.259493762062813, w1=0.706363393885752\n",
      "(250000,)\n",
      "Gradient Descent(81/1499): loss=9059.790862113343, w0=0.2577231695886177, w1=0.7054637735606766\n",
      "(250000,)\n",
      "Gradient Descent(82/1499): loss=8942.092531903942, w0=0.25596078853019777, w1=0.7045658917295414\n",
      "(250000,)\n",
      "Gradient Descent(83/1499): loss=8826.794786074086, w0=0.25420643236866836, w1=0.7036696971859131\n",
      "(250000,)\n",
      "Gradient Descent(84/1499): loss=8713.84241614056, w0=0.25245992921100024, w1=0.7027751440608792\n",
      "(250000,)\n",
      "Gradient Descent(85/1499): loss=8603.181605380201, w0=0.25072112042501077, w1=0.7018821913051392\n",
      "(250000,)\n",
      "Gradient Descent(86/1499): loss=8494.75987809748, w0=0.24898985940384077, w1=0.7009908022204661\n",
      "(250000,)\n",
      "Gradient Descent(87/1499): loss=8388.52605333357, w0=0.2472660104476125, w1=0.7001009440358431\n",
      "(250000,)\n",
      "Gradient Descent(88/1499): loss=8284.430202324642, w0=0.24554944775113255, w1=0.6992125875240277\n",
      "(250000,)\n",
      "Gradient Descent(89/1499): loss=8182.423609140009, w0=0.24384005448756266, w1=0.698325706654698\n",
      "(250000,)\n",
      "Gradient Descent(90/1499): loss=8082.458734031084, w0=0.24213772197893912, w1=0.6974402782807024\n",
      "(250000,)\n",
      "Gradient Descent(91/1499): loss=7984.489179104659, w0=0.24044234894528807, w1=0.6965562818542661\n",
      "(250000,)\n",
      "Gradient Descent(92/1499): loss=7888.469656001448, w0=0.23875384082486853, w1=0.6956736991703013\n",
      "(250000,)\n",
      "Gradient Descent(93/1499): loss=7794.355955316246, w0=0.23707210915878504, w1=0.6947925141342476\n",
      "(250000,)\n",
      "Gradient Descent(94/1499): loss=7702.1049175414255, w0=0.23539707103385402, w1=0.6939127125521062\n",
      "(250000,)\n",
      "Gradient Descent(95/1499): loss=7611.674405352732, w0=0.23372864857818898, w1=0.693034281940558\n",
      "(250000,)\n",
      "Gradient Descent(96/1499): loss=7523.02327708688, w0=0.2320667685044966, w1=0.6921572113552551\n",
      "(250000,)\n",
      "Gradient Descent(97/1499): loss=7436.1113612855215, w0=0.23041136169655055, w1=0.691281491235555\n",
      "(250000,)\n",
      "Gradient Descent(98/1499): loss=7350.899432200719, w0=0.2287623628347421, w1=0.6904071132641363\n",
      "(250000,)\n",
      "Gradient Descent(99/1499): loss=7267.349186173976, w0=0.22711971005699513, w1=0.6895340702400762\n",
      "(250000,)\n",
      "Gradient Descent(100/1499): loss=7185.423218814747, w0=0.22548334465168696, w1=0.6886623559641112\n",
      "(250000,)\n",
      "Gradient Descent(101/1499): loss=7105.085002915823, w0=0.22385321077953496, w1=0.6877919651349207\n",
      "(250000,)\n",
      "Gradient Descent(102/1499): loss=7026.298867052299, w0=0.2222292552216984, w1=0.686922893255384\n",
      "(250000,)\n",
      "Gradient Descent(103/1499): loss=6949.029974818699, w0=0.22061142715160592, w1=0.6860551365478618\n",
      "(250000,)\n",
      "Gradient Descent(104/1499): loss=6873.244304665065, w0=0.21899967792825603, w1=0.685188691877642\n",
      "(250000,)\n",
      "Gradient Descent(105/1499): loss=6798.908630298277, w0=0.21739396090895186, w1=0.6843235566837742\n",
      "(250000,)\n",
      "Gradient Descent(106/1499): loss=6725.990501619054, w0=0.21579423127962527, w1=0.6834597289165875\n",
      "(250000,)\n",
      "Gradient Descent(107/1499): loss=6654.458226168842, w0=0.21420044590108098, w1=0.6825972069812549\n",
      "(250000,)\n",
      "Gradient Descent(108/1499): loss=6584.280851063687, w0=0.21261256316964944, w1=0.6817359896868305\n",
      "(250000,)\n",
      "Gradient Descent(109/1499): loss=6515.428145394761, w0=0.2110305428908814, w1=0.6808760762002342\n",
      "(250000,)\n",
      "Gradient Descent(110/1499): loss=6447.870583077282, w0=0.2094543461650469, w1=0.6800174660047164\n",
      "(250000,)\n",
      "Gradient Descent(111/1499): loss=6381.579326131254, w0=0.2078839352833186, w1=0.6791601588623725\n",
      "(250000,)\n",
      "Gradient Descent(112/1499): loss=6316.526208379007, w0=0.2063192736336265, w1=0.6783041547803225\n",
      "(250000,)\n",
      "Gradient Descent(113/1499): loss=6252.683719545681, w0=0.20476032561526653, w1=0.6774494539802053\n",
      "(250000,)\n",
      "Gradient Descent(114/1499): loss=6190.024989749892, w0=0.20320705656143362, w1=0.6765960568706724\n",
      "(250000,)\n",
      "Gradient Descent(115/1499): loss=6128.523774372657, w0=0.20165943266892777, w1=0.6757439640225934\n",
      "(250000,)\n",
      "Gradient Descent(116/1499): loss=6068.154439293523, w0=0.20011742093435372, w1=0.6748931761467153\n",
      "(250000,)\n",
      "Gradient Descent(117/1499): loss=6008.891946483338, w0=0.19858098909619887, w1=0.6740436940735406\n",
      "(250000,)\n",
      "Gradient Descent(118/1499): loss=5950.711839943859, w0=0.19705010558223313, w1=0.6731955187352126\n",
      "(250000,)\n",
      "Gradient Descent(119/1499): loss=5893.5902319847455, w0=0.19552473946172658, w1=0.6723486511492154\n",
      "(250000,)\n",
      "Gradient Descent(120/1499): loss=5837.50378982902, w0=0.19400486040202924, w1=0.6715030924037155\n",
      "(250000,)\n",
      "Gradient Descent(121/1499): loss=5782.429722538373, w0=0.19249043862910042, w1=0.6706588436443873\n",
      "(250000,)\n",
      "Gradient Descent(122/1499): loss=5728.345768250187, w0=0.190981444891614, w1=0.6698159060625809\n",
      "(250000,)\n",
      "Gradient Descent(123/1499): loss=5675.230181718275, w0=0.18947785042830187, w1=0.6689742808847028\n",
      "(250000,)\n",
      "Gradient Descent(124/1499): loss=5623.061722149784, w0=0.18797962693822984, w1=0.6681339693626939\n",
      "(250000,)\n",
      "Gradient Descent(125/1499): loss=5571.819641330882, w0=0.1864867465537289, w1=0.6672949727654987\n",
      "(250000,)\n",
      "Gradient Descent(126/1499): loss=5521.4836720340945, w0=0.18499918181573166, w1=0.6664572923714309\n",
      "(250000,)\n",
      "Gradient Descent(127/1499): loss=5472.034016700411, w0=0.18351690565128706, w1=0.6656209294613493\n",
      "(250000,)\n",
      "Gradient Descent(128/1499): loss=5423.4513363894575, w0=0.18203989135304824, w1=0.6647858853125649\n",
      "(250000,)\n",
      "Gradient Descent(129/1499): loss=5375.7167399912805, w0=0.18056811256054794, w1=0.6639521611934097\n",
      "(250000,)\n",
      "Gradient Descent(130/1499): loss=5328.811773693332, w0=0.17910154324309316, w1=0.6631197583584035\n",
      "(250000,)\n",
      "Gradient Descent(131/1499): loss=5282.718410696633, w0=0.17764015768412725, w1=0.6622886780439597\n",
      "(250000,)\n",
      "Gradient Descent(132/1499): loss=5237.419041175058, w0=0.17618393046692146, w1=0.6614589214645784\n",
      "(250000,)\n",
      "Gradient Descent(133/1499): loss=5192.896462471943, w0=0.17473283646147167, w1=0.6606304898094801\n",
      "(250000,)\n",
      "Gradient Descent(134/1499): loss=5149.133869528368, w0=0.1732868508124871, w1=0.6598033842396366\n",
      "(250000,)\n",
      "Gradient Descent(135/1499): loss=5106.114845537581, w0=0.1718459489283693, w1=0.65897760588516\n",
      "(250000,)\n",
      "Gradient Descent(136/1499): loss=5063.82335282015, w0=0.17041010647108887, w1=0.6581531558430155\n",
      "(250000,)\n",
      "Gradient Descent(137/1499): loss=5022.243723914666, w0=0.16897929934687614, w1=0.6573300351750264\n",
      "(250000,)\n",
      "Gradient Descent(138/1499): loss=4981.360652878781, w0=0.16755350369765032, w1=0.6565082449061418\n",
      "(250000,)\n",
      "Gradient Descent(139/1499): loss=4941.159186795698, w0=0.1661326958931185, w1=0.6556877860229414\n",
      "(250000,)\n",
      "Gradient Descent(140/1499): loss=4901.624717481132, w0=0.16471685252348242, w1=0.654868659472355\n",
      "(250000,)\n",
      "Gradient Descent(141/1499): loss=4862.742973386083, w0=0.16330595039269685, w1=0.6540508661605733\n",
      "(250000,)\n",
      "Gradient Descent(142/1499): loss=4824.5000116907195, w0=0.16189996651222888, w1=0.6532344069521335\n",
      "(250000,)\n",
      "Gradient Descent(143/1499): loss=4786.882210584884, w0=0.16049887809527186, w1=0.6524192826691598\n",
      "(250000,)\n",
      "Gradient Descent(144/1499): loss=4749.876261730785, w0=0.15910266255137262, w1=0.6516054940907449\n",
      "(250000,)\n",
      "Gradient Descent(145/1499): loss=4713.469162903569, w0=0.15771129748143395, w1=0.6507930419524574\n",
      "(250000,)\n",
      "Gradient Descent(146/1499): loss=4677.648210805573, w0=0.1563247606730583, w1=0.6499819269459627\n",
      "(250000,)\n",
      "Gradient Descent(147/1499): loss=4642.4009940501255, w0=0.15494303009620192, w1=0.6491721497187458\n",
      "(250000,)\n",
      "Gradient Descent(148/1499): loss=4607.715386310929, w0=0.15356608389911103, w1=0.6483637108739249\n",
      "(250000,)\n",
      "Gradient Descent(149/1499): loss=4573.579539633051, w0=0.15219390040451528, w1=0.6475566109701469\n",
      "(250000,)\n",
      "Gradient Descent(150/1499): loss=4539.981877901787, w0=0.15082645810605483, w1=0.6467508505215569\n",
      "(250000,)\n",
      "Gradient Descent(151/1499): loss=4506.911090465536, w0=0.1494637356649208, w1=0.6459464299978316\n",
      "(250000,)\n",
      "Gradient Descent(152/1499): loss=4474.35612590921, w0=0.14810571190668997, w1=0.6451433498242727\n",
      "(250000,)\n",
      "Gradient Descent(153/1499): loss=4442.306185974475, w0=0.14675236581833673, w1=0.6443416103819515\n",
      "(250000,)\n",
      "Gradient Descent(154/1499): loss=4410.750719623433, w0=0.14540367654540678, w1=0.6435412120079004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(155/1499): loss=4379.679417242344, w0=0.14405962338933867, w1=0.6427421549953454\n",
      "(250000,)\n",
      "Gradient Descent(156/1499): loss=4349.082204982068, w0=0.14272018580492044, w1=0.6419444395939751\n",
      "(250000,)\n",
      "Gradient Descent(157/1499): loss=4318.949239231983, w0=0.14138534339786973, w1=0.6411480660102423\n",
      "(250000,)\n",
      "Gradient Descent(158/1499): loss=4289.270901224267, w0=0.14005507592252725, w1=0.6403530344076946\n",
      "(250000,)\n",
      "Gradient Descent(159/1499): loss=4260.037791765423, w0=0.1387293632796537, w1=0.6395593449073289\n",
      "(250000,)\n",
      "Gradient Descent(160/1499): loss=4231.2407260920745, w0=0.13740818551432216, w1=0.6387669975879693\n",
      "(250000,)\n",
      "Gradient Descent(161/1499): loss=4202.870728848048, w0=0.13609152281389747, w1=0.6379759924866634\n",
      "(250000,)\n",
      "Gradient Descent(162/1499): loss=4174.919029179943, w0=0.13477935550609632, w1=0.6371863295990957\n",
      "(250000,)\n",
      "Gradient Descent(163/1499): loss=4147.377055948332, w0=0.13347166405712108, w1=0.6363980088800157\n",
      "(250000,)\n",
      "Gradient Descent(164/1499): loss=4120.236433051892, w0=0.13216842906986176, w1=0.6356110302436777\n",
      "(250000,)\n",
      "Gradient Descent(165/1499): loss=4093.488974861783, w0=0.13086963128216103, w1=0.6348253935642917\n",
      "(250000,)\n",
      "Gradient Descent(166/1499): loss=4067.1266817636847, w0=0.12957525156513716, w1=0.6340410986764832\n",
      "(250000,)\n",
      "Gradient Descent(167/1499): loss=4041.141735804937, w0=0.1282852709215609, w1=0.6332581453757602\n",
      "(250000,)\n",
      "Gradient Descent(168/1499): loss=4015.5264964442977, w0=0.12699967048428207, w1=0.6324765334189867\n",
      "(250000,)\n",
      "Gradient Descent(169/1499): loss=3990.273496401908, w0=0.1257184315147026, w1=0.6316962625248603\n",
      "(250000,)\n",
      "Gradient Descent(170/1499): loss=3965.375437607093, w0=0.12444153540129242, w1=0.6309173323743945\n",
      "(250000,)\n",
      "Gradient Descent(171/1499): loss=3940.8251872416713, w0=0.12316896365814549, w1=0.6301397426114028\n",
      "(250000,)\n",
      "Gradient Descent(172/1499): loss=3916.615773876553, w0=0.12190069792357319, w1=0.6293634928429857\n",
      "(250000,)\n",
      "Gradient Descent(173/1499): loss=3892.7403836993744, w0=0.12063671995873274, w1=0.6285885826400179\n",
      "(250000,)\n",
      "Gradient Descent(174/1499): loss=3869.192356831066, w0=0.11937701164628817, w1=0.6278150115376373\n",
      "(250000,)\n",
      "Gradient Descent(175/1499): loss=3845.9651837292267, w0=0.11812155498910212, w1=0.6270427790357318\n",
      "(250000,)\n",
      "Gradient Descent(176/1499): loss=3823.052501676252, w0=0.11687033210895643, w1=0.6262718845994272\n",
      "(250000,)\n",
      "Gradient Descent(177/1499): loss=3800.4480913502352, w0=0.11562332524529985, w1=0.6255023276595727\n",
      "(250000,)\n",
      "Gradient Descent(178/1499): loss=3778.145873476665, w0=0.1143805167540214, w1=0.6247341076132253\n",
      "(250000,)\n",
      "Gradient Descent(179/1499): loss=3756.1399055590236, w0=0.11314188910624802, w1=0.623967223824132\n",
      "(250000,)\n",
      "Gradient Descent(180/1499): loss=3734.424378686419, w0=0.11190742488716504, w1=0.6232016756232087\n",
      "(250000,)\n",
      "Gradient Descent(181/1499): loss=3712.9936144164285, w0=0.11067710679485848, w1=0.622437462309018\n",
      "(250000,)\n",
      "Gradient Descent(182/1499): loss=3691.842061731386, w0=0.10945091763917807, w1=0.621674583148242\n",
      "(250000,)\n",
      "Gradient Descent(183/1499): loss=3670.964294066368, w0=0.1082288403406199, w1=0.6209130373761536\n",
      "(250000,)\n",
      "Gradient Descent(184/1499): loss=3650.3550064071865, w0=0.10701085792922804, w1=0.620152824197083\n",
      "(250000,)\n",
      "Gradient Descent(185/1499): loss=3630.0090124567555, w0=0.10579695354351398, w1=0.6193939427848809\n",
      "(250000,)\n",
      "Gradient Descent(186/1499): loss=3609.92124186818, w0=0.10458711042939339, w1=0.6186363922833782\n",
      "(250000,)\n",
      "Gradient Descent(187/1499): loss=3590.086737543023, w0=0.10338131193913945, w1=0.6178801718068412\n",
      "(250000,)\n",
      "Gradient Descent(188/1499): loss=3570.5006529932, w0=0.10217954153035198, w1=0.6171252804404229\n",
      "(250000,)\n",
      "Gradient Descent(189/1499): loss=3551.158249765001, w0=0.10098178276494199, w1=0.61637171724061\n",
      "(250000,)\n",
      "Gradient Descent(190/1499): loss=3532.054894923754, w0=0.09978801930813087, w1=0.6156194812356657\n",
      "(250000,)\n",
      "Gradient Descent(191/1499): loss=3513.186058597742, w0=0.0985982349274639, w1=0.6148685714260678\n",
      "(250000,)\n",
      "Gradient Descent(192/1499): loss=3494.54731157993, w0=0.09741241349183755, w1=0.6141189867849427\n",
      "(250000,)\n",
      "Gradient Descent(193/1499): loss=3476.134322986145, w0=0.09623053897054008, w1=0.6133707262584951\n",
      "(250000,)\n",
      "Gradient Descent(194/1499): loss=3457.9428579684195, w0=0.0950525954323052, w1=0.6126237887664322\n",
      "(250000,)\n",
      "Gradient Descent(195/1499): loss=3439.968775482119, w0=0.09387856704437814, w1=0.6118781732023841\n",
      "(250000,)\n",
      "Gradient Descent(196/1499): loss=3422.2080261056594, w0=0.09270843807159408, w1=0.6111338784343199\n",
      "(250000,)\n",
      "Gradient Descent(197/1499): loss=3404.6566499115206, w0=0.09154219287546847, w1=0.610390903304958\n",
      "(250000,)\n",
      "Gradient Descent(198/1499): loss=3387.310774387377, w0=0.09037981591329886, w1=0.6096492466321732\n",
      "(250000,)\n",
      "Gradient Descent(199/1499): loss=3370.16661240614, w0=0.08922129173727815, w1=0.608908907209398\n",
      "(250000,)\n",
      "Gradient Descent(200/1499): loss=3353.220460243761, w0=0.08806660499361883, w1=0.6081698838060201\n",
      "(250000,)\n",
      "Gradient Descent(201/1499): loss=3336.468695643696, w0=0.08691574042168801, w1=0.6074321751677747\n",
      "(250000,)\n",
      "Gradient Descent(202/1499): loss=3319.9077759268785, w0=0.08576868285315303, w1=0.6066957800171326\n",
      "(250000,)\n",
      "Gradient Descent(203/1499): loss=3303.5342361461767, w0=0.08462541721113741, w1=0.6059606970536838\n",
      "(250000,)\n",
      "Gradient Descent(204/1499): loss=3287.344687284247, w0=0.08348592850938691, w1=0.6052269249545161\n",
      "(250000,)\n",
      "Gradient Descent(205/1499): loss=3271.3358144937924, w0=0.08235020185144551, w1=0.6044944623745895\n",
      "(250000,)\n",
      "Gradient Descent(206/1499): loss=3255.5043753791883, w0=0.08121822242984114, w1=0.6037633079471061\n",
      "(250000,)\n",
      "Gradient Descent(207/1499): loss=3239.8471983185364, w0=0.08008997552528095, w1=0.6030334602838753\n",
      "(250000,)\n",
      "Gradient Descent(208/1499): loss=3224.361180825158, w0=0.07896544650585598, w1=0.602304917975675\n",
      "(250000,)\n",
      "Gradient Descent(209/1499): loss=3209.043287947634, w0=0.07784462082625503, w1=0.6015776795926078\n",
      "(250000,)\n",
      "Gradient Descent(210/1499): loss=3193.8905507074305, w0=0.07672748402698758, w1=0.6008517436844533\n",
      "(250000,)\n",
      "Gradient Descent(211/1499): loss=3178.900064573283, w0=0.07561402173361567, w1=0.6001271087810157\n",
      "(250000,)\n",
      "Gradient Descent(212/1499): loss=3164.0689879714196, w0=0.07450421965599449, w1=0.5994037733924676\n",
      "(250000,)\n",
      "Gradient Descent(213/1499): loss=3149.3945408308077, w0=0.07339806358752161, w1=0.598681736009689\n",
      "(250000,)\n",
      "Gradient Descent(214/1499): loss=3134.874003162586, w0=0.07229553940439475, w1=0.5979609951046021\n",
      "(250000,)\n",
      "Gradient Descent(215/1499): loss=3120.504713672881, w0=0.07119663306487783, w1=0.5972415491305026\n",
      "(250000,)\n",
      "Gradient Descent(216/1499): loss=3106.28406840821, w0=0.07010133060857535, w1=0.596523396522386\n",
      "(250000,)\n",
      "Gradient Descent(217/1499): loss=3092.2095194327135, w0=0.0690096181557148, w1=0.59580653569727\n",
      "(250000,)\n",
      "Gradient Descent(218/1499): loss=3078.2785735364623, w0=0.06792148190643715, w1=0.5950909650545139\n",
      "(250000,)\n",
      "Gradient Descent(219/1499): loss=3064.488790974097, w0=0.06683690814009523, w1=0.5943766829761316\n",
      "(250000,)\n",
      "Gradient Descent(220/1499): loss=3050.837784233098, w0=0.06575588321455975, w1=0.5936636878271033\n",
      "(250000,)\n",
      "Gradient Descent(221/1499): loss=3037.3232168309746, w0=0.0646783935655332, w1=0.5929519779556819\n",
      "(250000,)\n",
      "Gradient Descent(222/1499): loss=3023.942802140702, w0=0.0636044257058712, w1=0.5922415516936949\n",
      "(250000,)\n",
      "Gradient Descent(223/1499): loss=3010.694302243728, w0=0.0625339662249113, w1=0.5915324073568439\n",
      "(250000,)\n",
      "Gradient Descent(224/1499): loss=2997.5755268099138, w0=0.061467001787809186, w1=0.5908245432449996\n",
      "(250000,)\n",
      "Gradient Descent(225/1499): loss=2984.584332003752, w0=0.060403519134882205, w1=0.5901179576424924\n",
      "(250000,)\n",
      "Gradient Descent(226/1499): loss=2971.7186194162628, w0=0.05934350508095996, w1=0.5894126488184007\n",
      "(250000,)\n",
      "Gradient Descent(227/1499): loss=2958.976335021955, w0=0.05828694651474207, w1=0.588708615026834\n",
      "(250000,)\n",
      "Gradient Descent(228/1499): loss=2946.3554681602513, w0=0.057233830398162905, w1=0.5880058545072135\n",
      "(250000,)\n",
      "Gradient Descent(229/1499): loss=2933.8540505408187, w0=0.05618414376576322, w1=0.5873043654845485\n",
      "(250000,)\n",
      "Gradient Descent(230/1499): loss=2921.4701552722245, w0=0.055137873724068615, w1=0.5866041461697091\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(231/1499): loss=2909.2018959133707, w0=0.05409500745097471, w1=0.5859051947596964\n",
      "(250000,)\n",
      "Gradient Descent(232/1499): loss=2897.0474255471827, w0=0.05305553219513901, w1=0.5852075094379077\n",
      "(250000,)\n",
      "Gradient Descent(233/1499): loss=2885.004935876005, w0=0.052019435275379305, w1=0.5845110883743995\n",
      "(250000,)\n",
      "Gradient Descent(234/1499): loss=2873.07265633821, w0=0.050986704080078556, w1=0.5838159297261465\n",
      "(250000,)\n",
      "Gradient Descent(235/1499): loss=2861.2488532455113, w0=0.04995732606659623, w1=0.5831220316372974\n",
      "(250000,)\n",
      "Gradient Descent(236/1499): loss=2849.531828940501, w0=0.048931288760685934, w1=0.5824293922394272\n",
      "(250000,)\n",
      "Gradient Descent(237/1499): loss=2837.9199209739154, w0=0.047908579755919285, w1=0.581738009651787\n",
      "(250000,)\n",
      "Gradient Descent(238/1499): loss=2826.4115013011883, w0=0.04688918671311601, w1=0.581047881981549\n",
      "(250000,)\n",
      "Gradient Descent(239/1499): loss=2815.0049754978245, w0=0.04587309735978012, w1=0.5803590073240499\n",
      "(250000,)\n",
      "Gradient Descent(240/1499): loss=2803.698781993138, w0=0.044860299489542114, w1=0.5796713837630301\n",
      "(250000,)\n",
      "Gradient Descent(241/1499): loss=2792.4913913219516, w0=0.04385078096160716, w1=0.5789850093708705\n",
      "(250000,)\n",
      "Gradient Descent(242/1499): loss=2781.381305393809, w0=0.04284452970020915, w1=0.5782998822088256\n",
      "(250000,)\n",
      "Gradient Descent(243/1499): loss=2770.367056779288, w0=0.04184153369407057, w1=0.5776160003272535\n",
      "(250000,)\n",
      "Gradient Descent(244/1499): loss=2759.4472080130276, w0=0.040841780995868174, w1=0.5769333617658439\n",
      "(250000,)\n",
      "Gradient Descent(245/1499): loss=2748.620350913066, w0=0.03984525972170425, w1=0.5762519645538415\n",
      "(250000,)\n",
      "Gradient Descent(246/1499): loss=2737.885105916099, w0=0.038851958050583585, w1=0.5755718067102683\n",
      "(250000,)\n",
      "Gradient Descent(247/1499): loss=2727.2401214282845, w0=0.037861864223895925, w1=0.574892886244141\n",
      "(250000,)\n",
      "Gradient Descent(248/1499): loss=2716.6840731912457, w0=0.03687496654490396, w1=0.5742152011546874\n",
      "(250000,)\n",
      "Gradient Descent(249/1499): loss=2706.2156636628897, w0=0.03589125337823669, w1=0.5735387494315585\n",
      "(250000,)\n",
      "Gradient Descent(250/1499): loss=2695.833621412697, w0=0.03491071314938819, w1=0.5728635290550387\n",
      "(250000,)\n",
      "Gradient Descent(251/1499): loss=2685.5367005311646, w0=0.03393333434422165, w1=0.5721895379962528\n",
      "(250000,)\n",
      "Gradient Descent(252/1499): loss=2675.32368005303, w0=0.032959105508478616, w1=0.5715167742173708\n",
      "(250000,)\n",
      "Gradient Descent(253/1499): loss=2665.19336339399, w0=0.031988015247293486, w1=0.5708452356718087\n",
      "(250000,)\n",
      "Gradient Descent(254/1499): loss=2655.144577800566, w0=0.031020052224713022, w1=0.5701749203044282\n",
      "(250000,)\n",
      "Gradient Descent(255/1499): loss=2645.1761738128225, w0=0.030055205163220987, w1=0.5695058260517329\n",
      "(250000,)\n",
      "Gradient Descent(256/1499): loss=2635.2870247396363, w0=0.02909346284326773, w1=0.5688379508420621\n",
      "(250000,)\n",
      "Gradient Descent(257/1499): loss=2625.4760261462097, w0=0.02813481410280475, w1=0.5681712925957817\n",
      "(250000,)\n",
      "Gradient Descent(258/1499): loss=2615.7420953535448, w0=0.027179247836824086, w1=0.5675058492254729\n",
      "(250000,)\n",
      "Gradient Descent(259/1499): loss=2606.084170949611, w0=0.02622675299690259, w1=0.5668416186361186\n",
      "(250000,)\n",
      "Gradient Descent(260/1499): loss=2596.501212311896, w0=0.025277318590750917, w1=0.5661785987252868\n",
      "(250000,)\n",
      "Gradient Descent(261/1499): loss=2586.9921991411147, w0=0.02433093368176725, w1=0.5655167873833116\n",
      "(250000,)\n",
      "Gradient Descent(262/1499): loss=2577.5561310057733, w0=0.02338758738859568, w1=0.5648561824934722\n",
      "(250000,)\n",
      "Gradient Descent(263/1499): loss=2568.1920268973736, w0=0.02244726888468919, w1=0.5641967819321694\n",
      "(250000,)\n",
      "Gradient Descent(264/1499): loss=2558.8989247959526, w0=0.021509967397877177, w1=0.5635385835690988\n",
      "(250000,)\n",
      "Gradient Descent(265/1499): loss=2549.675881245781, w0=0.020575672209937505, w1=0.5628815852674234\n",
      "(250000,)\n",
      "Gradient Descent(266/1499): loss=2540.5219709409193, w0=0.01964437265617296, w1=0.5622257848839423\n",
      "(250000,)\n",
      "Gradient Descent(267/1499): loss=2531.436286320448, w0=0.018716058124992142, w1=0.5615711802692579\n",
      "(250000,)\n",
      "Gradient Descent(268/1499): loss=2522.4179371730975, w0=0.017790718057494676, w1=0.5609177692679409\n",
      "(250000,)\n",
      "Gradient Descent(269/1499): loss=2513.4660502511097, w0=0.01686834194706072, w1=0.5602655497186926\n",
      "(250000,)\n",
      "Gradient Descent(270/1499): loss=2504.5797688930634, w0=0.015948919338944732, w1=0.5596145194545051\n",
      "(250000,)\n",
      "Gradient Descent(271/1499): loss=2495.7582526554825, w0=0.01503243982987343, w1=0.5589646763028203\n",
      "(250000,)\n",
      "Gradient Descent(272/1499): loss=2487.0006769530205, w0=0.014118893067647888, w1=0.558316018085685\n",
      "(250000,)\n",
      "Gradient Descent(273/1499): loss=2478.306232706994, w0=0.013208268750749744, w1=0.5576685426199053\n",
      "(250000,)\n",
      "Gradient Descent(274/1499): loss=2469.6741260021035, w0=0.01230055662795147, w1=0.5570222477171984\n",
      "(250000,)\n",
      "Gradient Descent(275/1499): loss=2461.1035777511224, w0=0.011395746497930624, w1=0.5563771311843426\n",
      "(250000,)\n",
      "Gradient Descent(276/1499): loss=2452.593823367369, w0=0.010493828208888101, w1=0.5557331908233244\n",
      "(250000,)\n",
      "Gradient Descent(277/1499): loss=2444.1441124448056, w0=0.009594791658170268, w1=0.5550904244314848\n",
      "(250000,)\n",
      "Gradient Descent(278/1499): loss=2435.7537084455425, w0=0.008698626791894985, w1=0.5544488298016629\n",
      "(250000,)\n",
      "Gradient Descent(279/1499): loss=2427.4218883946082, w0=0.007805323604581455, w1=0.5538084047223373\n",
      "(250000,)\n",
      "Gradient Descent(280/1499): loss=2419.1479425817975, w0=0.006914872138783853, w1=0.5531691469777664\n",
      "(250000,)\n",
      "Gradient Descent(281/1499): loss=2410.931174270432, w0=0.006027262484728701, w1=0.5525310543481262\n",
      "(250000,)\n",
      "Gradient Descent(282/1499): loss=2402.770899412879, w0=0.005142484779955933, w1=0.5518941246096458\n",
      "(250000,)\n",
      "Gradient Descent(283/1499): loss=2394.6664463726584, w0=0.004260529208963627, w1=0.5512583555347422\n",
      "(250000,)\n",
      "Gradient Descent(284/1499): loss=2386.617155652995, w0=0.0033813860028563385, w1=0.550623744892152\n",
      "(250000,)\n",
      "Gradient Descent(285/1499): loss=2378.6223796316517, w0=0.0025050454389970206, w1=0.5499902904470624\n",
      "(250000,)\n",
      "Gradient Descent(286/1499): loss=2370.6814823019045, w0=0.0016314978406624637, w1=0.5493579899612392\n",
      "(250000,)\n",
      "Gradient Descent(287/1499): loss=2362.7938390195145, w0=0.000760733576702234, w1=0.5487268411931543\n",
      "(250000,)\n",
      "Gradient Descent(288/1499): loss=2354.9588362555533, w0=-0.00010725693879893596, w1=0.5480968418981104\n",
      "(250000,)\n",
      "Gradient Descent(289/1499): loss=2347.175871354938, w0=-0.0009724832468553451, w1=0.5474679898283643\n",
      "(250000,)\n",
      "Gradient Descent(290/1499): loss=2339.444352300564, w0=-0.0018349548439111475, w1=0.5468402827332489\n",
      "(250000,)\n",
      "Gradient Descent(291/1499): loss=2331.76369748287, w0=-0.0026946811821678372, w1=0.5462137183592928\n",
      "(250000,)\n",
      "Gradient Descent(292/1499): loss=2324.1333354747417, w0=-0.003551671669908445, w1=0.5455882944503387\n",
      "(250000,)\n",
      "Gradient Descent(293/1499): loss=2316.5527048116032, w0=-0.004405935671818457, w1=0.5449640087476602\n",
      "(250000,)\n",
      "Gradient Descent(294/1499): loss=2309.021253776589, w0=-0.0052574825093035325, w1=0.5443408589900761\n",
      "(250000,)\n",
      "Gradient Descent(295/1499): loss=2301.538440190671, w0=-0.006106321460804012, w1=0.5437188429140649\n",
      "(250000,)\n",
      "Gradient Descent(296/1499): loss=2294.103731207621, w0=-0.006952461762106292, w1=0.5430979582538755\n",
      "(250000,)\n",
      "Gradient Descent(297/1499): loss=2286.716603113712, w0=-0.007795912606651079, w1=0.5424782027416378\n",
      "(250000,)\n",
      "Gradient Descent(298/1499): loss=2279.3765411320214, w0=-0.008636683145838557, w1=0.5418595741074717\n",
      "(250000,)\n",
      "Gradient Descent(299/1499): loss=2272.0830392312455, w0=-0.009474782489330525, w1=0.5412420700795934\n",
      "(250000,)\n",
      "Gradient Descent(300/1499): loss=2264.8355999389173, w0=-0.01031021970534951, w1=0.5406256883844217\n",
      "(250000,)\n",
      "Gradient Descent(301/1499): loss=2257.633734158924, w0=-0.011143003820974915, w1=0.5400104267466819\n",
      "(250000,)\n",
      "Gradient Descent(302/1499): loss=2250.47696099321, w0=-0.01197314382243621, w1=0.5393962828895081\n",
      "(250000,)\n",
      "Gradient Descent(303/1499): loss=2243.364807567598, w0=-0.012800648655403233, w1=0.5387832545345449\n",
      "(250000,)\n",
      "Gradient Descent(304/1499): loss=2236.296808861586, w0=-0.0136255272252736, w1=0.5381713394020464\n",
      "(250000,)\n",
      "Gradient Descent(305/1499): loss=2229.2725075420813, w0=-0.014447788397457278, w1=0.5375605352109751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(306/1499): loss=2222.2914538009245, w0=-0.015267440997658342, w1=0.5369508396790986\n",
      "(250000,)\n",
      "Gradient Descent(307/1499): loss=2215.353205196166, w0=-0.016084493812153957, w1=0.5363422505230848\n",
      "(250000,)\n",
      "Gradient Descent(308/1499): loss=2208.4573264969536, w0=-0.01689895558807061, w1=0.5357347654585963\n",
      "(250000,)\n",
      "Gradient Descent(309/1499): loss=2201.6033895320024, w0=-0.017710835033657616, w1=0.5351283822003832\n",
      "(250000,)\n",
      "Gradient Descent(310/1499): loss=2194.7909730415154, w0=-0.01852014081855795, w1=0.5345230984623741\n",
      "(250000,)\n",
      "Gradient Descent(311/1499): loss=2188.019662532505, w0=-0.019326881574076422, w1=0.5339189119577668\n",
      "(250000,)\n",
      "Gradient Descent(312/1499): loss=2181.289050137421, w0=-0.020131065893445214, w1=0.5333158203991164\n",
      "(250000,)\n",
      "Gradient Descent(313/1499): loss=2174.5987344760024, w0=-0.02093270233208682, w1=0.5327138214984234\n",
      "(250000,)\n",
      "Gradient Descent(314/1499): loss=2167.948320520306, w0=-0.02173179940787444, w1=0.5321129129672195\n",
      "(250000,)\n",
      "Gradient Descent(315/1499): loss=2161.337419462797, w0=-0.0225283656013898, w1=0.5315130925166528\n",
      "(250000,)\n",
      "Gradient Descent(316/1499): loss=2154.765648587461, w0=-0.023322409356178502, w1=0.5309143578575714\n",
      "(250000,)\n",
      "Gradient Descent(317/1499): loss=2148.232631143857, w0=-0.024113939079002854, w1=0.5303167067006058\n",
      "(250000,)\n",
      "Gradient Descent(318/1499): loss=2141.7379962240343, w0=-0.024902963140092268, w1=0.5297201367562505\n",
      "(250000,)\n",
      "Gradient Descent(319/1499): loss=2135.2813786422585, w0=-0.02568948987339123, w1=0.5291246457349437\n",
      "(250000,)\n",
      "Gradient Descent(320/1499): loss=2128.862418817474, w0=-0.026473527576804854, w1=0.5285302313471464\n",
      "(250000,)\n",
      "Gradient Descent(321/1499): loss=2122.4807626584425, w0=-0.027255084512442084, w1=0.5279368913034201\n",
      "(250000,)\n",
      "Gradient Descent(322/1499): loss=2116.13606145149, w0=-0.028034168906856543, w1=0.5273446233145036\n",
      "(250000,)\n",
      "Gradient Descent(323/1499): loss=2109.827971750807, w0=-0.028810788951285055, w1=0.5267534250913879\n",
      "(250000,)\n",
      "Gradient Descent(324/1499): loss=2103.5561552712356, w0=-0.029584952801883876, w1=0.5261632943453909\n",
      "(250000,)\n",
      "Gradient Descent(325/1499): loss=2097.320278783497, w0=-0.03035666857996267, w1=0.5255742287882306\n",
      "(250000,)\n",
      "Gradient Descent(326/1499): loss=2091.1200140117817, w0=-0.031125944372216226, w1=0.5249862261320971\n",
      "(250000,)\n",
      "Gradient Descent(327/1499): loss=2084.955037533673, w0=-0.031892788230953956, w1=0.5243992840897238\n",
      "(250000,)\n",
      "Gradient Descent(328/1499): loss=2078.8250306823284, w0=-0.03265720817432721, w1=0.5238134003744571\n",
      "(250000,)\n",
      "Gradient Descent(329/1499): loss=2072.7296794508766, w0=-0.03341921218655444, w1=0.5232285727003255\n",
      "(250000,)\n",
      "Gradient Descent(330/1499): loss=2066.6686743989726, w0=-0.03417880821814417, w1=0.5226447987821075\n",
      "(250000,)\n",
      "Gradient Descent(331/1499): loss=2060.6417105614687, w0=-0.03493600418611588, w1=0.5220620763353979\n",
      "(250000,)\n",
      "Gradient Descent(332/1499): loss=2054.64848735914, w0=-0.03569080797421882, w1=0.5214804030766746\n",
      "(250000,)\n",
      "Gradient Descent(333/1499): loss=2048.6887085114317, w0=-0.03644322743314868, w1=0.5208997767233626\n",
      "(250000,)\n",
      "Gradient Descent(334/1499): loss=2042.762081951163, w0=-0.03719327038076227, w1=0.5203201949938979\n",
      "(250000,)\n",
      "Gradient Descent(335/1499): loss=2036.8683197411556, w0=-0.037940944602290146, w1=0.5197416556077908\n",
      "(250000,)\n",
      "Gradient Descent(336/1499): loss=2031.0071379927426, w0=-0.03868625785054724, w1=0.5191641562856875\n",
      "(250000,)\n",
      "Gradient Descent(337/1499): loss=2025.1782567860994, w0=-0.03942921784614149, w1=0.5185876947494306\n",
      "(250000,)\n",
      "Gradient Descent(338/1499): loss=2019.3814000923678, w0=-0.04016983227768057, w1=0.5180122687221199\n",
      "(250000,)\n",
      "Gradient Descent(339/1499): loss=2013.6162956975338, w0=-0.040908108801976574, w1=0.517437875928171\n",
      "(250000,)\n",
      "Gradient Descent(340/1499): loss=2007.8826751279971, w0=-0.04164405504424889, w1=0.5168645140933733\n",
      "(250000,)\n",
      "Gradient Descent(341/1499): loss=2002.1802735778235, w0=-0.0423776785983251, w1=0.5162921809449474\n",
      "(250000,)\n",
      "Gradient Descent(342/1499): loss=1996.508829837612, w0=-0.04310898702684007, w1=0.5157208742116016\n",
      "(250000,)\n",
      "Gradient Descent(343/1499): loss=1990.8680862249596, w0=-0.043837987861433135, w1=0.5151505916235869\n",
      "(250000,)\n",
      "Gradient Descent(344/1499): loss=1985.2577885164776, w0=-0.044564688602943464, w1=0.5145813309127519\n",
      "(250000,)\n",
      "Gradient Descent(345/1499): loss=1979.6776858813175, w0=-0.04528909672160363, w1=0.5140130898125966\n",
      "(250000,)\n",
      "Gradient Descent(346/1499): loss=1974.1275308161948, w0=-0.04601121965723137, w1=0.513445866058325\n",
      "(250000,)\n",
      "Gradient Descent(347/1499): loss=1968.6070790818376, w0=-0.04673106481941957, w1=0.5128796573868971\n",
      "(250000,)\n",
      "Gradient Descent(348/1499): loss=1963.1160896408674, w0=-0.047448639587724495, w1=0.5123144615370803\n",
      "(250000,)\n",
      "Gradient Descent(349/1499): loss=1957.6543245970486, w0=-0.048163951311852325, w1=0.5117502762494994\n",
      "(250000,)\n",
      "Gradient Descent(350/1499): loss=1952.2215491358909, w0=-0.0488770073118439, w1=0.5111870992666868\n",
      "(250000,)\n",
      "Gradient Descent(351/1499): loss=1946.8175314665705, w0=-0.04958781487825783, w1=0.5106249283331306\n",
      "(250000,)\n",
      "Gradient Descent(352/1499): loss=1941.4420427651307, w0=-0.050296381272351906, w1=0.5100637611953227\n",
      "(250000,)\n",
      "Gradient Descent(353/1499): loss=1936.0948571189442, w0=-0.05100271372626287, w1=0.5095035956018065\n",
      "(250000,)\n",
      "Gradient Descent(354/1499): loss=1930.775751472403, w0=-0.05170681944318448, w1=0.5089444293032225\n",
      "(250000,)\n",
      "Gradient Descent(355/1499): loss=1925.4845055738033, w0=-0.05240870559754406, w1=0.5083862600523548\n",
      "(250000,)\n",
      "Gradient Descent(356/1499): loss=1920.2209019234042, w0=-0.053108379335177335, w1=0.5078290856041753\n",
      "(250000,)\n",
      "Gradient Descent(357/1499): loss=1914.9847257226274, w0=-0.05380584777350176, w1=0.5072729037158884\n",
      "(250000,)\n",
      "Gradient Descent(358/1499): loss=1909.7757648243773, w0=-0.05450111800168823, w1=0.506717712146974\n",
      "(250000,)\n",
      "Gradient Descent(359/1499): loss=1904.59380968445, w0=-0.05519419708083126, w1=0.5061635086592307\n",
      "(250000,)\n",
      "Gradient Descent(360/1499): loss=1899.4386533140062, w0=-0.05588509204411763, w1=0.5056102910168172\n",
      "(250000,)\n",
      "Gradient Descent(361/1499): loss=1894.3100912330947, w0=-0.0565738098969935, w1=0.5050580569862942\n",
      "(250000,)\n",
      "Gradient Descent(362/1499): loss=1889.2079214251744, w0=-0.05726035761733002, w1=0.5045068043366647\n",
      "(250000,)\n",
      "Gradient Descent(363/1499): loss=1884.1319442926554, w0=-0.05794474215558746, w1=0.5039565308394139\n",
      "(250000,)\n",
      "Gradient Descent(364/1499): loss=1879.0819626133866, w0=-0.0586269704349779, w1=0.5034072342685486\n",
      "(250000,)\n",
      "Gradient Descent(365/1499): loss=1874.0577814981032, w0=-0.0593070493516264, w1=0.5028589124006357\n",
      "(250000,)\n",
      "Gradient Descent(366/1499): loss=1869.0592083488007, w0=-0.0599849857747308, w1=0.5023115630148401\n",
      "(250000,)\n",
      "Gradient Descent(367/1499): loss=1864.0860528180094, w0=-0.06066078654672005, w1=0.5017651838929621\n",
      "(250000,)\n",
      "Gradient Descent(368/1499): loss=1859.1381267689585, w0=-0.06133445848341117, w1=0.5012197728194736\n",
      "(250000,)\n",
      "Gradient Descent(369/1499): loss=1854.2152442365923, w0=-0.06200600837416477, w1=0.5006753275815548\n",
      "(250000,)\n",
      "Gradient Descent(370/1499): loss=1849.3172213894427, w0=-0.06267544298203927, w1=0.5001318459691289\n",
      "(250000,)\n",
      "Gradient Descent(371/1499): loss=1844.44387649232, w0=-0.06334276904394366, w1=0.49958932577489723\n",
      "(250000,)\n",
      "Gradient Descent(372/1499): loss=1839.595029869805, w0=-0.06400799327078904, w1=0.4990477647943734\n",
      "(250000,)\n",
      "Gradient Descent(373/1499): loss=1834.7705038705342, w0=-0.06467112234763867, w1=0.4985071608259165\n",
      "(250000,)\n",
      "Gradient Descent(374/1499): loss=1829.970122832248, w0=-0.06533216293385684, w1=0.49796751167076436\n",
      "(250000,)\n",
      "Gradient Descent(375/1499): loss=1825.1937130475937, w0=-0.0659911216632564, w1=0.4974288151330658\n",
      "(250000,)\n",
      "Gradient Descent(376/1499): loss=1820.441102730656, w0=-0.06664800514424493, w1=0.49689106901991237\n",
      "(250000,)\n",
      "Gradient Descent(377/1499): loss=1815.7121219842106, w0=-0.06730281995996976, w1=0.4963542711413696\n",
      "(250000,)\n",
      "Gradient Descent(378/1499): loss=1811.0066027676712, w0=-0.06795557266846158, w1=0.49581841931050763\n",
      "(250000,)\n",
      "Gradient Descent(379/1499): loss=1806.3243788657253, w0=-0.06860626980277698, w1=0.49528351134343124\n",
      "(250000,)\n",
      "Gradient Descent(380/1499): loss=1801.6652858576333, w0=-0.06925491787113956, w1=0.49474954505930946\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(381/1499): loss=1797.029161087187, w0=-0.06990152335708001, w1=0.49421651828040447\n",
      "(250000,)\n",
      "Gradient Descent(382/1499): loss=1792.4158436332968, w0=-0.07054609271957482, w1=0.49368442883210006\n",
      "(250000,)\n",
      "Gradient Descent(383/1499): loss=1787.8251742812088, w0=-0.0711886323931839, w1=0.4931532745429297\n",
      "(250000,)\n",
      "Gradient Descent(384/1499): loss=1783.2569954943294, w0=-0.07182914878818698, w1=0.4926230532446037\n",
      "(250000,)\n",
      "Gradient Descent(385/1499): loss=1778.7111513866391, w0=-0.07246764829071886, w1=0.4920937627720363\n",
      "(250000,)\n",
      "Gradient Descent(386/1499): loss=1774.1874876956977, w0=-0.07310413726290345, w1=0.49156540096337187\n",
      "(250000,)\n",
      "Gradient Descent(387/1499): loss=1769.6858517562075, w0=-0.07373862204298674, w1=0.4910379656600109\n",
      "(250000,)\n",
      "Gradient Descent(388/1499): loss=1765.2060924741356, w0=-0.07437110894546863, w1=0.4905114547066354\n",
      "(250000,)\n",
      "Gradient Descent(389/1499): loss=1760.7480603013778, w0=-0.0750016042612335, w1=0.48998586595123356\n",
      "(250000,)\n",
      "Gradient Descent(390/1499): loss=1756.3116072109535, w0=-0.07563011425767997, w1=0.48946119724512455\n",
      "(250000,)\n",
      "Gradient Descent(391/1499): loss=1751.896586672715, w0=-0.0762566451788492, w1=0.48893744644298204\n",
      "(250000,)\n",
      "Gradient Descent(392/1499): loss=1747.5028536295652, w0=-0.07688120324555238, w1=0.48841461140285786\n",
      "(250000,)\n",
      "Gradient Descent(393/1499): loss=1743.1302644741681, w0=-0.07750379465549706, w1=0.48789268998620505\n",
      "(250000,)\n",
      "Gradient Descent(394/1499): loss=1738.7786770261416, w0=-0.07812442558341239, w1=0.4873716800579002\n",
      "(250000,)\n",
      "Gradient Descent(395/1499): loss=1734.4479505097265, w0=-0.07874310218117334, w1=0.48685157948626573\n",
      "(250000,)\n",
      "Gradient Descent(396/1499): loss=1730.137945531908, w0=-0.07935983057792385, w1=0.4863323861430914\n",
      "(250000,)\n",
      "Gradient Descent(397/1499): loss=1725.8485240609984, w0=-0.07997461688019905, w1=0.4858140979036555\n",
      "(250000,)\n",
      "Gradient Descent(398/1499): loss=1721.5795494056522, w0=-0.0805874671720463, w1=0.4852967126467457\n",
      "(250000,)\n",
      "Gradient Descent(399/1499): loss=1717.330886194312, w0=-0.0811983875151454, w1=0.48478022825467926\n",
      "(250000,)\n",
      "Gradient Descent(400/1499): loss=1713.1024003550817, w0=-0.08180738394892766, w1=0.48426464261332314\n",
      "(250000,)\n",
      "Gradient Descent(401/1499): loss=1708.8939590960022, w0=-0.08241446249069406, w1=0.48374995361211326\n",
      "(250000,)\n",
      "Gradient Descent(402/1499): loss=1704.7054308857341, w0=-0.08301962913573248, w1=0.4832361591440737\n",
      "(250000,)\n",
      "Gradient Descent(403/1499): loss=1700.5366854346328, w0=-0.08362288985743382, w1=0.48272325710583536\n",
      "(250000,)\n",
      "Gradient Descent(404/1499): loss=1696.3875936762008, w0=-0.08422425060740733, w1=0.48221124539765414\n",
      "(250000,)\n",
      "Gradient Descent(405/1499): loss=1692.2580277489271, w0=-0.08482371731559486, w1=0.48170012192342904\n",
      "(250000,)\n",
      "Gradient Descent(406/1499): loss=1688.1478609784792, w0=-0.0854212958903843, w1=0.4811898845907193\n",
      "(250000,)\n",
      "Gradient Descent(407/1499): loss=1684.0569678602646, w0=-0.08601699221872199, w1=0.4806805313107619\n",
      "(250000,)\n",
      "Gradient Descent(408/1499): loss=1679.9852240423363, w0=-0.08661081216622428, w1=0.48017205999848783\n",
      "(250000,)\n",
      "Gradient Descent(409/1499): loss=1675.9325063086394, w0=-0.08720276157728817, w1=0.4796644685725389\n",
      "(250000,)\n",
      "Gradient Descent(410/1499): loss=1671.8986925625995, w0=-0.08779284627520104, w1=0.47915775495528334\n",
      "(250000,)\n",
      "Gradient Descent(411/1499): loss=1667.8836618110283, w0=-0.08838107206224953, w1=0.47865191707283156\n",
      "(250000,)\n",
      "Gradient Descent(412/1499): loss=1663.8872941483578, w0=-0.0889674447198275, w1=0.4781469528550513\n",
      "(250000,)\n",
      "Gradient Descent(413/1499): loss=1659.9094707411766, w0=-0.08955197000854313, w1=0.4776428602355828\n",
      "(250000,)\n",
      "Gradient Descent(414/1499): loss=1655.9500738130844, w0=-0.09013465366832522, w1=0.47713963715185287\n",
      "(250000,)\n",
      "Gradient Descent(415/1499): loss=1652.008986629835, w0=-0.09071550141852858, w1=0.4766372815450895\n",
      "(250000,)\n",
      "Gradient Descent(416/1499): loss=1648.0860934847701, w0=-0.09129451895803856, w1=0.4761357913603355\n",
      "(250000,)\n",
      "Gradient Descent(417/1499): loss=1644.1812796845486, w0=-0.09187171196537484, w1=0.47563516454646204\n",
      "(250000,)\n",
      "Gradient Descent(418/1499): loss=1640.2944315351453, w0=-0.09244708609879435, w1=0.47513539905618185\n",
      "(250000,)\n",
      "Gradient Descent(419/1499): loss=1636.4254363281225, w0=-0.09302064699639327, w1=0.474636492846062\n",
      "(250000,)\n",
      "Gradient Descent(420/1499): loss=1632.5741823271758, w0=-0.09359240027620847, w1=0.4741384438765366\n",
      "(250000,)\n",
      "Gradient Descent(421/1499): loss=1628.740558754933, w0=-0.09416235153631788, w1=0.47364125011191877\n",
      "(250000,)\n",
      "Gradient Descent(422/1499): loss=1624.9244557800073, w0=-0.0947305063549403, w1=0.4731449095204127\n",
      "(250000,)\n",
      "Gradient Descent(423/1499): loss=1621.1257645043072, w0=-0.09529687029053426, w1=0.4726494200741252\n",
      "(250000,)\n",
      "Gradient Descent(424/1499): loss=1617.3443769505784, w0=-0.09586144888189616, w1=0.4721547797490768\n",
      "(250000,)\n",
      "Gradient Descent(425/1499): loss=1613.580186050197, w0=-0.09642424764825774, w1=0.4716609865252131\n",
      "(250000,)\n",
      "Gradient Descent(426/1499): loss=1609.8330856311845, w0=-0.0969852720893827, w1=0.471168038386415\n",
      "(250000,)\n",
      "Gradient Descent(427/1499): loss=1606.102970406453, w0=-0.09754452768566246, w1=0.47067593332050944\n",
      "(250000,)\n",
      "Gradient Descent(428/1499): loss=1602.389735962277, w0=-0.0981020198982115, w1=0.4701846693192792\n",
      "(250000,)\n",
      "Gradient Descent(429/1499): loss=1598.6932787469807, w0=-0.09865775416896162, w1=0.4696942443784729\n",
      "(250000,)\n",
      "Gradient Descent(430/1499): loss=1595.0134960598334, w0=-0.09921173592075569, w1=0.4692046564978144\n",
      "(250000,)\n",
      "Gradient Descent(431/1499): loss=1591.350286040161, w0=-0.09976397055744059, w1=0.46871590368101207\n",
      "(250000,)\n",
      "Gradient Descent(432/1499): loss=1587.7035476566548, w0=-0.10031446346395949, w1=0.46822798393576776\n",
      "(250000,)\n",
      "Gradient Descent(433/1499): loss=1584.07318069688, w0=-0.10086322000644339, w1=0.46774089527378554\n",
      "(250000,)\n",
      "Gradient Descent(434/1499): loss=1580.4590857569842, w0=-0.10141024553230198, w1=0.4672546357107801\n",
      "(250000,)\n",
      "Gradient Descent(435/1499): loss=1576.8611642315873, w0=-0.10195554537031377, w1=0.4667692032664847\n",
      "(250000,)\n",
      "Gradient Descent(436/1499): loss=1573.2793183038648, w0=-0.10249912483071566, w1=0.46628459596465943\n",
      "(250000,)\n",
      "Gradient Descent(437/1499): loss=1569.713450935811, w0=-0.10304098920529163, w1=0.4658008118330986\n",
      "(250000,)\n",
      "Gradient Descent(438/1499): loss=1566.163465858677, w0=-0.10358114376746097, w1=0.4653178489036383\n",
      "(250000,)\n",
      "Gradient Descent(439/1499): loss=1562.6292675635893, w0=-0.10411959377236571, w1=0.4648357052121632\n",
      "(250000,)\n",
      "Gradient Descent(440/1499): loss=1559.1107612923324, w0=-0.10465634445695748, w1=0.4643543787986139\n",
      "(250000,)\n",
      "Gradient Descent(441/1499): loss=1555.6078530282987, w0=-0.10519140104008366, w1=0.4638738677069933\n",
      "(250000,)\n",
      "Gradient Descent(442/1499): loss=1552.1204494876088, w0=-0.1057247687225729, w1=0.463394169985373\n",
      "(250000,)\n",
      "Gradient Descent(443/1499): loss=1548.6484581103805, w0=-0.10625645268732005, w1=0.46291528368589957\n",
      "(250000,)\n",
      "Gradient Descent(444/1499): loss=1545.191787052161, w0=-0.10678645809937039, w1=0.4624372068648005\n",
      "(250000,)\n",
      "Gradient Descent(445/1499): loss=1541.7503451755085, w0=-0.10731479010600331, w1=0.4619599375823899\n",
      "(250000,)\n",
      "Gradient Descent(446/1499): loss=1538.324042041725, w0=-0.10784145383681533, w1=0.46148347390307387\n",
      "(250000,)\n",
      "Gradient Descent(447/1499): loss=1534.9127879027324, w0=-0.1083664544038025, w1=0.46100781389535594\n",
      "(250000,)\n",
      "Gradient Descent(448/1499): loss=1531.5164936930946, w0=-0.10888979690144225, w1=0.46053295563184193\n",
      "(250000,)\n",
      "Gradient Descent(449/1499): loss=1528.1350710221775, w0=-0.10941148640677456, w1=0.4600588971892449\n",
      "(250000,)\n",
      "Gradient Descent(450/1499): loss=1524.7684321664435, w0=-0.10993152797948262, w1=0.4595856366483898\n",
      "(250000,)\n",
      "Gradient Descent(451/1499): loss=1521.416490061887, w0=-0.11044992666197288, w1=0.45911317209421765\n",
      "(250000,)\n",
      "Gradient Descent(452/1499): loss=1518.0791582965924, w0=-0.11096668747945448, w1=0.4586415016157899\n",
      "(250000,)\n",
      "Gradient Descent(453/1499): loss=1514.7563511034236, w0=-0.11148181544001816, w1=0.45817062330629255\n",
      "(250000,)\n",
      "Gradient Descent(454/1499): loss=1511.4479833528421, w0=-0.11199531553471453, w1=0.4577005352630395\n",
      "(250000,)\n",
      "Gradient Descent(455/1499): loss=1508.1539705458413, w0=-0.11250719273763192, w1=0.45723123558747664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(456/1499): loss=1504.8742288070077, w0=-0.11301745200597348, w1=0.45676272238518484\n",
      "(250000,)\n",
      "Gradient Descent(457/1499): loss=1501.608674877693, w0=-0.11352609828013387, w1=0.45629499376588345\n",
      "(250000,)\n",
      "Gradient Descent(458/1499): loss=1498.3572261093047, w0=-0.11403313648377536, w1=0.455828047843433\n",
      "(250000,)\n",
      "Gradient Descent(459/1499): loss=1495.1198004567132, w0=-0.11453857152390338, w1=0.4553618827358383\n",
      "(250000,)\n",
      "Gradient Descent(460/1499): loss=1491.8963164717568, w0=-0.11504240829094156, w1=0.4548964965652508\n",
      "(250000,)\n",
      "Gradient Descent(461/1499): loss=1488.6866932968708, w0=-0.11554465165880617, w1=0.45443188745797125\n",
      "(250000,)\n",
      "Gradient Descent(462/1499): loss=1485.4908506588067, w0=-0.11604530648498018, w1=0.4539680535444519\n",
      "(250000,)\n",
      "Gradient Descent(463/1499): loss=1482.3087088624666, w0=-0.11654437761058663, w1=0.4535049929592986\n",
      "(250000,)\n",
      "Gradient Descent(464/1499): loss=1479.1401887848308, w0=-0.1170418698604616, w1=0.4530427038412725\n",
      "(250000,)\n",
      "Gradient Descent(465/1499): loss=1475.9852118689885, w0=-0.11753778804322662, w1=0.4525811843332922\n",
      "(250000,)\n",
      "Gradient Descent(466/1499): loss=1472.8437001182651, w0=-0.11803213695136061, w1=0.45212043258243495\n",
      "(250000,)\n",
      "Gradient Descent(467/1499): loss=1469.7155760904416, w0=-0.11852492136127128, w1=0.4516604467399382\n",
      "(250000,)\n",
      "Gradient Descent(468/1499): loss=1466.60076289207, w0=-0.11901614603336608, w1=0.4512012249612009\n",
      "(250000,)\n",
      "Gradient Descent(469/1499): loss=1463.499184172879, w0=-0.1195058157121226, w1=0.45074276540578445\n",
      "(250000,)\n",
      "Gradient Descent(470/1499): loss=1460.410764120268, w0=-0.11999393512615857, w1=0.4502850662374136\n",
      "(250000,)\n",
      "Gradient Descent(471/1499): loss=1457.3354274538872, w0=-0.12048050898830127, w1=0.44982812562397734\n",
      "(250000,)\n",
      "Gradient Descent(472/1499): loss=1454.2730994203066, w0=-0.12096554199565661, w1=0.4493719417375294\n",
      "(250000,)\n",
      "Gradient Descent(473/1499): loss=1451.2237057877653, w0=-0.12144903882967756, w1=0.4489165127542888\n",
      "(250000,)\n",
      "Gradient Descent(474/1499): loss=1448.1871728410056, w0=-0.12193100415623227, w1=0.44846183685463986\n",
      "(250000,)\n",
      "Gradient Descent(475/1499): loss=1445.1634273761804, w0=-0.12241144262567166, w1=0.4480079122231327\n",
      "(250000,)\n",
      "Gradient Descent(476/1499): loss=1442.1523966958518, w0=-0.12289035887289655, w1=0.44755473704848303\n",
      "(250000,)\n",
      "Gradient Descent(477/1499): loss=1439.1540086040554, w0=-0.12336775751742433, w1=0.44710230952357205\n",
      "(250000,)\n",
      "Gradient Descent(478/1499): loss=1436.1681914014412, w0=-0.12384364316345522, w1=0.4466506278454461\n",
      "(250000,)\n",
      "Gradient Descent(479/1499): loss=1433.1948738804979, w0=-0.12431802039993807, w1=0.44619969021531647\n",
      "(250000,)\n",
      "Gradient Descent(480/1499): loss=1430.2339853208364, w0=-0.12479089380063567, w1=0.44574949483855847\n",
      "(250000,)\n",
      "Gradient Descent(481/1499): loss=1427.2854554845528, w0=-0.12526226792418974, w1=0.44530003992471096\n",
      "(250000,)\n",
      "Gradient Descent(482/1499): loss=1424.3492146116587, w0=-0.1257321473141853, w1=0.44485132368747543\n",
      "(250000,)\n",
      "Gradient Descent(483/1499): loss=1421.4251934155789, w0=-0.1262005364992149, w1=0.4444033443447151\n",
      "(250000,)\n",
      "Gradient Descent(484/1499): loss=1418.5133230787153, w0=-0.12666743999294203, w1=0.4439561001184538\n",
      "(250000,)\n",
      "Gradient Descent(485/1499): loss=1415.6135352480783, w0=-0.12713286229416457, w1=0.4435095892348746\n",
      "(250000,)\n",
      "Gradient Descent(486/1499): loss=1412.7257620309801, w0=-0.12759680788687744, w1=0.4430638099243186\n",
      "(250000,)\n",
      "Gradient Descent(487/1499): loss=1409.8499359907908, w0=-0.128059281240335, w1=0.44261876042128345\n",
      "(250000,)\n",
      "Gradient Descent(488/1499): loss=1406.9859901427576, w0=-0.1285202868091131, w1=0.44217443896442166\n",
      "(250000,)\n",
      "Gradient Descent(489/1499): loss=1404.1338579498836, w0=-0.1289798290331706, w1=0.44173084379653893\n",
      "(250000,)\n",
      "Gradient Descent(490/1499): loss=1401.2934733188624, w0=-0.12943791233791055, w1=0.44128797316459234\n",
      "(250000,)\n",
      "Gradient Descent(491/1499): loss=1398.4647705960772, w0=-0.12989454113424098, w1=0.4408458253196883\n",
      "(250000,)\n",
      "Gradient Descent(492/1499): loss=1395.6476845636507, w0=-0.13034971981863533, w1=0.44040439851708046\n",
      "(250000,)\n",
      "Gradient Descent(493/1499): loss=1392.8421504355529, w0=-0.13080345277319239, w1=0.4399636910161677\n",
      "(250000,)\n",
      "Gradient Descent(494/1499): loss=1390.0481038537632, w0=-0.1312557443656959, w1=0.4395237010804917\n",
      "(250000,)\n",
      "Gradient Descent(495/1499): loss=1387.2654808844873, w0=-0.1317065989496739, w1=0.4390844269777344\n",
      "(250000,)\n",
      "Gradient Descent(496/1499): loss=1384.4942180144237, w0=-0.13215602086445746, w1=0.4386458669797158\n",
      "(250000,)\n",
      "Gradient Descent(497/1499): loss=1381.7342521470835, w0=-0.1326040144352392, w1=0.438208019362391\n",
      "(250000,)\n",
      "Gradient Descent(498/1499): loss=1378.9855205991616, w0=-0.13305058397313144, w1=0.43777088240584777\n",
      "(250000,)\n",
      "Gradient Descent(499/1499): loss=1376.2479610969524, w0=-0.1334957337752239, w1=0.4373344543943034\n",
      "(250000,)\n",
      "Gradient Descent(500/1499): loss=1373.5215117728214, w0=-0.13393946812464105, w1=0.43689873361610204\n",
      "(250000,)\n",
      "Gradient Descent(501/1499): loss=1370.8061111617192, w0=-0.13438179129059927, w1=0.4364637183637115\n",
      "(250000,)\n",
      "Gradient Descent(502/1499): loss=1368.10169819774, w0=-0.13482270752846331, w1=0.4360294069337203\n",
      "(250000,)\n",
      "Gradient Descent(503/1499): loss=1365.4082122107357, w0=-0.1352622210798028, w1=0.4355957976268341\n",
      "(250000,)\n",
      "Gradient Descent(504/1499): loss=1362.725592922962, w0=-0.135700336172448, w1=0.4351628887478728\n",
      "(250000,)\n",
      "Gradient Descent(505/1499): loss=1360.0537804457776, w0=-0.13613705702054563, w1=0.43473067860576675\n",
      "(250000,)\n",
      "Gradient Descent(506/1499): loss=1357.392715276383, w0=-0.13657238782461395, w1=0.43429916551355346\n",
      "(250000,)\n",
      "Gradient Descent(507/1499): loss=1354.7423382946035, w0=-0.13700633277159777, w1=0.43386834778837396\n",
      "(250000,)\n",
      "Gradient Descent(508/1499): loss=1352.1025907597111, w0=-0.13743889603492304, w1=0.433438223751469\n",
      "(250000,)\n",
      "Gradient Descent(509/1499): loss=1349.4734143072894, w0=-0.1378700817745511, w1=0.4330087917281754\n",
      "(250000,)\n",
      "Gradient Descent(510/1499): loss=1346.8547509461378, w0=-0.13829989413703253, w1=0.43258005004792216\n",
      "(250000,)\n",
      "Gradient Descent(511/1499): loss=1344.2465430552145, w0=-0.13872833725556086, w1=0.4321519970442264\n",
      "(250000,)\n",
      "Gradient Descent(512/1499): loss=1341.6487333806194, w0=-0.13915541525002573, w1=0.43172463105468944\n",
      "(250000,)\n",
      "Gradient Descent(513/1499): loss=1339.0612650326118, w0=-0.1395811322270659, w1=0.43129795042099267\n",
      "(250000,)\n",
      "Gradient Descent(514/1499): loss=1336.4840814826714, w0=-0.14000549228012182, w1=0.4308719534888933\n",
      "(250000,)\n",
      "Gradient Descent(515/1499): loss=1333.917126560586, w0=-0.14042849948948805, w1=0.4304466386082201\n",
      "(250000,)\n",
      "Gradient Descent(516/1499): loss=1331.3603444515848, w0=-0.14085015792236505, w1=0.43002200413286906\n",
      "(250000,)\n",
      "Gradient Descent(517/1499): loss=1328.8136796935034, w0=-0.141270471632911, w1=0.429598048420799\n",
      "(250000,)\n",
      "Gradient Descent(518/1499): loss=1326.2770771739772, w0=-0.14168944466229325, w1=0.42917476983402697\n",
      "(250000,)\n",
      "Gradient Descent(519/1499): loss=1323.7504821276798, w0=-0.14210708103873912, w1=0.4287521667386237\n",
      "(250000,)\n",
      "Gradient Descent(520/1499): loss=1321.2338401335871, w0=-0.14252338477758697, w1=0.428330237504709\n",
      "(250000,)\n",
      "Gradient Descent(521/1499): loss=1318.7270971122714, w0=-0.14293835988133644, w1=0.42790898050644693\n",
      "(250000,)\n",
      "Gradient Descent(522/1499): loss=1316.23019932324, w0=-0.1433520103396987, w1=0.4274883941220411\n",
      "(250000,)\n",
      "Gradient Descent(523/1499): loss=1313.743093362291, w0=-0.1437643401296463, w1=0.4270684767337297\n",
      "(250000,)\n",
      "Gradient Descent(524/1499): loss=1311.2657261589075, w0=-0.14417535321546285, w1=0.4266492267277808\n",
      "(250000,)\n",
      "Gradient Descent(525/1499): loss=1308.7980449736845, w0=-0.14458505354879211, w1=0.42623064249448683\n",
      "(250000,)\n",
      "Gradient Descent(526/1499): loss=1306.3399973957764, w0=-0.14499344506868717, w1=0.4258127224281602\n",
      "(250000,)\n",
      "Gradient Descent(527/1499): loss=1303.8915313403852, w0=-0.14540053170165904, w1=0.4253954649271276\n",
      "(250000,)\n",
      "Gradient Descent(528/1499): loss=1301.4525950462669, w0=-0.14580631736172517, w1=0.4249788683937251\n",
      "(250000,)\n",
      "Gradient Descent(529/1499): loss=1299.023137073276, w0=-0.14621080595045755, w1=0.42456293123429284\n",
      "(250000,)\n",
      "Gradient Descent(530/1499): loss=1296.6031062999293, w0=-0.14661400135703065, w1=0.4241476518591696\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(531/1499): loss=1294.1924519210024, w0=-0.14701590745826892, w1=0.42373302868268753\n",
      "(250000,)\n",
      "Gradient Descent(532/1499): loss=1291.7911234451522, w0=-0.14741652811869427, w1=0.42331906012316667\n",
      "(250000,)\n",
      "Gradient Descent(533/1499): loss=1289.3990706925633, w0=-0.14781586719057294, w1=0.42290574460290953\n",
      "(250000,)\n",
      "Gradient Descent(534/1499): loss=1287.016243792624, w0=-0.14821392851396245, w1=0.42249308054819534\n",
      "(250000,)\n",
      "Gradient Descent(535/1499): loss=1284.6425931816239, w0=-0.1486107159167581, w1=0.4220810663892746\n",
      "(250000,)\n",
      "Gradient Descent(536/1499): loss=1282.2780696004809, w0=-0.1490062332147391, w1=0.42166970056036346\n",
      "(250000,)\n",
      "Gradient Descent(537/1499): loss=1279.9226240924904, w0=-0.14940048421161475, w1=0.4212589814996378\n",
      "(250000,)\n",
      "Gradient Descent(538/1499): loss=1277.5762080010975, w0=-0.14979347269907012, w1=0.42084890764922767\n",
      "(250000,)\n",
      "Gradient Descent(539/1499): loss=1275.2387729676952, w0=-0.15018520245681147, w1=0.4204394774552113\n",
      "(250000,)\n",
      "Gradient Descent(540/1499): loss=1272.910270929447, w0=-0.15057567725261156, w1=0.4200306893676093\n",
      "(250000,)\n",
      "Gradient Descent(541/1499): loss=1270.5906541171303, w0=-0.15096490084235467, w1=0.4196225418403787\n",
      "(250000,)\n",
      "Gradient Descent(542/1499): loss=1268.2798750530014, w0=-0.15135287697008118, w1=0.4192150333314071\n",
      "(250000,)\n",
      "Gradient Descent(543/1499): loss=1265.9778865486867, w0=-0.1517396093680322, w1=0.41880816230250634\n",
      "(250000,)\n",
      "Gradient Descent(544/1499): loss=1263.6846417030954, w0=-0.1521251017566938, w1=0.41840192721940683\n",
      "(250000,)\n",
      "Gradient Descent(545/1499): loss=1261.400093900348, w0=-0.15250935784484085, w1=0.4179963265517511\n",
      "(250000,)\n",
      "Gradient Descent(546/1499): loss=1259.1241968077327, w0=-0.152892381329581, w1=0.4175913587730879\n",
      "(250000,)\n",
      "Gradient Descent(547/1499): loss=1256.8569043736834, w0=-0.153274175896398, w1=0.41718702236086574\n",
      "(250000,)\n",
      "Gradient Descent(548/1499): loss=1254.5981708257696, w0=-0.1536547452191951, w1=0.41678331579642686\n",
      "(250000,)\n",
      "Gradient Descent(549/1499): loss=1252.3479506687156, w0=-0.15403409296033801, w1=0.4163802375650007\n",
      "(250000,)\n",
      "Gradient Descent(550/1499): loss=1250.1061986824352, w0=-0.15441222277069783, w1=0.41597778615569786\n",
      "(250000,)\n",
      "Gradient Descent(551/1499): loss=1247.8728699200906, w0=-0.15478913828969346, w1=0.4155759600615035\n",
      "(250000,)\n",
      "Gradient Descent(552/1499): loss=1245.6479197061599, w0=-0.15516484314533407, w1=0.41517475777927093\n",
      "(250000,)\n",
      "Gradient Descent(553/1499): loss=1243.4313036345382, w0=-0.1555393409542612, w1=0.4147741778097153\n",
      "(250000,)\n",
      "Gradient Descent(554/1499): loss=1241.222977566647, w0=-0.1559126353217906, w1=0.41437421865740703\n",
      "(250000,)\n",
      "Gradient Descent(555/1499): loss=1239.022897629563, w0=-0.15628472984195396, w1=0.4139748788307651\n",
      "(250000,)\n",
      "Gradient Descent(556/1499): loss=1236.8310202141718, w0=-0.15665562809754036, w1=0.41357615684205085\n",
      "(250000,)\n",
      "Gradient Descent(557/1499): loss=1234.6473019733323, w0=-0.1570253336601374, w1=0.413178051207361\n",
      "(250000,)\n",
      "Gradient Descent(558/1499): loss=1232.4716998200602, w0=-0.1573938500901724, w1=0.4127805604466213\n",
      "(250000,)\n",
      "Gradient Descent(559/1499): loss=1230.3041709257336, w0=-0.15776118093695296, w1=0.41238368308357964\n",
      "(250000,)\n",
      "Gradient Descent(560/1499): loss=1228.144672718308, w0=-0.15812732973870774, w1=0.41198741764579944\n",
      "(250000,)\n",
      "Gradient Descent(561/1499): loss=1225.9931628805548, w0=-0.15849230002262674, w1=0.4115917626646529\n",
      "(250000,)\n",
      "Gradient Descent(562/1499): loss=1223.849599348313, w0=-0.1588560953049014, w1=0.41119671667531427\n",
      "(250000,)\n",
      "Gradient Descent(563/1499): loss=1221.7139403087579, w0=-0.1592187190907647, w1=0.41080227821675286\n",
      "(250000,)\n",
      "Gradient Descent(564/1499): loss=1219.5861441986872, w0=-0.15958017487453074, w1=0.4104084458317265\n",
      "(250000,)\n",
      "Gradient Descent(565/1499): loss=1217.466169702819, w0=-0.15994046613963434, w1=0.41001521806677427\n",
      "(250000,)\n",
      "Gradient Descent(566/1499): loss=1215.353975752111, w0=-0.16029959635867036, w1=0.40962259347221003\n",
      "(250000,)\n",
      "Gradient Descent(567/1499): loss=1213.2495215220913, w0=-0.16065756899343284, w1=0.4092305706021152\n",
      "(250000,)\n",
      "Gradient Descent(568/1499): loss=1211.1527664312052, w0=-0.1610143874949539, w1=0.4088391480143319\n",
      "(250000,)\n",
      "Gradient Descent(569/1499): loss=1209.0636701391775, w0=-0.16137005530354248, w1=0.40844832427045596\n",
      "(250000,)\n",
      "Gradient Descent(570/1499): loss=1206.9821925453894, w0=-0.16172457584882285, w1=0.40805809793582987\n",
      "(250000,)\n",
      "Gradient Descent(571/1499): loss=1204.9082937872686, w0=-0.16207795254977297, w1=0.40766846757953584\n",
      "(250000,)\n",
      "Gradient Descent(572/1499): loss=1202.841934238697, w0=-0.16243018881476257, w1=0.4072794317743886\n",
      "(250000,)\n",
      "Gradient Descent(573/1499): loss=1200.7830745084293, w0=-0.16278128804159112, w1=0.4068909890969284\n",
      "(250000,)\n",
      "Gradient Descent(574/1499): loss=1198.7316754385263, w0=-0.16313125361752562, w1=0.4065031381274139\n",
      "(250000,)\n",
      "Gradient Descent(575/1499): loss=1196.6876981028022, w0=-0.1634800889193381, w1=0.40611587744981503\n",
      "(250000,)\n",
      "Gradient Descent(576/1499): loss=1194.6511038052886, w0=-0.16382779731334302, w1=0.40572920565180576\n",
      "(250000,)\n",
      "Gradient Descent(577/1499): loss=1192.6218540787038, w0=-0.1641743821554344, w1=0.4053431213247569\n",
      "(250000,)\n",
      "Gradient Descent(578/1499): loss=1190.5999106829463, w0=-0.16451984679112291, w1=0.4049576230637292\n",
      "(250000,)\n",
      "Gradient Descent(579/1499): loss=1188.5852356035919, w0=-0.16486419455557266, w1=0.40457270946746554\n",
      "(250000,)\n",
      "Gradient Descent(580/1499): loss=1186.5777910504057, w0=-0.16520742877363775, w1=0.4041883791383843\n",
      "(250000,)\n",
      "Gradient Descent(581/1499): loss=1184.5775394558734, w0=-0.1655495527598988, w1=0.40380463068257166\n",
      "(250000,)\n",
      "Gradient Descent(582/1499): loss=1182.5844434737353, w0=-0.16589056981869918, w1=0.4034214627097746\n",
      "(250000,)\n",
      "Gradient Descent(583/1499): loss=1180.5984659775397, w0=-0.16623048324418113, w1=0.40303887383339326\n",
      "(250000,)\n",
      "Gradient Descent(584/1499): loss=1178.619570059207, w0=-0.16656929632032164, w1=0.40265686267047396\n",
      "(250000,)\n",
      "Gradient Descent(585/1499): loss=1176.6477190276019, w0=-0.16690701232096822, w1=0.40227542784170167\n",
      "(250000,)\n",
      "Gradient Descent(586/1499): loss=1174.682876407123, w0=-0.16724363450987445, w1=0.40189456797139267\n",
      "(250000,)\n",
      "Gradient Descent(587/1499): loss=1172.7250059363016, w0=-0.16757916614073534, w1=0.40151428168748715\n",
      "(250000,)\n",
      "Gradient Descent(588/1499): loss=1170.7740715664102, w0=-0.16791361045722258, w1=0.40113456762154187\n",
      "(250000,)\n",
      "Gradient Descent(589/1499): loss=1168.8300374600874, w0=-0.1682469706930196, w1=0.40075542440872275\n",
      "(250000,)\n",
      "Gradient Descent(590/1499): loss=1166.8928679899682, w0=-0.16857925007185642, w1=0.4003768506877974\n",
      "(250000,)\n",
      "Gradient Descent(591/1499): loss=1164.962527737331, w0=-0.16891045180754433, w1=0.3999988451011276\n",
      "(250000,)\n",
      "Gradient Descent(592/1499): loss=1163.038981490748, w0=-0.16924057910401052, w1=0.39962140629466203\n",
      "(250000,)\n",
      "Gradient Descent(593/1499): loss=1161.1221942447596, w0=-0.16956963515533238, w1=0.39924453291792866\n",
      "(250000,)\n",
      "Gradient Descent(594/1499): loss=1159.2121311985413, w0=-0.16989762314577175, w1=0.39886822362402735\n",
      "(250000,)\n",
      "Gradient Descent(595/1499): loss=1157.3087577546005, w0=-0.17022454624980896, w1=0.3984924770696222\n",
      "(250000,)\n",
      "Gradient Descent(596/1499): loss=1155.412039517468, w0=-0.1705504076321767, w1=0.3981172919149343\n",
      "(250000,)\n",
      "Gradient Descent(597/1499): loss=1153.5219422924083, w0=-0.17087521044789378, w1=0.39774266682373377\n",
      "(250000,)\n",
      "Gradient Descent(598/1499): loss=1151.6384320841396, w0=-0.1711989578422987, w1=0.3973686004633328\n",
      "(250000,)\n",
      "Gradient Descent(599/1499): loss=1149.7614750955597, w0=-0.17152165295108301, w1=0.39699509150457757\n",
      "(250000,)\n",
      "Gradient Descent(600/1499): loss=1147.891037726486, w0=-0.17184329890032465, w1=0.39662213862184104\n",
      "(250000,)\n",
      "Gradient Descent(601/1499): loss=1146.0270865724053, w0=-0.17216389880652094, w1=0.3962497404930152\n",
      "(250000,)\n",
      "Gradient Descent(602/1499): loss=1144.1695884232292, w0=-0.1724834557766216, w1=0.3958778957995036\n",
      "(250000,)\n",
      "Gradient Descent(603/1499): loss=1142.3185102620625, w0=-0.17280197290806154, w1=0.39550660322621367\n",
      "(250000,)\n",
      "Gradient Descent(604/1499): loss=1140.4738192639823, w0=-0.17311945328879347, w1=0.3951358614615491\n",
      "(250000,)\n",
      "Gradient Descent(605/1499): loss=1138.6354827948226, w0=-0.17343589999732034, w1=0.3947656691974024\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(606/1499): loss=1136.8034684099732, w0=-0.1737513161027278, w1=0.3943960251291471\n",
      "(250000,)\n",
      "Gradient Descent(607/1499): loss=1134.977743853181, w0=-0.17406570466471624, w1=0.39402692795563027\n",
      "(250000,)\n",
      "Gradient Descent(608/1499): loss=1133.1582770553657, w0=-0.17437906873363296, w1=0.3936583763791647\n",
      "(250000,)\n",
      "Gradient Descent(609/1499): loss=1131.3450361334453, w0=-0.17469141135050398, w1=0.3932903691055214\n",
      "(250000,)\n",
      "Gradient Descent(610/1499): loss=1129.537989389165, w0=-0.17500273554706577, w1=0.39292290484392206\n",
      "(250000,)\n",
      "Gradient Descent(611/1499): loss=1127.7371053079407, w0=-0.17531304434579698, w1=0.3925559823070311\n",
      "(250000,)\n",
      "Gradient Descent(612/1499): loss=1125.942352557707, w0=-0.1756223407599497, w1=0.3921896002109482\n",
      "(250000,)\n",
      "Gradient Descent(613/1499): loss=1124.153699987776, w0=-0.17593062779358093, w1=0.39182375727520063\n",
      "(250000,)\n",
      "Gradient Descent(614/1499): loss=1122.3711166277026, w0=-0.17623790844158368, w1=0.3914584522227356\n",
      "(250000,)\n",
      "Gradient Descent(615/1499): loss=1120.5945716861643, w0=-0.17654418568971805, w1=0.3910936837799125\n",
      "(250000,)\n",
      "Gradient Descent(616/1499): loss=1118.8240345498373, w0=-0.17684946251464206, w1=0.39072945067649517\n",
      "(250000,)\n",
      "Gradient Descent(617/1499): loss=1117.0594747822947, w0=-0.17715374188394245, w1=0.39036575164564435\n",
      "(250000,)\n",
      "Gradient Descent(618/1499): loss=1115.3008621229021, w0=-0.17745702675616526, w1=0.39000258542391003\n",
      "(250000,)\n",
      "Gradient Descent(619/1499): loss=1113.5481664857284, w0=-0.17775932008084636, w1=0.3896399507512235\n",
      "(250000,)\n",
      "Gradient Descent(620/1499): loss=1111.8013579584592, w0=-0.17806062479854173, w1=0.3892778463708899\n",
      "(250000,)\n",
      "Gradient Descent(621/1499): loss=1110.060406801322, w0=-0.1783609438408577, w1=0.3889162710295805\n",
      "(250000,)\n",
      "Gradient Descent(622/1499): loss=1108.3252834460166, w0=-0.17866028013048105, w1=0.38855522347732474\n",
      "(250000,)\n",
      "Gradient Descent(623/1499): loss=1106.5959584946554, w0=-0.1789586365812089, w1=0.3881947024675029\n",
      "(250000,)\n",
      "Gradient Descent(624/1499): loss=1104.8724027187095, w0=-0.17925601609797848, w1=0.38783470675683795\n",
      "(250000,)\n",
      "Gradient Descent(625/1499): loss=1103.1545870579635, w0=-0.17955242157689696, w1=0.3874752351053884\n",
      "(250000,)\n",
      "Gradient Descent(626/1499): loss=1101.4424826194772, w0=-0.17984785590527086, w1=0.38711628627654004\n",
      "(250000,)\n",
      "Gradient Descent(627/1499): loss=1099.736060676555, w0=-0.1801423219616355, w1=0.3867578590369986\n",
      "(250000,)\n",
      "Gradient Descent(628/1499): loss=1098.0352926677226, w0=-0.18043582261578425, w1=0.3863999521567818\n",
      "(250000,)\n",
      "Gradient Descent(629/1499): loss=1096.3401501957096, w0=-0.18072836072879783, w1=0.38604256440921175\n",
      "(250000,)\n",
      "Gradient Descent(630/1499): loss=1094.650605026444, w0=-0.18101993915307318, w1=0.38568569457090723\n",
      "(250000,)\n",
      "Gradient Descent(631/1499): loss=1092.9666290880464, w0=-0.1813105607323525, w1=0.38532934142177594\n",
      "(250000,)\n",
      "Gradient Descent(632/1499): loss=1091.288194469837, w0=-0.18160022830175188, w1=0.38497350374500683\n",
      "(250000,)\n",
      "Gradient Descent(633/1499): loss=1089.6152734213497, w0=-0.1818889446877901, w1=0.38461818032706224\n",
      "(250000,)\n",
      "Gradient Descent(634/1499): loss=1087.9478383513467, w0=-0.18217671270841715, w1=0.38426336995767046\n",
      "(250000,)\n",
      "Gradient Descent(635/1499): loss=1086.2858618268494, w0=-0.18246353517304256, w1=0.38390907142981767\n",
      "(250000,)\n",
      "Gradient Descent(636/1499): loss=1084.629316572168, w0=-0.18274941488256374, w1=0.38355528353974055\n",
      "(250000,)\n",
      "Gradient Descent(637/1499): loss=1082.978175467942, w0=-0.18303435462939416, w1=0.3832020050869183\n",
      "(250000,)\n",
      "Gradient Descent(638/1499): loss=1081.3324115501864, w0=-0.18331835719749134, w1=0.3828492348740652\n",
      "(250000,)\n",
      "Gradient Descent(639/1499): loss=1079.6919980093444, w0=-0.1836014253623849, w1=0.38249697170712255\n",
      "(250000,)\n",
      "Gradient Descent(640/1499): loss=1078.0569081893457, w0=-0.1838835618912043, w1=0.3821452143952513\n",
      "(250000,)\n",
      "Gradient Descent(641/1499): loss=1076.4271155866747, w0=-0.18416476954270647, w1=0.38179396175082414\n",
      "(250000,)\n",
      "Gradient Descent(642/1499): loss=1074.802593849439, w0=-0.18444505106730347, w1=0.38144321258941793\n",
      "(250000,)\n",
      "Gradient Descent(643/1499): loss=1073.1833167764519, w0=-0.18472440920708993, w1=0.3810929657298059\n",
      "(250000,)\n",
      "Gradient Descent(644/1499): loss=1071.5692583163122, w0=-0.18500284669587042, w1=0.3807432199939499\n",
      "(250000,)\n",
      "Gradient Descent(645/1499): loss=1069.9603925665003, w0=-0.18528036625918665, w1=0.3803939742069929\n",
      "(250000,)\n",
      "Gradient Descent(646/1499): loss=1068.356693772472, w0=-0.18555697061434456, w1=0.38004522719725115\n",
      "(250000,)\n",
      "Gradient Descent(647/1499): loss=1066.7581363267634, w0=-0.1858326624704414, w1=0.3796969777962065\n",
      "(250000,)\n",
      "Gradient Descent(648/1499): loss=1065.1646947680981, w0=-0.18610744452839253, w1=0.3793492248384988\n",
      "(250000,)\n",
      "Gradient Descent(649/1499): loss=1063.5763437805065, w0=-0.18638131948095832, w1=0.37900196716191814\n",
      "(250000,)\n",
      "Gradient Descent(650/1499): loss=1061.9930581924411, w0=-0.18665429001277065, w1=0.37865520360739724\n",
      "(250000,)\n",
      "Gradient Descent(651/1499): loss=1060.4148129759103, w0=-0.18692635880035963, w1=0.37830893301900365\n",
      "(250000,)\n",
      "Gradient Descent(652/1499): loss=1058.8415832456064, w0=-0.18719752851217994, w1=0.3779631542439323\n",
      "(250000,)\n",
      "Gradient Descent(653/1499): loss=1057.273344258046, w0=-0.18746780180863723, w1=0.37761786613249765\n",
      "(250000,)\n",
      "Gradient Descent(654/1499): loss=1055.7100714107164, w0=-0.1877371813421143, w1=0.3772730675381261\n",
      "(250000,)\n",
      "Gradient Descent(655/1499): loss=1054.151740241222, w0=-0.18800566975699734, w1=0.37692875731734843\n",
      "(250000,)\n",
      "Gradient Descent(656/1499): loss=1052.598326426444, w0=-0.1882732696897017, w1=0.37658493432979195\n",
      "(250000,)\n",
      "Gradient Descent(657/1499): loss=1051.0498057816994, w0=-0.1885399837686981, w1=0.37624159743817304\n",
      "(250000,)\n",
      "Gradient Descent(658/1499): loss=1049.5061542599094, w0=-0.18880581461453821, w1=0.3758987455082894\n",
      "(250000,)\n",
      "Gradient Descent(659/1499): loss=1047.9673479507699, w0=-0.18907076483988047, w1=0.37555637740901254\n",
      "(250000,)\n",
      "Gradient Descent(660/1499): loss=1046.4333630799333, w0=-0.18933483704951565, w1=0.37521449201228013\n",
      "(250000,)\n",
      "Gradient Descent(661/1499): loss=1044.9041760081882, w0=-0.18959803384039228, w1=0.37487308819308823\n",
      "(250000,)\n",
      "Gradient Descent(662/1499): loss=1043.3797632306494, w0=-0.18986035780164218, w1=0.3745321648294839\n",
      "(250000,)\n",
      "Gradient Descent(663/1499): loss=1041.8601013759517, w0=-0.19012181151460558, w1=0.3741917208025575\n",
      "(250000,)\n",
      "Gradient Descent(664/1499): loss=1040.3451672054512, w0=-0.19038239755285644, w1=0.3738517549964351\n",
      "(250000,)\n",
      "Gradient Descent(665/1499): loss=1038.834937612427, w0=-0.19064211848222745, w1=0.3735122662982708\n",
      "(250000,)\n",
      "Gradient Descent(666/1499): loss=1037.3293896212929, w0=-0.1909009768608351, w1=0.37317325359823933\n",
      "(250000,)\n",
      "Gradient Descent(667/1499): loss=1035.8285003868102, w0=-0.1911589752391044, w1=0.37283471578952837\n",
      "(250000,)\n",
      "Gradient Descent(668/1499): loss=1034.332247193311, w0=-0.19141611615979387, w1=0.37249665176833097\n",
      "(250000,)\n",
      "Gradient Descent(669/1499): loss=1032.8406074539205, w0=-0.19167240215802006, w1=0.37215906043383806\n",
      "(250000,)\n",
      "Gradient Descent(670/1499): loss=1031.3535587097886, w0=-0.19192783576128225, w1=0.3718219406882309\n",
      "(250000,)\n",
      "Gradient Descent(671/1499): loss=1029.8710786293227, w0=-0.19218241948948686, w1=0.37148529143667336\n",
      "(250000,)\n",
      "Gradient Descent(672/1499): loss=1028.3931450074306, w0=-0.19243615585497187, w1=0.37114911158730474\n",
      "(250000,)\n",
      "Gradient Descent(673/1499): loss=1026.9197357647652, w0=-0.19268904736253115, w1=0.3708134000512318\n",
      "(250000,)\n",
      "Gradient Descent(674/1499): loss=1025.4508289469704, w0=-0.19294109650943858, w1=0.3704781557425217\n",
      "(250000,)\n",
      "Gradient Descent(675/1499): loss=1023.9864027239406, w0=-0.19319230578547228, w1=0.3701433775781941\n",
      "(250000,)\n",
      "Gradient Descent(676/1499): loss=1022.5264353890772, w0=-0.19344267767293846, w1=0.3698090644782139\n",
      "(250000,)\n",
      "Gradient Descent(677/1499): loss=1021.070905358553, w0=-0.1936922146466955, w1=0.3694752153654836\n",
      "(250000,)\n",
      "Gradient Descent(678/1499): loss=1019.6197911705815, w0=-0.19394091917417766, w1=0.369141829165836\n",
      "(250000,)\n",
      "Gradient Descent(679/1499): loss=1018.1730714846914, w0=-0.19418879371541886, w1=0.3688089048080265\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import gradient_descent\n",
    "from costs import compute_loss\n",
    "initial_w=np.ones((30,))\n",
    "gradient_descent(y, tX, initial_w, 1500,10**(-6.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
