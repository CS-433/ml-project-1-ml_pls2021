{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a,b)=np.shape(tX)\n",
    "for i in range(b):\n",
    "    ind=np.where(tX[:,i]==-999)\n",
    "    tX_del=np.delete(tX[:,i],ind)\n",
    "    mean=np.mean(tX_del)\n",
    "    tX[ind,i]=mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from standardize import standardize\n",
    "tX=standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.14910656e-01  6.83319669e-02  4.07680272e-01 ...  1.14381874e+00\n",
      "  -2.52714288e+00  4.12510497e-01]\n",
      " [ 7.40827026e-01  5.52504823e-01  5.40136414e-01 ...  2.27784307e-14\n",
      "  -9.12411141e-15 -2.73819964e-01]\n",
      " [-9.69574743e-13  3.19515553e+00  1.09655998e+00 ...  2.27784307e-14\n",
      "  -9.12411141e-15 -2.93969845e-01]\n",
      " ...\n",
      " [-3.10930673e-01  3.19316447e-01 -1.30863670e-01 ...  2.27784307e-14\n",
      "  -9.12411141e-15 -3.17017229e-01]\n",
      " [-5.10097335e-01 -8.45323970e-01 -3.02973380e-01 ...  2.27784307e-14\n",
      "  -9.12411141e-15 -7.45439413e-01]\n",
      " [-9.69574743e-13  6.65336083e-01 -2.53522760e-01 ...  2.27784307e-14\n",
      "  -9.12411141e-15 -7.45439413e-01]]\n"
     ]
    }
   ],
   "source": [
    "np.shape(tX)\n",
    "print(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(tX!=-999.00,tX,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_=np.eye(5)\n",
    "id_[:,np.where(id_[:,2]!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(0/2999): loss=0.5, w0=0.002087025078864423, w1=-0.06671530258311346\n",
      "(250000,)\n",
      "Gradient Descent(1/2999): loss=0.4380272052145128, w0=0.0036957521960929735, w1=-0.10372799781426907\n",
      "(250000,)\n",
      "Gradient Descent(2/2999): loss=0.4211474497256975, w0=0.004964820789990872, w1=-0.13017469900971534\n",
      "(250000,)\n",
      "Gradient Descent(3/2999): loss=0.41412290113944905, w0=0.005000258794915865, w1=-0.14852081408286527\n",
      "(250000,)\n",
      "Gradient Descent(4/2999): loss=0.41033434541972397, w0=0.004198481058692314, w1=-0.16219539254299803\n",
      "(250000,)\n",
      "Gradient Descent(5/2999): loss=0.40791606588398444, w0=0.002818124158119527, w1=-0.17262316600830285\n",
      "(250000,)\n",
      "Gradient Descent(6/2999): loss=0.4061759466328463, w0=0.001119835045295737, w1=-0.18084361393969\n",
      "(250000,)\n",
      "Gradient Descent(7/2999): loss=0.40481478523502973, w0=-0.0007235818124710915, w1=-0.18747896740977482\n",
      "(250000,)\n",
      "Gradient Descent(8/2999): loss=0.4036892916149945, w0=-0.002596011562660495, w1=-0.19295462688853818\n",
      "(250000,)\n",
      "Gradient Descent(9/2999): loss=0.40272437913765485, w0=-0.004426320906815309, w1=-0.1975577114861946\n",
      "(250000,)\n",
      "Gradient Descent(10/2999): loss=0.40187713436666084, w0=-0.00617267174951908, w1=-0.2014902083751662\n",
      "(250000,)\n",
      "Gradient Descent(11/2999): loss=0.40112097303810224, w0=-0.007812461954534285, w1=-0.2048965771940658\n",
      "(250000,)\n",
      "Gradient Descent(12/2999): loss=0.40043823364996917, w0=-0.009335117742855122, w1=-0.2078826145399258\n",
      "(250000,)\n",
      "Gradient Descent(13/2999): loss=0.3998164878911367, w0=-0.010737460815775762, w1=-0.21052729295770922\n",
      "(250000,)\n",
      "Gradient Descent(14/2999): loss=0.3992465773944318, w0=-0.012020718720295608, w1=-0.21289069051823922\n",
      "(250000,)\n",
      "Gradient Descent(15/2999): loss=0.3987214998055432, w0=-0.013188651115776882, w1=-0.2150193007609129\n",
      "(250000,)\n",
      "Gradient Descent(16/2999): loss=0.39823573879910873, w0=-0.014246386659369574, w1=-0.21694967813854574\n",
      "(250000,)\n",
      "Gradient Descent(17/2999): loss=0.3977848404064451, w0=-0.015199712883963927, w1=-0.21871097629477332\n",
      "(250000,)\n",
      "Gradient Descent(18/2999): loss=0.3973651336852079, w0=-0.016054649468410363, w1=-0.2203267475321988\n",
      "(250000,)\n",
      "Gradient Descent(19/2999): loss=0.3969735400847738, w0=-0.016817197607820016, w1=-0.22181623929956637\n",
      "(250000,)\n",
      "Gradient Descent(20/2999): loss=0.3966074395236799, w0=-0.017493197660149316, w1=-0.22319534370354843\n",
      "(250000,)\n",
      "Gradient Descent(21/2999): loss=0.3962645739426314, w0=-0.01808825273813517, w1=-0.22447730379471376\n",
      "(250000,)\n",
      "Gradient Descent(22/2999): loss=0.3959429763120088, w0=-0.018607691966403567, w1=-0.22567324677667053\n",
      "(250000,)\n",
      "Gradient Descent(23/2999): loss=0.3956409173391004, w0=-0.01905655721200286, w1=-0.22679259215801598\n",
      "(250000,)\n",
      "Gradient Descent(24/2999): loss=0.3953568647380303, w0=-0.019439603380475026, w1=-0.22784336819731554\n",
      "(250000,)\n",
      "Gradient Descent(25/2999): loss=0.39508945158197917, w0=-0.0197613062663699, w1=-0.22883246012952396\n",
      "(250000,)\n",
      "Gradient Descent(26/2999): loss=0.3948374513330905, w0=-0.020025874349446034, w1=-0.2297658069552983\n",
      "(250000,)\n",
      "Gradient Descent(27/2999): loss=0.3945997578595006, w0=-0.02023726240071291, w1=-0.23064855895271183\n",
      "(250000,)\n",
      "Gradient Descent(28/2999): loss=0.39437536923190386, w0=-0.02039918565953667, w1=-0.2314852048438418\n",
      "(250000,)\n",
      "Gradient Descent(29/2999): loss=0.39416337442428934, w0=-0.020515133885012974, w1=-0.2322796752650646\n",
      "(250000,)\n",
      "Gradient Descent(30/2999): loss=0.39396294227549106, w0=-0.02058838490884423, w1=-0.23303542755207912\n",
      "(250000,)\n",
      "Gradient Descent(31/2999): loss=0.39377331223252954, w0=-0.020622017507956884, w1=-0.23375551566044125\n",
      "(250000,)\n",
      "Gradient Descent(32/2999): loss=0.3935937865146482, w0=-0.020618923525429378, w1=-0.23444264816629387\n",
      "(250000,)\n",
      "Gradient Descent(33/2999): loss=0.3934237234226433, w0=-0.020581819230142993, w1=-0.23509923663918836\n",
      "(250000,)\n",
      "Gradient Descent(34/2999): loss=0.39326253158107854, w0=-0.02051325593858565, w1=-0.2357274361868738\n",
      "(250000,)\n",
      "Gradient Descent(35/2999): loss=0.39310966494782196, w0=-0.020415629938388743, w1=-0.23632917959709315\n",
      "(250000,)\n",
      "Gradient Descent(36/2999): loss=0.39296461846053815, w0=-0.02029119175967396, w1=-0.2369062062130276\n",
      "(250000,)\n",
      "Gradient Descent(37/2999): loss=0.39282692421648624, w0=-0.020142054841442797, w1=-0.2374600864551214\n",
      "(250000,)\n",
      "Gradient Descent(38/2999): loss=0.39269614810245634, w0=-0.01997020363860007, w1=-0.2379922427267338\n",
      "(250000,)\n",
      "Gradient Descent(39/2999): loss=0.3925718868075155, w0=-0.019777501212237217, w1=-0.23850396730281093\n",
      "(250000,)\n",
      "Gradient Descent(40/2999): loss=0.39245376516359576, w0=-0.019565696342343772, w1=-0.23899643769097093\n",
      "(250000,)\n",
      "Gradient Descent(41/2999): loss=0.39234143376868597, w0=-0.019336430198615057, w1=-0.2394707298666505\n",
      "(250000,)\n",
      "Gradient Descent(42/2999): loss=0.3922345668551011, w0=-0.019091242601709146, w1=-0.23992782971343382\n",
      "(250000,)\n",
      "Gradient Descent(43/2999): loss=0.39213286037147543, w0=-0.018831577904276076, w1=-0.24036864294270122\n",
      "(250000,)\n",
      "Gradient Descent(44/2999): loss=0.39203603025208994, w0=-0.0185587905183663, w1=-0.2407940037204642\n",
      "(250000,)\n",
      "Gradient Descent(45/2999): loss=0.39194381085117824, w0=-0.018274150113416942, w1=-0.24120468219151486\n",
      "(250000,)\n",
      "Gradient Descent(46/2999): loss=0.3918559535231443, w0=-0.017978846506887983, w1=-0.2416013910601028\n",
      "(250000,)\n",
      "Gradient Descent(47/2999): loss=0.39177222533234035, w0=-0.017673994267744682, w1=-0.24198479136092813\n",
      "(250000,)\n",
      "Gradient Descent(48/2999): loss=0.3916924078782896, w0=-0.01736063705132494, w1=-0.24235549753324853\n",
      "(250000,)\n",
      "Gradient Descent(49/2999): loss=0.39161629622411465, w0=-0.017039751682660486, w1=-0.2427140818935068\n",
      "(250000,)\n",
      "Gradient Descent(50/2999): loss=0.3915436979175027, w0=-0.016712252004011934, w1=-0.24306107858742304\n",
      "(250000,)\n",
      "Gradient Descent(51/2999): loss=0.3914744320948563, w0=-0.016378992501206027, w1=-0.2433969870904325\n",
      "(250000,)\n",
      "Gradient Descent(52/2999): loss=0.3914083286604127, w0=-0.01604077172230998, w1=-0.2437222753152516\n",
      "(250000,)\n",
      "Gradient Descent(53/2999): loss=0.3913452275330706, w0=-0.01569833550122485, w1=-0.2440373823768774\n",
      "(250000,)\n",
      "Gradient Descent(54/2999): loss=0.39128497795448824, w0=-0.01535237999791485, w1=-0.24434272105818658\n",
      "(250000,)\n",
      "Gradient Descent(55/2999): loss=0.3912274378527339, w0=-0.015003554566200051, w1=-0.2446386800132696\n",
      "(250000,)\n",
      "Gradient Descent(56/2999): loss=0.39117247325638305, w0=-0.014652464459316948, w1=-0.24492562574052845\n",
      "(250000,)\n",
      "Gradient Descent(57/2999): loss=0.391119957754499, w0=-0.014299673382786841, w1=-0.24520390435322814\n",
      "(250000,)\n",
      "Gradient Descent(58/2999): loss=0.39106977199840287, w0=-0.013945705903519354, w1=-0.2454738431714973\n",
      "(250000,)\n",
      "Gradient Descent(59/2999): loss=0.3910218032415582, w0=-0.013591049723512057, w1=-0.24573575215661808\n",
      "(250000,)\n",
      "Gradient Descent(60/2999): loss=0.39097594491425314, w0=-0.013236157825982434, w1=-0.24598992520574478\n",
      "(250000,)\n",
      "Gradient Descent(61/2999): loss=0.39093209623009795, w0=-0.012881450501281285, w1=-0.24623664132287315\n",
      "(250000,)\n",
      "Gradient Descent(62/2999): loss=0.3908901618216323, w0=-0.012527317259483736, w1=-0.2464761656798877\n",
      "(250000,)\n",
      "Gradient Descent(63/2999): loss=0.39085005140260076, w0=-0.012174118636132112, w1=-0.24670875057979563\n",
      "(250000,)\n",
      "Gradient Descent(64/2999): loss=0.3908116794546811, w0=-0.011822187897211865, w1=-0.2469346363327708\n",
      "(250000,)\n",
      "Gradient Descent(65/2999): loss=0.39077496493665276, w0=-0.011471832649074561, w1=-0.24715405205434376\n",
      "(250000,)\n",
      "Gradient Descent(66/2999): loss=0.39073983101417975, w0=-0.011123336358679306, w1=-0.24736721639395842\n",
      "(250000,)\n",
      "Gradient Descent(67/2999): loss=0.39070620480854507, w0=-0.010776959789203483, w1=-0.24757433820114455\n",
      "(250000,)\n",
      "Gradient Descent(68/2999): loss=0.3906740171628232, w0=-0.010432942355773998, w1=-0.24777561713571028\n",
      "(250000,)\n",
      "Gradient Descent(69/2999): loss=0.3906432024241118, w0=-0.01009150340578987, w1=-0.24797124422762007\n",
      "(250000,)\n",
      "Gradient Descent(70/2999): loss=0.3906136982405613, w0=-0.009752843428044196, w1=-0.24816140239157877\n",
      "(250000,)\n",
      "Gradient Descent(71/2999): loss=0.39058544537205636, w0=-0.00941714519460763, w1=-0.24834626690077719\n",
      "(250000,)\n",
      "Gradient Descent(72/2999): loss=0.3905583875134964, w0=-0.009084574839205075, w1=-0.24852600582375844\n",
      "(250000,)\n",
      "Gradient Descent(73/2999): loss=0.39053247112971523, w0=-0.008755282875600833, w1=-0.24870078042792945\n",
      "(250000,)\n",
      "Gradient Descent(74/2999): loss=0.39050764530116144, w0=-0.008429405159305141, w1=-0.24887074555285768\n",
      "(250000,)\n",
      "Gradient Descent(75/2999): loss=0.3904838615795319, w0=-0.00810706379572448, w1=-0.2490360499561564\n",
      "(250000,)\n",
      "Gradient Descent(76/2999): loss=0.39046107385262385, w0=-0.007788367997699782, w1=-0.24919683663446318\n",
      "(250000,)\n",
      "Gradient Descent(77/2999): loss=0.39043923821772436, w0=-0.007473414895208993, w1=-0.24935324312175283\n",
      "(250000,)\n",
      "Gradient Descent(78/2999): loss=0.3904183128629191, w0=-0.007162290299853009, w1=-0.24950540176699326\n",
      "(250000,)\n",
      "Gradient Descent(79/2999): loss=0.39039825795574784, w0=-0.006855069426596292, w1=-0.2496534399929463\n",
      "(250000,)\n",
      "Gradient Descent(80/2999): loss=0.39037903553868214, w0=-0.0065518175750943625, w1=-0.24979748053773193\n",
      "(250000,)\n",
      "Gradient Descent(81/2999): loss=0.3903606094309436, w0=-0.006252590772809775, w1=-0.24993764168061228\n",
      "(250000,)\n",
      "Gradient Descent(82/2999): loss=0.3903429451362179, w0=-0.005957436381995429, w1=-0.2500740374533069\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(83/2999): loss=0.3903260097558551, w0=-0.005666393672508324, w1=-0.25020677783802225\n",
      "(250000,)\n",
      "Gradient Descent(84/2999): loss=0.39030977190718114, w0=-0.0053794943623082965, w1=-0.250335968953263\n",
      "(250000,)\n",
      "Gradient Descent(85/2999): loss=0.3902942016465708, w0=-0.005096763127393746, w1=-0.25046171322839195\n",
      "(250000,)\n",
      "Gradient Descent(86/2999): loss=0.39027927039696264, w0=-0.00481821808283013, w1=-0.2505841095678122\n",
      "(250000,)\n",
      "Gradient Descent(87/2999): loss=0.3902649508795194, w0=-0.0045438712364360215, w1=-0.2507032535055645\n",
      "(250000,)\n",
      "Gradient Descent(88/2999): loss=0.39025121704916005, w0=-0.004273728916606166, w1=-0.25081923735105993\n",
      "(250000,)\n",
      "Gradient Descent(89/2999): loss=0.39023804403371015, w0=-0.004007792175670169, w1=-0.2509321503266013\n",
      "(250000,)\n",
      "Gradient Descent(90/2999): loss=0.3902254080764373, w0=-0.003746057170109578, w1=-0.25104207869728956\n",
      "(250000,)\n",
      "Gradient Descent(91/2999): loss=0.39021328648175313, w0=-0.0034885155188842344, w1=-0.25114910589385664\n",
      "(250000,)\n",
      "Gradient Descent(92/2999): loss=0.39020165756388203, w0=-0.0032351546410514043, w1=-0.25125331262892164\n",
      "(250000,)\n",
      "Gradient Descent(93/2999): loss=0.3901905005983107, w0=-0.0029859580737970834, w1=-0.25135477700712133\n",
      "(250000,)\n",
      "Gradient Descent(94/2999): loss=0.3901797957758437, w0=-0.002740905771938857, w1=-0.25145357462953005\n",
      "(250000,)\n",
      "Gradient Descent(95/2999): loss=0.39016952415910644, w0=-0.002499974389902694, w1=-0.2515497786927482\n",
      "(250000,)\n",
      "Gradient Descent(96/2999): loss=0.39015966764134546, w0=-0.0022631375471223445, w1=-0.25164346008300764\n",
      "(250000,)\n",
      "Gradient Descent(97/2999): loss=0.3901502089073864, w0=-0.0020303660777593905, w1=-0.2517346874656137\n",
      "(250000,)\n",
      "Gradient Descent(98/2999): loss=0.39014113139662326, w0=-0.0018016282655938342, w1=-0.25182352737001856\n",
      "(250000,)\n",
      "Gradient Descent(99/2999): loss=0.3901324192679152, w0=-0.0015768900648899951, w1=-0.2519100442707965\n",
      "(250000,)\n",
      "Gradient Descent(100/2999): loss=0.39012405736628125, w0=-0.001356115307999595, w1=-0.25199430066477185\n",
      "(250000,)\n",
      "Gradient Descent(101/2999): loss=0.3901160311912885, w0=-0.0011392659004233487, w1=-0.2520763571445305\n",
      "(250000,)\n",
      "Gradient Descent(102/2999): loss=0.3901083268670343, w0=-0.0009263020040140858, w1=-0.25215627246852823\n",
      "(250000,)\n",
      "Gradient Descent(103/2999): loss=0.3901009311136338, w0=-0.0007171822089683391, w1=-0.25223410362799414\n",
      "(250000,)\n",
      "Gradient Descent(104/2999): loss=0.39009383122012625, w0=-0.000511863695218822, w1=-0.25230990591081315\n",
      "(250000,)\n",
      "Gradient Descent(105/2999): loss=0.39008701501872084, w0=-0.00031030238380798977, w1=-0.2523837329625569\n",
      "(250000,)\n",
      "Gradient Descent(106/2999): loss=0.39008047086030906, w0=-0.00011245307879207165, w1=-0.2524556368448224\n",
      "(250000,)\n",
      "Gradient Descent(107/2999): loss=0.39007418759117063, w0=8.173039980408973e-05, w1=-0.2525256680910253\n",
      "(250000,)\n",
      "Gradient Descent(108/2999): loss=0.3900681545308118, w0=0.00027229509148851856, w1=-0.25259387575978515\n",
      "(250000,)\n",
      "Gradient Descent(109/2999): loss=0.39006236145087203, w0=0.00045928877879313694, w1=-0.2526603074860316\n",
      "(250000,)\n",
      "Gradient Descent(110/2999): loss=0.3900567985550418, w0=0.0006427598783121494, w1=-0.2527250095299498\n",
      "(250000,)\n",
      "Gradient Descent(111/2999): loss=0.3900514564599399, w0=0.0008227573388691346, w1=-0.2527880268238784\n",
      "(250000,)\n",
      "Gradient Descent(112/2999): loss=0.39004632617689655, w0=0.0009993305464160718, w1=-0.2528494030172637\n",
      "(250000,)\n",
      "Gradient Descent(113/2999): loss=0.39004139909459734, w0=0.001172529235288698, w1=-0.25290918051976913\n",
      "(250000,)\n",
      "Gradient Descent(114/2999): loss=0.39003666696254113, w0=0.001342403405462379, w1=-0.25296740054263117\n",
      "(250000,)\n",
      "Gradient Descent(115/2999): loss=0.3900321218752722, w0=0.0015090032454714677, w1=-0.25302410313834905\n",
      "(250000,)\n",
      "Gradient Descent(116/2999): loss=0.39002775625734387, w0=0.0016723790606729904, w1=-0.25307932723878884\n",
      "(250000,)\n",
      "Gradient Descent(117/2999): loss=0.3900235628489806, w0=0.0018325812065524548, w1=-0.25313311069177846\n",
      "(250000,)\n",
      "Gradient Descent(118/2999): loss=0.3900195346923984, w0=0.001989660026785369, w1=-0.25318549029626575\n",
      "(250000,)\n",
      "Gradient Descent(119/2999): loss=0.39001566511875396, w0=0.0021436657957835848, w1=-0.25323650183610746\n",
      "(250000,)\n",
      "Gradient Descent(120/2999): loss=0.39001194773569015, w0=0.0022946486654696597, w1=-0.2532861801125527\n",
      "(250000,)\n",
      "Gradient Descent(121/2999): loss=0.3900083764154482, w0=0.0024426586160362354, w1=-0.2533345589754816\n",
      "(250000,)\n",
      "Gradient Descent(122/2999): loss=0.39000494528351837, w0=0.002587745410460308, w1=-0.2533816713534559\n",
      "(250000,)\n",
      "Gradient Descent(123/2999): loss=0.39000164870780285, w0=0.0027299585525544373, w1=-0.2534275492826356\n",
      "(250000,)\n",
      "Gradient Descent(124/2999): loss=0.38999848128826714, w0=0.00286934724834887, w1=-0.25347222393461255\n",
      "(250000,)\n",
      "Gradient Descent(125/2999): loss=0.3899954378470542, w0=0.0030059603706090584, w1=-0.2535157256432091\n",
      "(250000,)\n",
      "Gradient Descent(126/2999): loss=0.38999251341904034, w0=0.003139846426303994, w1=-0.25355808393028795\n",
      "(250000,)\n",
      "Gradient Descent(127/2999): loss=0.3899897032428122, w0=0.0032710535268503705, w1=-0.25359932753061565\n",
      "(250000,)\n",
      "Gradient Descent(128/2999): loss=0.3899870027520433, w0=0.0033996293609671237, w1=-0.2536394844158219\n",
      "(250000,)\n",
      "Gradient Descent(129/2999): loss=0.38998440756725256, w0=0.0035256211699836596, w1=-0.25367858181749303\n",
      "(250000,)\n",
      "Gradient Descent(130/2999): loss=0.38998191348792566, w0=0.003649075725453708, w1=-0.2537166462494365\n",
      "(250000,)\n",
      "Gradient Descent(131/2999): loss=0.38997951648498314, w0=0.003770039308934516, w1=-0.25375370352915233\n",
      "(250000,)\n",
      "Gradient Descent(132/2999): loss=0.3899772126935791, w0=0.003888557693798899, w1=-0.2537897787985438\n",
      "(250000,)\n",
      "Gradient Descent(133/2999): loss=0.3899749984062135, w0=0.004004676128954665, w1=-0.25382489654390056\n",
      "(250000,)\n",
      "Gradient Descent(134/2999): loss=0.38997287006614567, w0=0.0041184393243529, w1=-0.2538590806151829\n",
      "(250000,)\n",
      "Gradient Descent(135/2999): loss=0.3899708242610941, w0=0.00422989143817291, w1=-0.25389235424463746\n",
      "(250000,)\n",
      "Gradient Descent(136/2999): loss=0.38996885771720935, w0=0.00433907606557791, w1=-0.25392474006477017\n",
      "(250000,)\n",
      "Gradient Descent(137/2999): loss=0.38996696729330804, w0=0.004446036228941253, w1=-0.2539562601257041\n",
      "(250000,)\n",
      "Gradient Descent(138/2999): loss=0.3899651499753542, w0=0.004550814369448414, w1=-0.25398693591194604\n",
      "(250000,)\n",
      "Gradient Descent(139/2999): loss=0.38996340287118086, w0=0.004653452339985413, w1=-0.25401678835858577\n",
      "(250000,)\n",
      "Gradient Descent(140/2999): loss=0.3899617232054359, w0=0.004753991399229055, w1=-0.254045837866951\n",
      "(250000,)\n",
      "Gradient Descent(141/2999): loss=0.3899601083147461, w0=0.004852472206859201, w1=-0.25407410431973904\n",
      "(250000,)\n",
      "Gradient Descent(142/2999): loss=0.38995855564308746, w0=0.004948934819817656, w1=-0.25410160709564605\n",
      "(250000,)\n",
      "Gradient Descent(143/2999): loss=0.38995706273735303, w0=0.005043418689542438, w1=-0.2541283650835136\n",
      "(250000,)\n",
      "Gradient Descent(144/2999): loss=0.3899556272431099, w0=0.0051359626601104056, w1=-0.25415439669601114\n",
      "(250000,)\n",
      "Gradient Descent(145/2999): loss=0.38995424690053637, w0=0.005226604967224635, w1=-0.25417971988287236\n",
      "(250000,)\n",
      "Gradient Descent(146/2999): loss=0.3899529195405317, w0=0.005315383237986838, w1=-0.2542043521437026\n",
      "(250000,)\n",
      "Gradient Descent(147/2999): loss=0.38995164308098995, w0=0.005402334491398355, w1=-0.2542283105403738\n",
      "(250000,)\n",
      "Gradient Descent(148/2999): loss=0.3899504155232326, w0=0.005487495139536508, w1=-0.25425161170902233\n",
      "(250000,)\n",
      "Gradient Descent(149/2999): loss=0.38994923494859024, w0=0.005570900989356125, w1=-0.2542742718716649\n",
      "(250000,)\n",
      "Gradient Descent(150/2999): loss=0.38994809951513, w0=0.005652587245068964, w1=-0.2542963068474472\n",
      "(250000,)\n",
      "Gradient Descent(151/2999): loss=0.3899470074545179, w0=0.00573258851105647, w1=-0.254317732063538\n",
      "(250000,)\n",
      "Gradient Descent(152/2999): loss=0.38994595706901675, w0=0.00581093879527391, w1=-0.25433856256568327\n",
      "(250000,)\n",
      "Gradient Descent(153/2999): loss=0.38994494672860625, w0=0.0058876715131063505, w1=-0.254358813028431\n",
      "(250000,)\n",
      "Gradient Descent(154/2999): loss=0.38994397486822585, w0=0.005962819491639312, w1=-0.2543784977650409\n",
      "(250000,)\n",
      "Gradient Descent(155/2999): loss=0.3899430399851319, w0=0.0060364149743090375, w1=-0.2543976307370887\n",
      "(250000,)\n",
      "Gradient Descent(156/2999): loss=0.3899421406363652, w0=0.006108489625899534, w1=-0.25441622556377713\n",
      "(250000,)\n",
      "Gradient Descent(157/2999): loss=0.38994127543632473, w0=0.0061790745378553745, w1=-0.2544342955309636\n",
      "(250000,)\n",
      "Gradient Descent(158/2999): loss=0.389940443054441, w0=0.006248200233881165, w1=-0.25445185359991507\n",
      "(250000,)\n",
      "Gradient Descent(159/2999): loss=0.3899396422129473, w0=0.0063158966758004274, w1=-0.2544689124157998\n",
      "(250000,)\n",
      "Gradient Descent(160/2999): loss=0.38993887168474295, w0=0.006382193269648122, w1=-0.2544854843159248\n",
      "(250000,)\n",
      "Gradient Descent(161/2999): loss=0.3899381302913448, w0=0.006447118871972798, w1=-0.2545015813377289\n",
      "(250000,)\n",
      "Gradient Descent(162/2999): loss=0.3899374169009239, w0=0.006510701796325815, w1=-0.25451721522653864\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(163/2999): loss=0.3899367304264231, w0=0.0065729698199163385, w1=-0.2545323974430969\n",
      "(250000,)\n",
      "Gradient Descent(164/2999): loss=0.38993606982375245, w0=0.006633950190412363, w1=-0.2545471391708705\n",
      "(250000,)\n",
      "Gradient Descent(165/2999): loss=0.38993543409005893, w0=0.006693669632869142, w1=-0.25456145132314534\n",
      "(250000,)\n",
      "Gradient Descent(166/2999): loss=0.38993482226206705, w0=0.006752154356767652, w1=-0.25457534454991626\n",
      "(250000,)\n",
      "Gradient Descent(167/2999): loss=0.3899342334144891, w0=0.00680943006314678, w1=-0.25458882924457826\n",
      "(250000,)\n",
      "Gradient Descent(168/2999): loss=0.38993366665849777, w0=0.006865521951814092, w1=-0.25460191555042605\n",
      "(250000,)\n",
      "Gradient Descent(169/2999): loss=0.389933121140265, w0=0.0069204547286209304, w1=-0.25461461336696845\n",
      "(250000,)\n",
      "Gradient Descent(170/2999): loss=0.38993259603955827, w0=0.006974252612788644, w1=-0.2546269323560633\n",
      "(250000,)\n",
      "Gradient Descent(171/2999): loss=0.38993209056839495, w0=0.0070269393442735795, w1=-0.2546388819478798\n",
      "(250000,)\n",
      "Gradient Descent(172/2999): loss=0.3899316039697517, w0=0.0070785381911593285, w1=-0.25465047134669294\n",
      "(250000,)\n",
      "Gradient Descent(173/2999): loss=0.389931135516327, w0=0.007129071957065527, w1=-0.2546617095365161\n",
      "(250000,)\n",
      "Gradient Descent(174/2999): loss=0.3899306845093538, w0=0.007178562988563275, w1=-0.25467260528657754\n",
      "(250000,)\n",
      "Gradient Descent(175/2999): loss=0.38993025027746, w0=0.007227033182587959, w1=-0.2546831671566444\n",
      "(250000,)\n",
      "Gradient Descent(176/2999): loss=0.389929832175576, w0=0.007274503993840838, w1=-0.25469340350220104\n",
      "(250000,)\n",
      "Gradient Descent(177/2999): loss=0.38992942958388727, w0=0.007320996442171558, w1=-0.2547033224794848\n",
      "(250000,)\n",
      "Gradient Descent(178/2999): loss=0.3899290419068278, w0=0.007366531119934168, w1=-0.2547129320503846\n",
      "(250000,)\n",
      "Gradient Descent(179/2999): loss=0.3899286685721162, w0=0.007411128199309933, w1=-0.2547222399872066\n",
      "(250000,)\n",
      "Gradient Descent(180/2999): loss=0.38992830902982883, w0=0.007454807439590697, w1=-0.2547312538773108\n",
      "(250000,)\n",
      "Gradient Descent(181/2999): loss=0.38992796275151326, w0=0.007497588194416978, w1=-0.25473998112762314\n",
      "(250000,)\n",
      "Gradient Descent(182/2999): loss=0.38992762922933416, w0=0.007539489418965613, w1=-0.25474842896902633\n",
      "(250000,)\n",
      "Gradient Descent(183/2999): loss=0.3899273079752576, w0=0.007580529677082053, w1=-0.254756604460634\n",
      "(250000,)\n",
      "Gradient Descent(184/2999): loss=0.38992699852026463, w0=0.007620727148352935, w1=-0.25476451449395104\n",
      "(250000,)\n",
      "Gradient Descent(185/2999): loss=0.38992670041359884, w0=0.007660099635114855, w1=-0.2547721657969241\n",
      "(250000,)\n",
      "Gradient Descent(186/2999): loss=0.3899264132220439, w0=0.007698664569395774, w1=-0.2547795649378852\n",
      "(250000,)\n",
      "Gradient Descent(187/2999): loss=0.38992613652922975, w0=0.007736439019785582, w1=-0.2547867183293921\n",
      "(250000,)\n",
      "Gradient Descent(188/2999): loss=0.38992586993496675, w0=0.007773439698233003, w1=-0.25479363223196844\n",
      "(250000,)\n",
      "Gradient Descent(189/2999): loss=0.38992561305460677, w0=0.00780968296676606, w1=-0.2548003127577463\n",
      "(250000,)\n",
      "Gradient Descent(190/2999): loss=0.38992536551843027, w0=0.007845184844133697, w1=-0.25480676587401474\n",
      "(250000,)\n",
      "Gradient Descent(191/2999): loss=0.3899251269710576, w0=0.007879961012366458, w1=-0.2548129974066765\n",
      "(250000,)\n",
      "Gradient Descent(192/2999): loss=0.38992489707088385, w0=0.007914026823254355, w1=-0.25481901304361604\n",
      "(250000,)\n",
      "Gradient Descent(193/2999): loss=0.3899246754895363, w0=0.007947397304740183, w1=-0.2548248183379811\n",
      "(250000,)\n",
      "Gradient Descent(194/2999): loss=0.3899244619113537, w0=0.007980087167226977, w1=-0.2548304187113808\n",
      "(250000,)\n",
      "Gradient Descent(195/2999): loss=0.38992425603288783, w0=0.00801211080979832, w1=-0.2548358194570023\n",
      "(250000,)\n",
      "Gradient Descent(196/2999): loss=0.3899240575624211, w0=0.008043482326350474, w1=-0.2548410257426484\n",
      "(250000,)\n",
      "Gradient Descent(197/2999): loss=0.3899238662195084, w0=0.008074215511635502, w1=-0.2548460426136988\n",
      "(250000,)\n",
      "Gradient Descent(198/2999): loss=0.3899236817345331, w0=0.008104323867214687, w1=-0.25485087499599673\n",
      "(250000,)\n",
      "Gradient Descent(199/2999): loss=0.3899235038482834, w0=0.008133820607321724, w1=-0.25485552769866304\n",
      "(250000,)\n",
      "Gradient Descent(200/2999): loss=0.3899233323115437, w0=0.008162718664635311, w1=-0.25486000541684023\n",
      "(250000,)\n",
      "Gradient Descent(201/2999): loss=0.38992316688470413, w0=0.008191030695960892, w1=-0.25486431273436827\n",
      "(250000,)\n",
      "Gradient Descent(202/2999): loss=0.3899230073373839, w0=0.00821876908782142, w1=-0.25486845412639364\n",
      "(250000,)\n",
      "Gradient Descent(203/2999): loss=0.38992285344807076, w0=0.008245945961957128, w1=-0.2548724339619142\n",
      "(250000,)\n",
      "Gradient Descent(204/2999): loss=0.38992270500377374, w0=0.008272573180734459, w1=-0.2548762565062612\n",
      "(250000,)\n",
      "Gradient Descent(205/2999): loss=0.3899225617996909, w0=0.008298662352464318, w1=-0.2548799259235201\n",
      "(250000,)\n",
      "Gradient Descent(206/2999): loss=0.38992242363888957, w0=0.008324224836629996, w1=-0.25488344627889237\n",
      "(250000,)\n",
      "Gradient Descent(207/2999): loss=0.38992229033199927, w0=0.008349271749025047, w1=-0.2548868215409993\n",
      "(250000,)\n",
      "Gradient Descent(208/2999): loss=0.3899221616969169, w0=0.008373813966801705, w1=-0.25489005558412986\n",
      "(250000,)\n",
      "Gradient Descent(209/2999): loss=0.3899220375585232, w0=0.008397862133430304, w1=-0.2548931521904339\n",
      "(250000,)\n",
      "Gradient Descent(210/2999): loss=0.38992191774841156, w0=0.008421426663570352, w1=-0.2548961150520622\n",
      "(250000,)\n",
      "Gradient Descent(211/2999): loss=0.38992180210462624, w0=0.008444517747853873, w1=-0.2548989477732547\n",
      "(250000,)\n",
      "Gradient Descent(212/2999): loss=0.38992169047141145, w0=0.008467145357581763, w1=-0.2549016538723787\n",
      "(250000,)\n",
      "Gradient Descent(213/2999): loss=0.38992158269897115, w0=0.008489319249333958, w1=-0.25490423678391766\n",
      "(250000,)\n",
      "Gradient Descent(214/2999): loss=0.3899214786432362, w0=0.008511048969494185, w1=-0.2549066998604128\n",
      "(250000,)\n",
      "Gradient Descent(215/2999): loss=0.3899213781656433, w0=0.008532343858690191, w1=-0.2549090463743579\n",
      "(250000,)\n",
      "Gradient Descent(216/2999): loss=0.38992128113292085, w0=0.008553213056150371, w1=-0.254911279520049\n",
      "(250000,)\n",
      "Gradient Descent(217/2999): loss=0.38992118741688364, w0=0.008573665503977652, w1=-0.25491340241539007\n",
      "(250000,)\n",
      "Gradient Descent(218/2999): loss=0.3899210968942357, w0=0.008593709951341707, w1=-0.2549154181036559\n",
      "(250000,)\n",
      "Gradient Descent(219/2999): loss=0.3899210094463818, w0=0.008613354958590383, w1=-0.2549173295552132\n",
      "(250000,)\n",
      "Gradient Descent(220/2999): loss=0.3899209249592441, w0=0.008632608901281397, w1=-0.2549191396692007\n",
      "(250000,)\n",
      "Gradient Descent(221/2999): loss=0.38992084332308874, w0=0.008651479974135415, w1=-0.2549208512751704\n",
      "(250000,)\n",
      "Gradient Descent(222/2999): loss=0.38992076443235685, w0=0.008669976194911432, w1=-0.25492246713468913\n",
      "(250000,)\n",
      "Gradient Descent(223/2999): loss=0.38992068818550396, w0=0.008688105408205664, w1=-0.2549239899429034\n",
      "(250000,)\n",
      "Gradient Descent(224/2999): loss=0.3899206144848449, w0=0.008705875289174887, w1=-0.254925422330067\n",
      "(250000,)\n",
      "Gradient Descent(225/2999): loss=0.3899205432364042, w0=0.008723293347185503, w1=-0.2549267668630333\n",
      "(250000,)\n",
      "Gradient Descent(226/2999): loss=0.3899204743497734, w0=0.008740366929389239, w1=-0.2549280260467123\n",
      "(250000,)\n",
      "Gradient Descent(227/2999): loss=0.38992040773797426, w0=0.008757103224226763, w1=-0.2549292023254939\n",
      "(250000,)\n",
      "Gradient Descent(228/2999): loss=0.38992034331732534, w0=0.008773509264860212, w1=-0.2549302980846381\n",
      "(250000,)\n",
      "Gradient Descent(229/2999): loss=0.3899202810073158, w0=0.008789591932535827, w1=-0.2549313156516328\n",
      "(250000,)\n",
      "Gradient Descent(230/2999): loss=0.38992022073048316, w0=0.0088053579598778, w1=-0.2549322572975203\n",
      "(250000,)\n",
      "Gradient Descent(231/2999): loss=0.3899201624122964, w0=0.008820813934114427, w1=-0.2549331252381927\n",
      "(250000,)\n",
      "Gradient Descent(232/2999): loss=0.3899201059810425, w0=0.008835966300237695, w1=-0.2549339216356581\n",
      "(250000,)\n",
      "Gradient Descent(233/2999): loss=0.389920051367719, w0=0.008850821364097497, w1=-0.2549346485992765\n",
      "(250000,)\n",
      "Gradient Descent(234/2999): loss=0.38991999850592945, w0=0.008865385295431481, w1=-0.2549353081869683\n",
      "(250000,)\n",
      "Gradient Descent(235/2999): loss=0.3899199473317835, w0=0.008879664130831695, w1=-0.2549359024063944\n",
      "(250000,)\n",
      "Gradient Descent(236/2999): loss=0.38991989778380065, w0=0.008893663776649184, w1=-0.25493643321610926\n",
      "(250000,)\n",
      "Gradient Descent(237/2999): loss=0.38991984980281813, w0=0.00890739001183755, w1=-0.25493690252668755\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(238/2999): loss=0.3899198033319018, w0=0.008920848490736589, w1=-0.25493731220182514\n",
      "(250000,)\n",
      "Gradient Descent(239/2999): loss=0.389919758316261, w0=0.008934044745797174, w1=-0.25493766405941476\n",
      "(250000,)\n",
      "Gradient Descent(240/2999): loss=0.38991971470316666, w0=0.008946984190248312, w1=-0.2549379598725968\n",
      "(250000,)\n",
      "Gradient Descent(241/2999): loss=0.3899196724418718, w0=0.008959672120707593, w1=-0.2549382013707869\n",
      "(250000,)\n",
      "Gradient Descent(242/2999): loss=0.38991963148353664, w0=0.008972113719735974, w1=-0.254938390240679\n",
      "(250000,)\n",
      "Gradient Descent(243/2999): loss=0.38991959178115504, w0=0.008984314058338017, w1=-0.2549385281272267\n",
      "(250000,)\n",
      "Gradient Descent(244/2999): loss=0.38991955328948447, w0=0.008996278098408531, w1=-0.2549386166346016\n",
      "(250000,)\n",
      "Gradient Descent(245/2999): loss=0.3899195159649793, w0=0.009008010695126748, w1=-0.2549386573271298\n",
      "(250000,)\n",
      "Gradient Descent(246/2999): loss=0.3899194797657255, w0=0.009019516599298994, w1=-0.2549386517302076\n",
      "(250000,)\n",
      "Gradient Descent(247/2999): loss=0.3899194446513787, w0=0.009030800459650778, w1=-0.25493860133119617\n",
      "(250000,)\n",
      "Gradient Descent(248/2999): loss=0.3899194105831039, w0=0.009041866825069429, w1=-0.2549385075802959\n",
      "(250000,)\n",
      "Gradient Descent(249/2999): loss=0.38991937752351896, w0=0.009052720146798166, w1=-0.2549383718914011\n",
      "(250000,)\n",
      "Gradient Descent(250/2999): loss=0.3899193454366382, w0=0.009063364780582549, w1=-0.2549381956429352\n",
      "(250000,)\n",
      "Gradient Descent(251/2999): loss=0.3899193142878199, w0=0.00907380498877033, w1=-0.25493798017866715\n",
      "(250000,)\n",
      "Gradient Descent(252/2999): loss=0.3899192840437151, w0=0.00908404494236554, w1=-0.25493772680850957\n",
      "(250000,)\n",
      "Gradient Descent(253/2999): loss=0.3899192546722183, w0=0.009094088723037814, w1=-0.2549374368092986\n",
      "(250000,)\n",
      "Gradient Descent(254/2999): loss=0.38991922614242036, w0=0.009103940325087868, w1=-0.2549371114255563\n",
      "(250000,)\n",
      "Gradient Descent(255/2999): loss=0.3899191984245631, w0=0.009113603657369919, w1=-0.2549367518702359\n",
      "(250000,)\n",
      "Gradient Descent(256/2999): loss=0.38991917148999544, w0=0.009123082545172035, w1=-0.25493635932545006\n",
      "(250000,)\n",
      "Gradient Descent(257/2999): loss=0.38991914531113175, w0=0.0091323807320552, w1=-0.2549359349431832\n",
      "(250000,)\n",
      "Gradient Descent(258/2999): loss=0.389919119861411, w0=0.009141501881652043, w1=-0.2549354798459871\n",
      "(250000,)\n",
      "Gradient Descent(259/2999): loss=0.3899190951152585, w0=0.009150449579425946, w1=-0.25493499512766166\n",
      "(250000,)\n",
      "Gradient Descent(260/2999): loss=0.38991907104804807, w0=0.009159227334391427, w1=-0.25493448185391987\n",
      "(250000,)\n",
      "Gradient Descent(261/2999): loss=0.38991904763606616, w0=0.009167838580796641, w1=-0.25493394106303774\n",
      "(250000,)\n",
      "Gradient Descent(262/2999): loss=0.38991902485647784, w0=0.009176286679768672, w1=-0.2549333737664901\n",
      "(250000,)\n",
      "Gradient Descent(263/2999): loss=0.38991900268729296, w0=0.009184574920922539, w1=-0.25493278094957184\n",
      "(250000,)\n",
      "Gradient Descent(264/2999): loss=0.3899189811073346, w0=0.00919270652393458, w1=-0.2549321635720051\n",
      "(250000,)\n",
      "Gradient Descent(265/2999): loss=0.3899189600962083, w0=0.009200684640081017, w1=-0.2549315225685332\n",
      "(250000,)\n",
      "Gradient Descent(266/2999): loss=0.38991893963427254, w0=0.009208512353742435, w1=-0.25493085884950095\n",
      "(250000,)\n",
      "Gradient Descent(267/2999): loss=0.38991891970260995, w0=0.009216192683874901, w1=-0.2549301733014222\n",
      "(250000,)\n",
      "Gradient Descent(268/2999): loss=0.3899189002830009, w0=0.009223728585448409, w1=-0.25492946678753436\n",
      "(250000,)\n",
      "Gradient Descent(269/2999): loss=0.3899188813578961, w0=0.009231122950853432, w1=-0.2549287401483408\n",
      "(250000,)\n",
      "Gradient Descent(270/2999): loss=0.38991886291039257, w0=0.009238378611276134, w1=-0.2549279942021411\n",
      "(250000,)\n",
      "Gradient Descent(271/2999): loss=0.3899188449242077, w0=0.00924549833804307, w1=-0.25492722974554893\n",
      "(250000,)\n",
      "Gradient Descent(272/2999): loss=0.3899188273836578, w0=0.009252484843935893, w1=-0.25492644755399935\n",
      "(250000,)\n",
      "Gradient Descent(273/2999): loss=0.3899188102736341, w0=0.00925934078447686, w1=-0.25492564838224363\n",
      "(250000,)\n",
      "Gradient Descent(274/2999): loss=0.38991879357958187, w0=0.009266068759185618, w1=-0.25492483296483376\n",
      "(250000,)\n",
      "Gradient Descent(275/2999): loss=0.3899187772874794, w0=0.009272671312808036, w1=-0.25492400201659565\n",
      "(250000,)\n",
      "Gradient Descent(276/2999): loss=0.38991876138381804, w0=0.009279150936517589, w1=-0.25492315623309225\n",
      "(250000,)\n",
      "Gradient Descent(277/2999): loss=0.38991874585558284, w0=0.009285510069089963, w1=-0.2549222962910759\n",
      "(250000,)\n",
      "Gradient Descent(278/2999): loss=0.3899187306902343, w0=0.00929175109805142, w1=-0.25492142284893043\n",
      "(250000,)\n",
      "Gradient Descent(279/2999): loss=0.3899187158756902, w0=0.009297876360801548, w1=-0.25492053654710417\n",
      "(250000,)\n",
      "Gradient Descent(280/2999): loss=0.3899187014003088, w0=0.00930388814571091, w1=-0.2549196380085321\n",
      "(250000,)\n",
      "Gradient Descent(281/2999): loss=0.38991868725287215, w0=0.009309788693194153, w1=-0.2549187278390498\n",
      "(250000,)\n",
      "Gradient Descent(282/2999): loss=0.3899186734225702, w0=0.009315580196759175, w1=-0.25491780662779684\n",
      "(250000,)\n",
      "Gradient Descent(283/2999): loss=0.38991865989898605, w0=0.009321264804032796, w1=-0.2549168749476124\n",
      "(250000,)\n",
      "Gradient Descent(284/2999): loss=0.3899186466720804, w0=0.009326844617763532, w1=-0.2549159333554212\n",
      "(250000,)\n",
      "Gradient Descent(285/2999): loss=0.389918633732178, w0=0.009332321696801904, w1=-0.254914982392611\n",
      "(250000,)\n",
      "Gradient Descent(286/2999): loss=0.389918621069954, w0=0.00933769805705885, w1=-0.2549140225854019\n",
      "(250000,)\n",
      "Gradient Descent(287/2999): loss=0.3899186086764211, w0=0.009342975672442681, w1=-0.25491305444520695\n",
      "(250000,)\n",
      "Gradient Descent(288/2999): loss=0.3899185965429165, w0=0.009348156475775077, w1=-0.2549120784689849\n",
      "(250000,)\n",
      "Gradient Descent(289/2999): loss=0.38991858466108975, w0=0.00935324235968659, w1=-0.2549110951395852\n",
      "(250000,)\n",
      "Gradient Descent(290/2999): loss=0.3899185730228918, w0=0.009358235177492104, w1=-0.25491010492608507\n",
      "(250000,)\n",
      "Gradient Descent(291/2999): loss=0.3899185616205635, w0=0.00936313674404671, w1=-0.254909108284119\n",
      "(250000,)\n",
      "Gradient Descent(292/2999): loss=0.38991855044662416, w0=0.009367948836582441, w1=-0.25490810565620087\n",
      "(250000,)\n",
      "Gradient Descent(293/2999): loss=0.3899185394938622, w0=0.00937267319552624, w1=-0.25490709747203905\n",
      "(250000,)\n",
      "Gradient Descent(294/2999): loss=0.3899185287553243, w0=0.00937731152529969, w1=-0.25490608414884414\n",
      "(250000,)\n",
      "Gradient Descent(295/2999): loss=0.38991851822430673, w0=0.009381865495100802, w1=-0.25490506609163\n",
      "(250000,)\n",
      "Gradient Descent(296/2999): loss=0.38991850789434507, w0=0.009386336739668343, w1=-0.25490404369350794\n",
      "(250000,)\n",
      "Gradient Descent(297/2999): loss=0.38991849775920623, w0=0.009390726860029026, w1=-0.2549030173359743\n",
      "(250000,)\n",
      "Gradient Descent(298/2999): loss=0.3899184878128791, w0=0.009395037424228047, w1=-0.2549019873891917\n",
      "(250000,)\n",
      "Gradient Descent(299/2999): loss=0.3899184780495669, w0=0.009399269968043251, w1=-0.2549009542122635\n",
      "(250000,)\n",
      "Gradient Descent(300/2999): loss=0.3899184684636793, w0=0.009403425995683305, w1=-0.25489991815350294\n",
      "(250000,)\n",
      "Gradient Descent(301/2999): loss=0.38991845904982414, w0=0.00940750698047032, w1=-0.25489887955069535\n",
      "(250000,)\n",
      "Gradient Descent(302/2999): loss=0.3899184498028009, w0=0.009411514365507193, w1=-0.25489783873135496\n",
      "(250000,)\n",
      "Gradient Descent(303/2999): loss=0.3899184407175928, w0=0.009415449564330028, w1=-0.25489679601297577\n",
      "(250000,)\n",
      "Gradient Descent(304/2999): loss=0.38991843178936086, w0=0.00941931396154598, w1=-0.2548957517032768\n",
      "(250000,)\n",
      "Gradient Descent(305/2999): loss=0.3899184230134368, w0=0.009423108913456888, w1=-0.2548947061004416\n",
      "(250000,)\n",
      "Gradient Descent(306/2999): loss=0.38991841438531694, w0=0.00942683574866893, w1=-0.254893659493353\n",
      "(250000,)\n",
      "Gradient Descent(307/2999): loss=0.3899184059006561, w0=0.009430495768688749, w1=-0.25489261216182163\n",
      "(250000,)\n",
      "Gradient Descent(308/2999): loss=0.38991839755526136, w0=0.00943409024850619, w1=-0.2548915643768099\n",
      "(250000,)\n",
      "Gradient Descent(309/2999): loss=0.3899183893450877, w0=0.009437620437164113, w1=-0.25489051640065086\n",
      "(250000,)\n",
      "Gradient Descent(310/2999): loss=0.3899183812662309, w0=0.009441087558315442, w1=-0.25488946848726196\n",
      "(250000,)\n",
      "Gradient Descent(311/2999): loss=0.3899183733149236, w0=0.009444492810767853, w1=-0.25488842088235397\n",
      "(250000,)\n",
      "Gradient Descent(312/2999): loss=0.38991836548752995, w0=0.009447837369016297, w1=-0.2548873738236353\n",
      "(250000,)\n",
      "Gradient Descent(313/2999): loss=0.3899183577805404, w0=0.009451122383763675, w1=-0.2548863275410115\n",
      "(250000,)\n",
      "Gradient Descent(314/2999): loss=0.38991835019056753, w0=0.009454348982429894, w1=-0.25488528225678037\n",
      "(250000,)\n",
      "Gradient Descent(315/2999): loss=0.38991834271434145, w0=0.009457518269649624, w1=-0.2548842381858227\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(316/2999): loss=0.3899183353487055, w0=0.009460631327759011, w1=-0.25488319553578836\n",
      "(250000,)\n",
      "Gradient Descent(317/2999): loss=0.38991832809061217, w0=0.009463689217271497, w1=-0.2548821545072787\n",
      "(250000,)\n",
      "Gradient Descent(318/2999): loss=0.3899183209371188, w0=0.00946669297734313, w1=-0.25488111529402413\n",
      "(250000,)\n",
      "Gradient Descent(319/2999): loss=0.38991831388538417, w0=0.009469643626227533, w1=-0.2548800780830582\n",
      "(250000,)\n",
      "Gradient Descent(320/2999): loss=0.38991830693266455, w0=0.00947254216172072, w1=-0.2548790430548874\n",
      "(250000,)\n",
      "Gradient Descent(321/2999): loss=0.3899183000763102, w0=0.009475389561596114, w1=-0.2548780103836573\n",
      "(250000,)\n",
      "Gradient Descent(322/2999): loss=0.38991829331376193, w0=0.009478186784029891, w1=-0.25487698023731475\n",
      "(250000,)\n",
      "Gradient Descent(323/2999): loss=0.389918286642548, w0=0.009480934768016865, w1=-0.2548759527777662\n",
      "(250000,)\n",
      "Gradient Descent(324/2999): loss=0.3899182800602806, w0=0.009483634433777212, w1=-0.25487492816103285\n",
      "(250000,)\n",
      "Gradient Descent(325/2999): loss=0.38991827356465314, w0=0.009486286683154176, w1=-0.25487390653740194\n",
      "(250000,)\n",
      "Gradient Descent(326/2999): loss=0.3899182671534371, w0=0.009488892400002944, w1=-0.2548728880515745\n",
      "(250000,)\n",
      "Gradient Descent(327/2999): loss=0.3899182608244789, w0=0.00949145245057097, w1=-0.2548718728428099\n",
      "(250000,)\n",
      "Gradient Descent(328/2999): loss=0.3899182545756982, w0=0.009493967683869843, w1=-0.254870861045067\n",
      "(250000,)\n",
      "Gradient Descent(329/2999): loss=0.38991824840508393, w0=0.009496438932039032, w1=-0.25486985278714214\n",
      "(250000,)\n",
      "Gradient Descent(330/2999): loss=0.38991824231069294, w0=0.00949886701070146, w1=-0.25486884819280375\n",
      "(250000,)\n",
      "Gradient Descent(331/2999): loss=0.38991823629064654, w0=0.009501252719311386, w1=-0.25486784738092394\n",
      "(250000,)\n",
      "Gradient Descent(332/2999): loss=0.38991823034312917, w0=0.009503596841494502, w1=-0.2548668504656072\n",
      "(250000,)\n",
      "Gradient Descent(333/2999): loss=0.3899182244663854, w0=0.00950590014538064, w1=-0.2548658575563159\n",
      "(250000,)\n",
      "Gradient Descent(334/2999): loss=0.3899182186587178, w0=0.009508163383929052, w1=-0.2548648687579932\n",
      "(250000,)\n",
      "Gradient Descent(335/2999): loss=0.3899182129184854, w0=0.009510387295246616, w1=-0.25486388417118255\n",
      "(250000,)\n",
      "Gradient Descent(336/2999): loss=0.38991820724410103, w0=0.009512572602899004, w1=-0.2548629038921451\n",
      "(250000,)\n",
      "Gradient Descent(337/2999): loss=0.38991820163402985, w0=0.009514720016215034, w1=-0.2548619280129739\n",
      "(250000,)\n",
      "Gradient Descent(338/2999): loss=0.38991819608678724, w0=0.009516830230584342, w1=-0.25486095662170566\n",
      "(250000,)\n",
      "Gradient Descent(339/2999): loss=0.3899181906009368, w0=0.009518903927748493, w1=-0.2548599898024298\n",
      "(250000,)\n",
      "Gradient Descent(340/2999): loss=0.38991818517508936, w0=0.009520941776085754, w1=-0.25485902763539503\n",
      "(250000,)\n",
      "Gradient Descent(341/2999): loss=0.38991817980790033, w0=0.009522944430889612, w1=-0.2548580701971134\n",
      "(250000,)\n",
      "Gradient Descent(342/2999): loss=0.38991817449806876, w0=0.009524912534641172, w1=-0.2548571175604619\n",
      "(250000,)\n",
      "Gradient Descent(343/2999): loss=0.3899181692443358, w0=0.00952684671727566, w1=-0.25485616979478193\n",
      "(250000,)\n",
      "Gradient Descent(344/2999): loss=0.3899181640454825, w0=0.009528747596443031, w1=-0.2548552269659758\n",
      "(250000,)\n",
      "Gradient Descent(345/2999): loss=0.38991815890032955, w0=0.009530615777762928, w1=-0.2548542891366018\n",
      "(250000,)\n",
      "Gradient Descent(346/2999): loss=0.38991815380773465, w0=0.009532451855074073, w1=-0.25485335636596645\n",
      "(250000,)\n",
      "Gradient Descent(347/2999): loss=0.3899181487665919, w0=0.00953425641067822, w1=-0.2548524287102148\n",
      "(250000,)\n",
      "Gradient Descent(348/2999): loss=0.38991814377583056, w0=0.00953603001557875, w1=-0.2548515062224183\n",
      "(250000,)\n",
      "Gradient Descent(349/2999): loss=0.38991813883441334, w0=0.009537773229714164, w1=-0.2548505889526613\n",
      "(250000,)\n",
      "Gradient Descent(350/2999): loss=0.38991813394133584, w0=0.009539486602186432, w1=-0.25484967694812466\n",
      "(250000,)\n",
      "Gradient Descent(351/2999): loss=0.3899181290956245, w0=0.009541170671484368, w1=-0.2548487702531679\n",
      "(250000,)\n",
      "Gradient Descent(352/2999): loss=0.38991812429633665, w0=0.009542825965702207, w1=-0.2548478689094093\n",
      "(250000,)\n",
      "Gradient Descent(353/2999): loss=0.3899181195425583, w0=0.00954445300275342, w1=-0.2548469729558041\n",
      "(250000,)\n",
      "Gradient Descent(354/2999): loss=0.38991811483340405, w0=0.009546052290579865, w1=-0.2548460824287206\n",
      "(250000,)\n",
      "Gradient Descent(355/2999): loss=0.3899181101680152, w0=0.009547624327356431, w1=-0.2548451973620149\n",
      "(250000,)\n",
      "Gradient Descent(356/2999): loss=0.3899181055455601, w0=0.009549169601691295, w1=-0.25484431778710337\n",
      "(250000,)\n",
      "Gradient Descent(357/2999): loss=0.3899181009652313, w0=0.00955068859282175, w1=-0.25484344373303386\n",
      "(250000,)\n",
      "Gradient Descent(358/2999): loss=0.38991809642624675, w0=0.00955218177080591, w1=-0.2548425752265547\n",
      "(250000,)\n",
      "Gradient Descent(359/2999): loss=0.38991809192784754, w0=0.00955364959671021, w1=-0.2548417122921825\n",
      "(250000,)\n",
      "Gradient Descent(360/2999): loss=0.38991808746929735, w0=0.009555092522792888, w1=-0.254840854952268\n",
      "(250000,)\n",
      "Gradient Descent(361/2999): loss=0.38991808304988224, w0=0.009556510992683516, w1=-0.2548400032270605\n",
      "(250000,)\n",
      "Gradient Descent(362/2999): loss=0.3899180786689092, w0=0.00955790544155864, w1=-0.25483915713477084\n",
      "(250000,)\n",
      "Gradient Descent(363/2999): loss=0.3899180743257058, w0=0.009559276296313667, w1=-0.25483831669163226\n",
      "(250000,)\n",
      "Gradient Descent(364/2999): loss=0.38991807001961903, w0=0.009560623975731092, w1=-0.25483748191196076\n",
      "(250000,)\n",
      "Gradient Descent(365/2999): loss=0.38991806575001564, w0=0.009561948890645036, w1=-0.2548366528082131\n",
      "(250000,)\n",
      "Gradient Descent(366/2999): loss=0.38991806151628017, w0=0.009563251444102307, w1=-0.2548358293910439\n",
      "(250000,)\n",
      "Gradient Descent(367/2999): loss=0.38991805731781487, w0=0.009564532031520025, w1=-0.2548350116693611\n",
      "(250000,)\n",
      "Gradient Descent(368/2999): loss=0.38991805315403966, w0=0.009565791040839823, w1=-0.25483419965038034\n",
      "(250000,)\n",
      "Gradient Descent(369/2999): loss=0.38991804902439064, w0=0.009567028852678794, w1=-0.2548333933396776\n",
      "(250000,)\n",
      "Gradient Descent(370/2999): loss=0.38991804492832005, w0=0.009568245840477193, w1=-0.25483259274124087\n",
      "(250000,)\n",
      "Gradient Descent(371/2999): loss=0.3899180408652955, w0=0.009569442370642976, w1=-0.2548317978575206\n",
      "(250000,)\n",
      "Gradient Descent(372/2999): loss=0.38991803683479964, w0=0.009570618802693307, w1=-0.25483100868947844\n",
      "(250000,)\n",
      "Gradient Descent(373/2999): loss=0.3899180328363297, w0=0.009571775489392986, w1=-0.2548302252366355\n",
      "(250000,)\n",
      "Gradient Descent(374/2999): loss=0.38991802886939625, w0=0.009572912776889986, w1=-0.2548294474971186\n",
      "(250000,)\n",
      "Gradient Descent(375/2999): loss=0.38991802493352384, w0=0.009574031004848061, w1=-0.25482867546770616\n",
      "(250000,)\n",
      "Gradient Descent(376/2999): loss=0.38991802102824974, w0=0.009575130506576577, w1=-0.2548279091438722\n",
      "(250000,)\n",
      "Gradient Descent(377/2999): loss=0.38991801715312396, w0=0.009576211609157564, w1=-0.25482714851982996\n",
      "(250000,)\n",
      "Gradient Descent(378/2999): loss=0.3899180133077085, w0=0.0095772746335701, w1=-0.25482639358857384\n",
      "(250000,)\n",
      "Gradient Descent(379/2999): loss=0.3899180094915771, w0=0.009578319894812044, w1=-0.2548256443419206\n",
      "(250000,)\n",
      "Gradient Descent(380/2999): loss=0.389918005704315, w0=0.00957934770201918, w1=-0.25482490077054964\n",
      "(250000,)\n",
      "Gradient Descent(381/2999): loss=0.38991800194551807, w0=0.009580358358581873, w1=-0.2548241628640417\n",
      "(250000,)\n",
      "Gradient Descent(382/2999): loss=0.38991799821479295, w0=0.009581352162259239, w1=-0.25482343061091733\n",
      "(250000,)\n",
      "Gradient Descent(383/2999): loss=0.38991799451175657, w0=0.009582329405290942, w1=-0.2548227039986738\n",
      "(250000,)\n",
      "Gradient Descent(384/2999): loss=0.38991799083603557, w0=0.00958329037450659, w1=-0.25482198301382136\n",
      "(250000,)\n",
      "Gradient Descent(385/2999): loss=0.38991798718726617, w0=0.009584235351432869, w1=-0.2548212676419186\n",
      "(250000,)\n",
      "Gradient Descent(386/2999): loss=0.38991798356509405, w0=0.009585164612398405, w1=-0.25482055786760655\n",
      "(250000,)\n",
      "Gradient Descent(387/2999): loss=0.38991797996917354, w0=0.009586078428636414, w1=-0.2548198536746425\n",
      "(250000,)\n",
      "Gradient Descent(388/2999): loss=0.38991797639916764, w0=0.009586977066385217, w1=-0.25481915504593244\n",
      "(250000,)\n",
      "Gradient Descent(389/2999): loss=0.38991797285474816, w0=0.009587860786986667, w1=-0.2548184619635629\n",
      "(250000,)\n",
      "Gradient Descent(390/2999): loss=0.38991796933559436, w0=0.00958872984698247, w1=-0.2548177744088318\n",
      "(250000,)\n",
      "Gradient Descent(391/2999): loss=0.38991796584139343, w0=0.009589584498208516, w1=-0.2548170923622788\n",
      "(250000,)\n",
      "Gradient Descent(392/2999): loss=0.3899179623718405, w0=0.009590424987887257, w1=-0.25481641580371456\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(393/2999): loss=0.3899179589266377, w0=0.009591251558718107, w1=-0.2548157447122494\n",
      "(250000,)\n",
      "Gradient Descent(394/2999): loss=0.3899179555054943, w0=0.009592064448966051, w1=-0.25481507906632106\n",
      "(250000,)\n",
      "Gradient Descent(395/2999): loss=0.3899179521081264, w0=0.00959286389254828, w1=-0.254814418843722\n",
      "(250000,)\n",
      "Gradient Descent(396/2999): loss=0.38991794873425684, w0=0.009593650119119125, w1=-0.2548137640216258\n",
      "(250000,)\n",
      "Gradient Descent(397/2999): loss=0.3899179453836146, w0=0.009594423354153215, w1=-0.25481311457661276\n",
      "(250000,)\n",
      "Gradient Descent(398/2999): loss=0.3899179420559352, w0=0.009595183819026869, w1=-0.2548124704846949\n",
      "(250000,)\n",
      "Gradient Descent(399/2999): loss=0.3899179387509601, w0=0.009595931731097832, w1=-0.2548118317213406\n",
      "(250000,)\n",
      "Gradient Descent(400/2999): loss=0.3899179354684362, w0=0.009596667303783335, w1=-0.25481119826149806\n",
      "(250000,)\n",
      "Gradient Descent(401/2999): loss=0.3899179322081166, w0=0.009597390746636594, w1=-0.2548105700796185\n",
      "(250000,)\n",
      "Gradient Descent(402/2999): loss=0.3899179289697596, w0=0.009598102265421637, w1=-0.2548099471496786\n",
      "(250000,)\n",
      "Gradient Descent(403/2999): loss=0.3899179257531285, w0=0.00959880206218669, w1=-0.2548093294452024\n",
      "(250000,)\n",
      "Gradient Descent(404/2999): loss=0.3899179225579921, w0=0.009599490335335968, w1=-0.2548087169392825\n",
      "(250000,)\n",
      "Gradient Descent(405/2999): loss=0.3899179193841242, w0=0.00960016727970006, w1=-0.25480810960460076\n",
      "(250000,)\n",
      "Gradient Descent(406/2999): loss=0.38991791623130306, w0=0.009600833086604792, w1=-0.2548075074134485\n",
      "(250000,)\n",
      "Gradient Descent(407/2999): loss=0.38991791309931156, w0=0.009601487943938749, w1=-0.254806910337746\n",
      "(250000,)\n",
      "Gradient Descent(408/2999): loss=0.3899179099879377, w0=0.00960213203621933, w1=-0.25480631834906153\n",
      "(250000,)\n",
      "Gradient Descent(409/2999): loss=0.38991790689697325, w0=0.009602765544657558, w1=-0.25480573141862983\n",
      "(250000,)\n",
      "Gradient Descent(410/2999): loss=0.3899179038262143, w0=0.009603388647221427, w1=-0.25480514951737027\n",
      "(250000,)\n",
      "Gradient Descent(411/2999): loss=0.3899179007754613, w0=0.009604001518698082, w1=-0.2548045726159041\n",
      "(250000,)\n",
      "Gradient Descent(412/2999): loss=0.38991789774451835, w0=0.009604604330754675, w1=-0.25480400068457165\n",
      "(250000,)\n",
      "Gradient Descent(413/2999): loss=0.3899178947331937, w0=0.009605197251997929, w1=-0.2548034336934487\n",
      "(250000,)\n",
      "Gradient Descent(414/2999): loss=0.3899178917412993, w0=0.009605780448032588, w1=-0.25480287161236265\n",
      "(250000,)\n",
      "Gradient Descent(415/2999): loss=0.3899178887686505, w0=0.009606354081518617, w1=-0.254802314410908\n",
      "(250000,)\n",
      "Gradient Descent(416/2999): loss=0.3899178858150663, w0=0.009606918312227241, w1=-0.2548017620584616\n",
      "(250000,)\n",
      "Gradient Descent(417/2999): loss=0.3899178828803692, w0=0.009607473297095858, w1=-0.25480121452419746\n",
      "(250000,)\n",
      "Gradient Descent(418/2999): loss=0.3899178799643851, w0=0.009608019190281857, w1=-0.2548006717771007\n",
      "(250000,)\n",
      "Gradient Descent(419/2999): loss=0.3899178770669426, w0=0.009608556143215286, w1=-0.2548001337859819\n",
      "(250000,)\n",
      "Gradient Descent(420/2999): loss=0.38991787418787405, w0=0.009609084304650531, w1=-0.2547996005194901\n",
      "(250000,)\n",
      "Gradient Descent(421/2999): loss=0.3899178713270145, w0=0.009609603820716899, w1=-0.2547990719461263\n",
      "(250000,)\n",
      "Gradient Descent(422/2999): loss=0.38991786848420196, w0=0.009610114834968202, w1=-0.2547985480342558\n",
      "(250000,)\n",
      "Gradient Descent(423/2999): loss=0.3899178656592773, w0=0.009610617488431294, w1=-0.2547980287521208\n",
      "(250000,)\n",
      "Gradient Descent(424/2999): loss=0.3899178628520843, w0=0.00961111191965374, w1=-0.2547975140678523\n",
      "(250000,)\n",
      "Gradient Descent(425/2999): loss=0.38991786006246926, w0=0.009611598264750417, w1=-0.2547970039494814\n",
      "(250000,)\n",
      "Gradient Descent(426/2999): loss=0.3899178572902812, w0=0.00961207665744921, w1=-0.254796498364951\n",
      "(250000,)\n",
      "Gradient Descent(427/2999): loss=0.3899178545353716, w0=0.009612547229135831, w1=-0.2547959972821263\n",
      "(250000,)\n",
      "Gradient Descent(428/2999): loss=0.3899178517975945, w0=0.009613010108897682, w1=-0.25479550066880574\n",
      "(250000,)\n",
      "Gradient Descent(429/2999): loss=0.3899178490768063, w0=0.009613465423566898, w1=-0.25479500849273085\n",
      "(250000,)\n",
      "Gradient Descent(430/2999): loss=0.3899178463728657, w0=0.009613913297762473, w1=-0.25479452072159653\n",
      "(250000,)\n",
      "Gradient Descent(431/2999): loss=0.3899178436856337, w0=0.009614353853931567, w1=-0.2547940373230605\n",
      "(250000,)\n",
      "Gradient Descent(432/2999): loss=0.3899178410149737, w0=0.009614787212390036, w1=-0.25479355826475264\n",
      "(250000,)\n",
      "Gradient Descent(433/2999): loss=0.3899178383607507, w0=0.009615213491362046, w1=-0.254793083514284\n",
      "(250000,)\n",
      "Gradient Descent(434/2999): loss=0.38991783572283245, w0=0.009615632807019046, w1=-0.2547926130392555\n",
      "(250000,)\n",
      "Gradient Descent(435/2999): loss=0.38991783310108835, w0=0.009616045273517806, w1=-0.2547921468072666\n",
      "(250000,)\n",
      "Gradient Descent(436/2999): loss=0.3899178304953898, w0=0.009616451003037842, w1=-0.2547916847859232\n",
      "(250000,)\n",
      "Gradient Descent(437/2999): loss=0.3899178279056102, w0=0.009616850105817993, w1=-0.2547912269428458\n",
      "(250000,)\n",
      "Gradient Descent(438/2999): loss=0.38991782533162456, w0=0.00961724269019237, w1=-0.2547907732456769\n",
      "(250000,)\n",
      "Gradient Descent(439/2999): loss=0.3899178227733105, w0=0.009617628862625495, w1=-0.25479032366208876\n",
      "(250000,)\n",
      "Gradient Descent(440/2999): loss=0.38991782023054633, w0=0.009618008727746813, w1=-0.2547898781597902\n",
      "(250000,)\n",
      "Gradient Descent(441/2999): loss=0.389917817703213, w0=0.009618382388384507, w1=-0.2547894367065337\n",
      "(250000,)\n",
      "Gradient Descent(442/2999): loss=0.38991781519119234, w0=0.009618749945598646, w1=-0.25478899927012216\n",
      "(250000,)\n",
      "Gradient Descent(443/2999): loss=0.38991781269436865, w0=0.009619111498713655, w1=-0.2547885658184152\n",
      "(250000,)\n",
      "Gradient Descent(444/2999): loss=0.3899178102126273, w0=0.009619467145350205, w1=-0.2547881363193354\n",
      "(250000,)\n",
      "Gradient Descent(445/2999): loss=0.3899178077458551, w0=0.009619816981456435, w1=-0.2547877107408745\n",
      "(250000,)\n",
      "Gradient Descent(446/2999): loss=0.389917805293941, w0=0.009620161101338565, w1=-0.25478728905109893\n",
      "(250000,)\n",
      "Gradient Descent(447/2999): loss=0.3899178028567749, w0=0.00962049959769091, w1=-0.2547868712181557\n",
      "(250000,)\n",
      "Gradient Descent(448/2999): loss=0.3899178004342482, w0=0.009620832561625325, w1=-0.2547864572102775\n",
      "(250000,)\n",
      "Gradient Descent(449/2999): loss=0.389917798026254, w0=0.009621160082700054, w1=-0.25478604699578816\n",
      "(250000,)\n",
      "Gradient Descent(450/2999): loss=0.38991779563268636, w0=0.00962148224894803, w1=-0.25478564054310754\n",
      "(250000,)\n",
      "Gradient Descent(451/2999): loss=0.38991779325344106, w0=0.009621799146904619, w1=-0.25478523782075635\n",
      "(250000,)\n",
      "Gradient Descent(452/2999): loss=0.38991779088841483, w0=0.009622110861634824, w1=-0.25478483879736075\n",
      "(250000,)\n",
      "Gradient Descent(453/2999): loss=0.38991778853750625, w0=0.009622417476759956, w1=-0.254784443441657\n",
      "(250000,)\n",
      "Gradient Descent(454/2999): loss=0.38991778620061435, w0=0.009622719074483796, w1=-0.2547840517224956\n",
      "(250000,)\n",
      "Gradient Descent(455/2999): loss=0.38991778387764003, w0=0.009623015735618239, w1=-0.2547836636088453\n",
      "(250000,)\n",
      "Gradient Descent(456/2999): loss=0.38991778156848517, w0=0.009623307539608472, w1=-0.25478327906979736\n",
      "(250000,)\n",
      "Gradient Descent(457/2999): loss=0.38991777927305277, w0=0.00962359456455762, w1=-0.25478289807456916\n",
      "(250000,)\n",
      "Gradient Descent(458/2999): loss=0.38991777699124697, w0=0.009623876887250948, w1=-0.25478252059250794\n",
      "(250000,)\n",
      "Gradient Descent(459/2999): loss=0.3899177747229731, w0=0.009624154583179575, w1=-0.2547821465930941\n",
      "(250000,)\n",
      "Gradient Descent(460/2999): loss=0.3899177724681372, w0=0.009624427726563757, w1=-0.25478177604594493\n",
      "(250000,)\n",
      "Gradient Descent(461/2999): loss=0.38991777022664714, w0=0.009624696390375706, w1=-0.25478140892081735\n",
      "(250000,)\n",
      "Gradient Descent(462/2999): loss=0.38991776799841105, w0=0.009624960646361968, w1=-0.25478104518761147\n",
      "(250000,)\n",
      "Gradient Descent(463/2999): loss=0.3899177657833386, w0=0.00962522056506538, w1=-0.25478068481637306\n",
      "(250000,)\n",
      "Gradient Descent(464/2999): loss=0.3899177635813401, w0=0.009625476215846609, w1=-0.2547803277772967\n",
      "(250000,)\n",
      "Gradient Descent(465/2999): loss=0.3899177613923272, w0=0.009625727666905249, w1=-0.25477997404072833\n",
      "(250000,)\n",
      "Gradient Descent(466/2999): loss=0.3899177592162121, w0=0.009625974985300562, w1=-0.2547796235771677\n",
      "(250000,)\n",
      "Gradient Descent(467/2999): loss=0.3899177570529081, w0=0.009626218236971807, w1=-0.254779276357271\n",
      "(250000,)\n",
      "Gradient Descent(468/2999): loss=0.38991775490232955, w0=0.009626457486758153, w1=-0.254778932351853\n",
      "(250000,)\n",
      "Gradient Descent(469/2999): loss=0.3899177527643915, w0=0.009626692798418228, w1=-0.2547785915318894\n",
      "(250000,)\n",
      "Gradient Descent(470/2999): loss=0.3899177506390099, w0=0.009626924234649328, w1=-0.2547782538685188\n",
      "(250000,)\n",
      "Gradient Descent(471/2999): loss=0.3899177485261018, w0=0.009627151857106216, w1=-0.25477791933304467\n",
      "(250000,)\n",
      "Gradient Descent(472/2999): loss=0.3899177464255845, w0=0.009627375726419602, w1=-0.2547775878969373\n",
      "(250000,)\n",
      "Gradient Descent(473/2999): loss=0.3899177443373771, w0=0.009627595902214247, w1=-0.25477725953183544\n",
      "(250000,)\n",
      "Gradient Descent(474/2999): loss=0.3899177422613984, w0=0.009627812443126719, w1=-0.25477693420954817\n",
      "(250000,)\n",
      "Gradient Descent(475/2999): loss=0.38991774019756864, w0=0.009628025406822841, w1=-0.25477661190205636\n",
      "(250000,)\n",
      "Gradient Descent(476/2999): loss=0.3899177381458088, w0=0.009628234850014801, w1=-0.2547762925815141\n",
      "(250000,)\n",
      "Gradient Descent(477/2999): loss=0.3899177361060406, w0=0.009628440828477915, w1=-0.2547759762202501\n",
      "(250000,)\n",
      "Gradient Descent(478/2999): loss=0.3899177340781861, w0=0.009628643397067116, w1=-0.25477566279076913\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(479/2999): loss=0.38991773206216873, w0=0.009628842609733084, w1=-0.25477535226575304\n",
      "(250000,)\n",
      "Gradient Descent(480/2999): loss=0.38991773005791247, w0=0.009629038519538129, w1=-0.25477504461806194\n",
      "(250000,)\n",
      "Gradient Descent(481/2999): loss=0.3899177280653414, w0=0.00962923117867173, w1=-0.2547747398207354\n",
      "(250000,)\n",
      "Gradient Descent(482/2999): loss=0.3899177260843812, w0=0.009629420638465818, w1=-0.25477443784699316\n",
      "(250000,)\n",
      "Gradient Descent(483/2999): loss=0.38991772411495745, w0=0.009629606949409722, w1=-0.25477413867023624\n",
      "(250000,)\n",
      "Gradient Descent(484/2999): loss=0.3899177221569971, w0=0.009629790161164898, w1=-0.25477384226404765\n",
      "(250000,)\n",
      "Gradient Descent(485/2999): loss=0.3899177202104271, w0=0.009629970322579347, w1=-0.25477354860219315\n",
      "(250000,)\n",
      "Gradient Descent(486/2999): loss=0.3899177182751755, w0=0.009630147481701764, w1=-0.2547732576586219\n",
      "(250000,)\n",
      "Gradient Descent(487/2999): loss=0.3899177163511709, w0=0.009630321685795444, w1=-0.25477296940746713\n",
      "(250000,)\n",
      "Gradient Descent(488/2999): loss=0.3899177144383423, w0=0.009630492981351914, w1=-0.25477268382304663\n",
      "(250000,)\n",
      "Gradient Descent(489/2999): loss=0.3899177125366194, w0=0.009630661414104309, w1=-0.25477240087986314\n",
      "(250000,)\n",
      "Gradient Descent(490/2999): loss=0.3899177106459328, w0=0.009630827029040513, w1=-0.254772120552605\n",
      "(250000,)\n",
      "Gradient Descent(491/2999): loss=0.3899177087662132, w0=0.009630989870416044, w1=-0.2547718428161463\n",
      "(250000,)\n",
      "Gradient Descent(492/2999): loss=0.3899177068973924, w0=0.009631149981766712, w1=-0.25477156764554715\n",
      "(250000,)\n",
      "Gradient Descent(493/2999): loss=0.3899177050394025, w0=0.009631307405921037, w1=-0.25477129501605417\n",
      "(250000,)\n",
      "Gradient Descent(494/2999): loss=0.38991770319217606, w0=0.00963146218501244, w1=-0.25477102490310033\n",
      "(250000,)\n",
      "Gradient Descent(495/2999): loss=0.3899177013556464, w0=0.00963161436049119, w1=-0.25477075728230536\n",
      "(250000,)\n",
      "Gradient Descent(496/2999): loss=0.38991769952974725, w0=0.009631763973136188, w1=-0.25477049212947567\n",
      "(250000,)\n",
      "Gradient Descent(497/2999): loss=0.389917697714413, w0=0.009631911063066457, w1=-0.25477022942060445\n",
      "(250000,)\n",
      "Gradient Descent(498/2999): loss=0.3899176959095785, w0=0.0096320556697525, w1=-0.25476996913187167\n",
      "(250000,)\n",
      "Gradient Descent(499/2999): loss=0.38991769411517907, w0=0.009632197832027359, w1=-0.2547697112396439\n",
      "(250000,)\n",
      "Gradient Descent(500/2999): loss=0.3899176923311507, w0=0.009632337588097567, w1=-0.2547694557204744\n",
      "(250000,)\n",
      "Gradient Descent(501/2999): loss=0.38991769055742953, w0=0.009632474975553845, w1=-0.2547692025511026\n",
      "(250000,)\n",
      "Gradient Descent(502/2999): loss=0.389917688793953, w0=0.009632610031381602, w1=-0.2547689517084543\n",
      "(250000,)\n",
      "Gradient Descent(503/2999): loss=0.38991768704065793, w0=0.009632742791971266, w1=-0.2547687031696414\n",
      "(250000,)\n",
      "Gradient Descent(504/2999): loss=0.3899176852974826, w0=0.009632873293128418, w1=-0.25476845691196126\n",
      "(250000,)\n",
      "Gradient Descent(505/2999): loss=0.38991768356436524, w0=0.009633001570083734, w1=-0.25476821291289675\n",
      "(250000,)\n",
      "Gradient Descent(506/2999): loss=0.38991768184124465, w0=0.009633127657502759, w1=-0.2547679711501157\n",
      "(250000,)\n",
      "Gradient Descent(507/2999): loss=0.3899176801280602, w0=0.009633251589495488, w1=-0.25476773160147065\n",
      "(250000,)\n",
      "Gradient Descent(508/2999): loss=0.3899176784247517, w0=0.009633373399625807, w1=-0.2547674942449984\n",
      "(250000,)\n",
      "Gradient Descent(509/2999): loss=0.38991767673125916, w0=0.009633493120920706, w1=-0.2547672590589194\n",
      "(250000,)\n",
      "Gradient Descent(510/2999): loss=0.38991767504752345, w0=0.009633610785879371, w1=-0.2547670260216375\n",
      "(250000,)\n",
      "Gradient Descent(511/2999): loss=0.3899176733734856, w0=0.009633726426482128, w1=-0.25476679511173916\n",
      "(250000,)\n",
      "Gradient Descent(512/2999): loss=0.38991767170908703, w0=0.009633840074199148, w1=-0.25476656630799327\n",
      "(250000,)\n",
      "Gradient Descent(513/2999): loss=0.3899176700542699, w0=0.009633951759999083, w1=-0.25476633958935024\n",
      "(250000,)\n",
      "Gradient Descent(514/2999): loss=0.38991766840897657, w0=0.009634061514357483, w1=-0.25476611493494156\n",
      "(250000,)\n",
      "Gradient Descent(515/2999): loss=0.38991766677314976, w0=0.009634169367265128, w1=-0.2547658923240791\n",
      "(250000,)\n",
      "Gradient Descent(516/2999): loss=0.38991766514673276, w0=0.009634275348236106, w1=-0.25476567173625453\n",
      "(250000,)\n",
      "Gradient Descent(517/2999): loss=0.3899176635296693, w0=0.009634379486315868, w1=-0.25476545315113863\n",
      "(250000,)\n",
      "Gradient Descent(518/2999): loss=0.3899176619219032, w0=0.009634481810089046, w1=-0.2547652365485805\n",
      "(250000,)\n",
      "Gradient Descent(519/2999): loss=0.3899176603233789, w0=0.009634582347687194, w1=-0.254765021908607\n",
      "(250000,)\n",
      "Gradient Descent(520/2999): loss=0.3899176587340416, w0=0.009634681126796336, w1=-0.25476480921142175\n",
      "(250000,)\n",
      "Gradient Descent(521/2999): loss=0.3899176571538362, w0=0.009634778174664405, w1=-0.2547645984374047\n",
      "(250000,)\n",
      "Gradient Descent(522/2999): loss=0.38991765558270836, w0=0.009634873518108593, w1=-0.25476438956711095\n",
      "(250000,)\n",
      "Gradient Descent(523/2999): loss=0.3899176540206042, w0=0.009634967183522478, w1=-0.2547641825812704\n",
      "(250000,)\n",
      "Gradient Descent(524/2999): loss=0.3899176524674698, w0=0.009635059196883105, w1=-0.25476397746078666\n",
      "(250000,)\n",
      "Gradient Descent(525/2999): loss=0.38991765092325226, w0=0.009635149583757911, w1=-0.254763774186736\n",
      "(250000,)\n",
      "Gradient Descent(526/2999): loss=0.38991764938789847, w0=0.009635238369311513, w1=-0.254763572740367\n",
      "(250000,)\n",
      "Gradient Descent(527/2999): loss=0.3899176478613559, w0=0.00963532557831239, w1=-0.2547633731030992\n",
      "(250000,)\n",
      "Gradient Descent(528/2999): loss=0.38991764634357257, w0=0.009635411235139455, w1=-0.2547631752565226\n",
      "(250000,)\n",
      "Gradient Descent(529/2999): loss=0.3899176448344964, w0=0.009635495363788509, w1=-0.2547629791823965\n",
      "(250000,)\n",
      "Gradient Descent(530/2999): loss=0.38991764333407614, w0=0.009635577987878573, w1=-0.25476278486264864\n",
      "(250000,)\n",
      "Gradient Descent(531/2999): loss=0.3899176418422605, w0=0.0096356591306581, w1=-0.25476259227937437\n",
      "(250000,)\n",
      "Gradient Descent(532/2999): loss=0.38991764035899906, w0=0.009635738815011091, w1=-0.2547624014148356\n",
      "(250000,)\n",
      "Gradient Descent(533/2999): loss=0.3899176388842409, w0=0.009635817063463127, w1=-0.25476221225145984\n",
      "(250000,)\n",
      "Gradient Descent(534/2999): loss=0.38991763741793634, w0=0.009635893898187234, w1=-0.25476202477183935\n",
      "(250000,)\n",
      "Gradient Descent(535/2999): loss=0.3899176359600354, w0=0.009635969341009704, w1=-0.25476183895873006\n",
      "(250000,)\n",
      "Gradient Descent(536/2999): loss=0.38991763451048883, w0=0.009636043413415783, w1=-0.2547616547950506\n",
      "(250000,)\n",
      "Gradient Descent(537/2999): loss=0.38991763306924726, w0=0.009636116136555264, w1=-0.2547614722638813\n",
      "(250000,)\n",
      "Gradient Descent(538/2999): loss=0.3899176316362623, w0=0.009636187531247984, w1=-0.25476129134846337\n",
      "(250000,)\n",
      "Gradient Descent(539/2999): loss=0.3899176302114851, w0=0.009636257617989254, w1=-0.2547611120321976\n",
      "(250000,)\n",
      "Gradient Descent(540/2999): loss=0.3899176287948679, w0=0.00963632641695513, w1=-0.2547609342986435\n",
      "(250000,)\n",
      "Gradient Descent(541/2999): loss=0.38991762738636254, w0=0.009636393948007662, w1=-0.2547607581315182\n",
      "(250000,)\n",
      "Gradient Descent(542/2999): loss=0.38991762598592183, w0=0.009636460230699988, w1=-0.25476058351469566\n",
      "(250000,)\n",
      "Gradient Descent(543/2999): loss=0.38991762459349844, w0=0.009636525284281395, w1=-0.2547604104322052\n",
      "(250000,)\n",
      "Gradient Descent(544/2999): loss=0.3899176232090453, w0=0.009636589127702266, w1=-0.25476023886823085\n",
      "(250000,)\n",
      "Gradient Descent(545/2999): loss=0.38991762183251616, w0=0.009636651779618938, w1=-0.2547600688071101\n",
      "(250000,)\n",
      "Gradient Descent(546/2999): loss=0.38991762046386447, w0=0.009636713258398483, w1=-0.2547599002333329\n",
      "(250000,)\n",
      "Gradient Descent(547/2999): loss=0.3899176191030445, w0=0.009636773582123405, w1=-0.25475973313154054\n",
      "(250000,)\n",
      "Gradient Descent(548/2999): loss=0.38991761775001016, w0=0.009636832768596263, w1=-0.2547595674865248\n",
      "(250000,)\n",
      "Gradient Descent(549/2999): loss=0.38991761640471634, w0=0.009636890835344197, w1=-0.2547594032832265\n",
      "(250000,)\n",
      "Gradient Descent(550/2999): loss=0.389917615067118, w0=0.00963694779962339, w1=-0.2547592405067348\n",
      "(250000,)\n",
      "Gradient Descent(551/2999): loss=0.38991761373717004, w0=0.009637003678423461, w1=-0.254759079142286\n",
      "(250000,)\n",
      "Gradient Descent(552/2999): loss=0.38991761241482814, w0=0.009637058488471762, w1=-0.25475891917526233\n",
      "(250000,)\n",
      "Gradient Descent(553/2999): loss=0.3899176111000479, w0=0.009637112246237608, w1=-0.25475876059119107\n",
      "(250000,)\n",
      "Gradient Descent(554/2999): loss=0.3899176097927855, w0=0.009637164967936446, w1=-0.2547586033757434\n",
      "(250000,)\n",
      "Gradient Descent(555/2999): loss=0.389917608492997, w0=0.009637216669533948, w1=-0.25475844751473326\n",
      "(250000,)\n",
      "Gradient Descent(556/2999): loss=0.38991760720063917, w0=0.009637267366750007, w1=-0.25475829299411634\n",
      "(250000,)\n",
      "Gradient Descent(557/2999): loss=0.3899176059156688, w0=0.009637317075062709, w1=-0.25475813979998896\n",
      "(250000,)\n",
      "Gradient Descent(558/2999): loss=0.3899176046380431, w0=0.00963736580971221, w1=-0.25475798791858695\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(559/2999): loss=0.3899176033677193, w0=0.009637413585704569, w1=-0.25475783733628476\n",
      "(250000,)\n",
      "Gradient Descent(560/2999): loss=0.38991760210465504, w0=0.00963746041781547, w1=-0.2547576880395941\n",
      "(250000,)\n",
      "Gradient Descent(561/2999): loss=0.38991760084880844, w0=0.009637506320593939, w1=-0.25475754001516304\n",
      "(250000,)\n",
      "Gradient Descent(562/2999): loss=0.3899175996001374, w0=0.009637551308365955, w1=-0.25475739324977487\n",
      "(250000,)\n",
      "Gradient Descent(563/2999): loss=0.38991759835860057, w0=0.00963759539523801, w1=-0.254757247730347\n",
      "(250000,)\n",
      "Gradient Descent(564/2999): loss=0.38991759712415636, w0=0.009637638595100638, w1=-0.25475710344393\n",
      "(250000,)\n",
      "Gradient Descent(565/2999): loss=0.3899175958967641, w0=0.009637680921631819, w1=-0.25475696037770623\n",
      "(250000,)\n",
      "Gradient Descent(566/2999): loss=0.38991759467638276, w0=0.00963772238830042, w1=-0.2547568185189891\n",
      "(250000,)\n",
      "Gradient Descent(567/2999): loss=0.3899175934629717, w0=0.009637763008369469, w1=-0.2547566778552219\n",
      "(250000,)\n",
      "Gradient Descent(568/2999): loss=0.3899175922564909, w0=0.009637802794899465, w1=-0.2547565383739765\n",
      "(250000,)\n",
      "Gradient Descent(569/2999): loss=0.3899175910569001, w0=0.009637841760751602, w1=-0.2547564000629526\n",
      "(250000,)\n",
      "Gradient Descent(570/2999): loss=0.3899175898641595, w0=0.009637879918590901, w1=-0.2547562629099764\n",
      "(250000,)\n",
      "Gradient Descent(571/2999): loss=0.3899175886782295, w0=0.009637917280889337, w1=-0.2547561269029997\n",
      "(250000,)\n",
      "Gradient Descent(572/2999): loss=0.389917587499071, w0=0.009637953859928902, w1=-0.2547559920300988\n",
      "(250000,)\n",
      "Gradient Descent(573/2999): loss=0.3899175863266446, w0=0.009637989667804622, w1=-0.25475585827947334\n",
      "(250000,)\n",
      "Gradient Descent(574/2999): loss=0.38991758516091174, w0=0.009638024716427485, w1=-0.25475572563944543\n",
      "(250000,)\n",
      "Gradient Descent(575/2999): loss=0.3899175840018335, w0=0.009638059017527371, w1=-0.2547555940984584\n",
      "(250000,)\n",
      "Gradient Descent(576/2999): loss=0.3899175828493716, w0=0.009638092582655914, w1=-0.2547554636450758\n",
      "(250000,)\n",
      "Gradient Descent(577/2999): loss=0.38991758170348817, w0=0.0096381254231893, w1=-0.25475533426798047\n",
      "(250000,)\n",
      "Gradient Descent(578/2999): loss=0.38991758056414505, w0=0.00963815755033104, w1=-0.2547552059559734\n",
      "(250000,)\n",
      "Gradient Descent(579/2999): loss=0.38991757943130456, w0=0.009638188975114686, w1=-0.25475507869797265\n",
      "(250000,)\n",
      "Gradient Descent(580/2999): loss=0.3899175783049293, w0=0.009638219708406526, w1=-0.2547549524830124\n",
      "(250000,)\n",
      "Gradient Descent(581/2999): loss=0.38991757718498204, w0=0.009638249760908157, w1=-0.2547548273002418\n",
      "(250000,)\n",
      "Gradient Descent(582/2999): loss=0.38991757607142563, w0=0.009638279143159137, w1=-0.2547547031389241\n",
      "(250000,)\n",
      "Gradient Descent(583/2999): loss=0.38991757496422347, w0=0.009638307865539484, w1=-0.2547545799884356\n",
      "(250000,)\n",
      "Gradient Descent(584/2999): loss=0.38991757386333875, w0=0.009638335938272192, w1=-0.2547544578382644\n",
      "(250000,)\n",
      "Gradient Descent(585/2999): loss=0.38991757276873545, w0=0.009638363371425675, w1=-0.25475433667800973\n",
      "(250000,)\n",
      "Gradient Descent(586/2999): loss=0.3899175716803773, w0=0.009638390174916233, w1=-0.25475421649738067\n",
      "(250000,)\n",
      "Gradient Descent(587/2999): loss=0.38991757059822824, w0=0.00963841635851036, w1=-0.2547540972861953\n",
      "(250000,)\n",
      "Gradient Descent(588/2999): loss=0.3899175695222528, w0=0.009638441931827153, w1=-0.2547539790343797\n",
      "(250000,)\n",
      "Gradient Descent(589/2999): loss=0.38991756845241526, w0=0.009638466904340559, w1=-0.25475386173196674\n",
      "(250000,)\n",
      "Gradient Descent(590/2999): loss=0.3899175673886804, w0=0.00963849128538167, w1=-0.2547537453690954\n",
      "(250000,)\n",
      "Gradient Descent(591/2999): loss=0.38991756633101343, w0=0.009638515084140927, w1=-0.2547536299360096\n",
      "(250000,)\n",
      "Gradient Descent(592/2999): loss=0.3899175652793791, w0=0.00963853830967033, w1=-0.2547535154230572\n",
      "(250000,)\n",
      "Gradient Descent(593/2999): loss=0.389917564233743, w0=0.009638560970885578, w1=-0.2547534018206891\n",
      "(250000,)\n",
      "Gradient Descent(594/2999): loss=0.38991756319407067, w0=0.00963858307656819, w1=-0.25475328911945844\n",
      "(250000,)\n",
      "Gradient Descent(595/2999): loss=0.3899175621603279, w0=0.009638604635367604, w1=-0.2547531773100192\n",
      "(250000,)\n",
      "Gradient Descent(596/2999): loss=0.3899175611324805, w0=0.009638625655803203, w1=-0.25475306638312556\n",
      "(250000,)\n",
      "Gradient Descent(597/2999): loss=0.3899175601104946, w0=0.00963864614626633, w1=-0.25475295632963096\n",
      "(250000,)\n",
      "Gradient Descent(598/2999): loss=0.38991755909433684, w0=0.00963866611502231, w1=-0.25475284714048707\n",
      "(250000,)\n",
      "Gradient Descent(599/2999): loss=0.3899175580839737, w0=0.009638685570212337, w1=-0.2547527388067428\n",
      "(250000,)\n",
      "Gradient Descent(600/2999): loss=0.3899175570793717, w0=0.009638704519855455, w1=-0.2547526313195435\n",
      "(250000,)\n",
      "Gradient Descent(601/2999): loss=0.38991755608049794, w0=0.009638722971850413, w1=-0.2547525246701298\n",
      "(250000,)\n",
      "Gradient Descent(602/2999): loss=0.3899175550873197, w0=0.009638740933977529, w1=-0.25475241884983707\n",
      "(250000,)\n",
      "Gradient Descent(603/2999): loss=0.38991755409980416, w0=0.009638758413900517, w1=-0.25475231385009406\n",
      "(250000,)\n",
      "Gradient Descent(604/2999): loss=0.389917553117919, w0=0.00963877541916827, w1=-0.2547522096624223\n",
      "(250000,)\n",
      "Gradient Descent(605/2999): loss=0.38991755214163176, w0=0.00963879195721667, w1=-0.25475210627843514\n",
      "(250000,)\n",
      "Gradient Descent(606/2999): loss=0.3899175511709106, w0=0.009638808035370287, w1=-0.25475200368983675\n",
      "(250000,)\n",
      "Gradient Descent(607/2999): loss=0.3899175502057235, w0=0.009638823660844096, w1=-0.2547519018884213\n",
      "(250000,)\n",
      "Gradient Descent(608/2999): loss=0.38991754924603855, w0=0.009638838840745192, w1=-0.254751800866072\n",
      "(250000,)\n",
      "Gradient Descent(609/2999): loss=0.38991754829182457, w0=0.009638853582074393, w1=-0.2547517006147605\n",
      "(250000,)\n",
      "Gradient Descent(610/2999): loss=0.3899175473430501, w0=0.009638867891727918, w1=-0.2547516011265455\n",
      "(250000,)\n",
      "Gradient Descent(611/2999): loss=0.3899175463996839, w0=0.009638881776498975, w1=-0.25475150239357236\n",
      "(250000,)\n",
      "Gradient Descent(612/2999): loss=0.3899175454616951, w0=0.009638895243079343, w1=-0.25475140440807204\n",
      "(250000,)\n",
      "Gradient Descent(613/2999): loss=0.3899175445290529, w0=0.009638908298060904, w1=-0.2547513071623602\n",
      "(250000,)\n",
      "Gradient Descent(614/2999): loss=0.3899175436017267, w0=0.009638920947937196, w1=-0.2547512106488365\n",
      "(250000,)\n",
      "Gradient Descent(615/2999): loss=0.38991754267968615, w0=0.009638933199104908, w1=-0.2547511148599836\n",
      "(250000,)\n",
      "Gradient Descent(616/2999): loss=0.3899175417629008, w0=0.009638945057865344, w1=-0.2547510197883664\n",
      "(250000,)\n",
      "Gradient Descent(617/2999): loss=0.38991754085134056, w0=0.00963895653042591, w1=-0.25475092542663125\n",
      "(250000,)\n",
      "Gradient Descent(618/2999): loss=0.3899175399449758, w0=0.00963896762290152, w1=-0.254750831767505\n",
      "(250000,)\n",
      "Gradient Descent(619/2999): loss=0.3899175390437766, w0=0.009638978341316025, w1=-0.25475073880379434\n",
      "(250000,)\n",
      "Gradient Descent(620/2999): loss=0.3899175381477134, w0=0.009638988691603569, w1=-0.25475064652838486\n",
      "(250000,)\n",
      "Gradient Descent(621/2999): loss=0.3899175372567568, w0=0.009638998679609982, w1=-0.2547505549342403\n",
      "(250000,)\n",
      "Gradient Descent(622/2999): loss=0.3899175363708778, w0=0.009639008311094132, w1=-0.25475046401440177\n",
      "(250000,)\n",
      "Gradient Descent(623/2999): loss=0.3899175354900472, w0=0.009639017591729204, w1=-0.2547503737619869\n",
      "(250000,)\n",
      "Gradient Descent(624/2999): loss=0.3899175346142361, w0=0.009639026527104036, w1=-0.2547502841701891\n",
      "(250000,)\n",
      "Gradient Descent(625/2999): loss=0.3899175337434159, w0=0.00963903512272439, w1=-0.25475019523227677\n",
      "(250000,)\n",
      "Gradient Descent(626/2999): loss=0.38991753287755804, w0=0.009639043384014206, w1=-0.2547501069415925\n",
      "(250000,)\n",
      "Gradient Descent(627/2999): loss=0.3899175320166341, w0=0.009639051316316859, w1=-0.25475001929155233\n",
      "(250000,)\n",
      "Gradient Descent(628/2999): loss=0.3899175311606159, w0=0.009639058924896339, w1=-0.254749932275645\n",
      "(250000,)\n",
      "Gradient Descent(629/2999): loss=0.38991753030947535, w0=0.009639066214938527, w1=-0.25474984588743116\n",
      "(250000,)\n",
      "Gradient Descent(630/2999): loss=0.38991752946318464, w0=0.009639073191552277, w1=-0.2547497601205427\n",
      "(250000,)\n",
      "Gradient Descent(631/2999): loss=0.38991752862171614, w0=0.009639079859770661, w1=-0.2547496749686818\n",
      "(250000,)\n",
      "Gradient Descent(632/2999): loss=0.38991752778504213, w0=0.009639086224552073, w1=-0.2547495904256205\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(633/2999): loss=0.38991752695313525, w0=0.009639092290781378, w1=-0.2547495064851998\n",
      "(250000,)\n",
      "Gradient Descent(634/2999): loss=0.3899175261259683, w0=0.009639098063270993, w1=-0.2547494231413288\n",
      "(250000,)\n",
      "Gradient Descent(635/2999): loss=0.3899175253035142, w0=0.009639103546762007, w1=-0.2547493403879843\n",
      "(250000,)\n",
      "Gradient Descent(636/2999): loss=0.3899175244857459, w0=0.009639108745925237, w1=-0.2547492582192098\n",
      "(250000,)\n",
      "Gradient Descent(637/2999): loss=0.38991752367263693, w0=0.009639113665362298, w1=-0.25474917662911495\n",
      "(250000,)\n",
      "Gradient Descent(638/2999): loss=0.3899175228641605, w0=0.009639118309606632, w1=-0.2547490956118748\n",
      "(250000,)\n",
      "Gradient Descent(639/2999): loss=0.38991752206029, w0=0.009639122683124538, w1=-0.2547490151617292\n",
      "(250000,)\n",
      "Gradient Descent(640/2999): loss=0.3899175212609993, w0=0.009639126790316203, w1=-0.2547489352729819\n",
      "(250000,)\n",
      "Gradient Descent(641/2999): loss=0.38991752046626227, w0=0.009639130635516633, w1=-0.2547488559400002\n",
      "(250000,)\n",
      "Gradient Descent(642/2999): loss=0.3899175196760528, w0=0.009639134222996693, w1=-0.2547487771572138\n",
      "(250000,)\n",
      "Gradient Descent(643/2999): loss=0.3899175188903451, w0=0.009639137556964052, w1=-0.2547486989191146\n",
      "(250000,)\n",
      "Gradient Descent(644/2999): loss=0.3899175181091136, w0=0.009639140641564107, w1=-0.25474862122025593\n",
      "(250000,)\n",
      "Gradient Descent(645/2999): loss=0.3899175173323324, w0=0.009639143480880942, w1=-0.2547485440552516\n",
      "(250000,)\n",
      "Gradient Descent(646/2999): loss=0.3899175165599765, w0=0.009639146078938243, w1=-0.25474846741877555\n",
      "(250000,)\n",
      "Gradient Descent(647/2999): loss=0.3899175157920202, w0=0.009639148439700162, w1=-0.2547483913055611\n",
      "(250000,)\n",
      "Gradient Descent(648/2999): loss=0.3899175150284387, w0=0.009639150567072265, w1=-0.25474831571040035\n",
      "(250000,)\n",
      "Gradient Descent(649/2999): loss=0.3899175142692071, w0=0.009639152464902387, w1=-0.2547482406281435\n",
      "(250000,)\n",
      "Gradient Descent(650/2999): loss=0.3899175135143003, w0=0.009639154136981461, w1=-0.25474816605369827\n",
      "(250000,)\n",
      "Gradient Descent(651/2999): loss=0.3899175127636937, w0=0.00963915558704441, w1=-0.2547480919820292\n",
      "(250000,)\n",
      "Gradient Descent(652/2999): loss=0.3899175120173629, w0=0.009639156818770965, w1=-0.25474801840815714\n",
      "(250000,)\n",
      "Gradient Descent(653/2999): loss=0.3899175112752833, w0=0.00963915783578648, w1=-0.25474794532715855\n",
      "(250000,)\n",
      "Gradient Descent(654/2999): loss=0.3899175105374308, w0=0.009639158641662765, w1=-0.254747872734165\n",
      "(250000,)\n",
      "Gradient Descent(655/2999): loss=0.38991750980378115, w0=0.009639159239918848, w1=-0.2547478006243624\n",
      "(250000,)\n",
      "Gradient Descent(656/2999): loss=0.38991750907431044, w0=0.009639159634021777, w1=-0.2547477289929906\n",
      "(250000,)\n",
      "Gradient Descent(657/2999): loss=0.38991750834899486, w0=0.009639159827387405, w1=-0.2547476578353426\n",
      "(250000,)\n",
      "Gradient Descent(658/2999): loss=0.38991750762781063, w0=0.009639159823381132, w1=-0.2547475871467642\n",
      "(250000,)\n",
      "Gradient Descent(659/2999): loss=0.3899175069107343, w0=0.009639159625318684, w1=-0.25474751692265324\n",
      "(250000,)\n",
      "Gradient Descent(660/2999): loss=0.3899175061977422, w0=0.009639159236466808, w1=-0.25474744715845915\n",
      "(250000,)\n",
      "Gradient Descent(661/2999): loss=0.38991750548881127, w0=0.009639158660044023, w1=-0.25474737784968227\n",
      "(250000,)\n",
      "Gradient Descent(662/2999): loss=0.38991750478391823, w0=0.009639157899221335, w1=-0.2547473089918733\n",
      "(250000,)\n",
      "Gradient Descent(663/2999): loss=0.38991750408304016, w0=0.00963915695712293, w1=-0.25474724058063297\n",
      "(250000,)\n",
      "Gradient Descent(664/2999): loss=0.38991750338615394, w0=0.009639155836826884, w1=-0.2547471726116111\n",
      "(250000,)\n",
      "Gradient Descent(665/2999): loss=0.389917502693237, w0=0.009639154541365826, w1=-0.25474710508050646\n",
      "(250000,)\n",
      "Gradient Descent(666/2999): loss=0.3899175020042665, w0=0.009639153073727631, w1=-0.25474703798306586\n",
      "(250000,)\n",
      "Gradient Descent(667/2999): loss=0.38991750131922026, w0=0.009639151436856062, w1=-0.254746971315084\n",
      "(250000,)\n",
      "Gradient Descent(668/2999): loss=0.3899175006380756, w0=0.009639149633651437, w1=-0.25474690507240255\n",
      "(250000,)\n",
      "Gradient Descent(669/2999): loss=0.3899174999608104, w0=0.009639147666971253, w1=-0.25474683925090996\n",
      "(250000,)\n",
      "Gradient Descent(670/2999): loss=0.38991749928740255, w0=0.009639145539630842, w1=-0.2547467738465407\n",
      "(250000,)\n",
      "Gradient Descent(671/2999): loss=0.3899174986178299, w0=0.009639143254403964, w1=-0.25474670885527495\n",
      "(250000,)\n",
      "Gradient Descent(672/2999): loss=0.38991749795207087, w0=0.009639140814023449, w1=-0.2547466442731379\n",
      "(250000,)\n",
      "Gradient Descent(673/2999): loss=0.3899174972901034, w0=0.009639138221181783, w1=-0.2547465800961994\n",
      "(250000,)\n",
      "Gradient Descent(674/2999): loss=0.38991749663190595, w0=0.009639135478531704, w1=-0.2547465163205733\n",
      "(250000,)\n",
      "Gradient Descent(675/2999): loss=0.3899174959774573, w0=0.009639132588686804, w1=-0.25474645294241727\n",
      "(250000,)\n",
      "Gradient Descent(676/2999): loss=0.38991749532673575, w0=0.00963912955422206, w1=-0.25474638995793186\n",
      "(250000,)\n",
      "Gradient Descent(677/2999): loss=0.3899174946797201, w0=0.009639126377674456, w1=-0.2547463273633604\n",
      "(250000,)\n",
      "Gradient Descent(678/2999): loss=0.38991749403638937, w0=0.009639123061543521, w1=-0.2547462651549883\n",
      "(250000,)\n",
      "Gradient Descent(679/2999): loss=0.3899174933967224, w0=0.009639119608291863, w1=-0.25474620332914283\n",
      "(250000,)\n",
      "Gradient Descent(680/2999): loss=0.38991749276069837, w0=0.009639116020345726, w1=-0.25474614188219225\n",
      "(250000,)\n",
      "Gradient Descent(681/2999): loss=0.3899174921282965, w0=0.009639112300095539, w1=-0.2547460808105457\n",
      "(250000,)\n",
      "Gradient Descent(682/2999): loss=0.3899174914994961, w0=0.009639108449896407, w1=-0.2547460201106527\n",
      "(250000,)\n",
      "Gradient Descent(683/2999): loss=0.38991749087427685, w0=0.009639104472068647, w1=-0.25474595977900244\n",
      "(250000,)\n",
      "Gradient Descent(684/2999): loss=0.38991749025261807, w0=0.009639100368898318, w1=-0.25474589981212364\n",
      "(250000,)\n",
      "Gradient Descent(685/2999): loss=0.3899174896344996, w0=0.00963909614263768, w1=-0.2547458402065839\n",
      "(250000,)\n",
      "Gradient Descent(686/2999): loss=0.38991748901990125, w0=0.009639091795505718, w1=-0.2547457809589894\n",
      "(250000,)\n",
      "Gradient Descent(687/2999): loss=0.38991748840880297, w0=0.009639087329688637, w1=-0.25474572206598434\n",
      "(250000,)\n",
      "Gradient Descent(688/2999): loss=0.3899174878011847, w0=0.009639082747340306, w1=-0.25474566352425054\n",
      "(250000,)\n",
      "Gradient Descent(689/2999): loss=0.38991748719702674, w0=0.00963907805058276, w1=-0.2547456053305072\n",
      "(250000,)\n",
      "Gradient Descent(690/2999): loss=0.38991748659630926, w0=0.009639073241506672, w1=-0.2547455474815101\n",
      "(250000,)\n",
      "Gradient Descent(691/2999): loss=0.38991748599901277, w0=0.009639068322171786, w1=-0.2547454899740516\n",
      "(250000,)\n",
      "Gradient Descent(692/2999): loss=0.3899174854051176, w0=0.009639063294607385, w1=-0.2547454328049599\n",
      "(250000,)\n",
      "Gradient Descent(693/2999): loss=0.3899174848146046, w0=0.009639058160812721, w1=-0.25474537597109886\n",
      "(250000,)\n",
      "Gradient Descent(694/2999): loss=0.38991748422745426, w0=0.009639052922757472, w1=-0.2547453194693674\n",
      "(250000,)\n",
      "Gradient Descent(695/2999): loss=0.38991748364364753, w0=0.009639047582382163, w1=-0.2547452632966992\n",
      "(250000,)\n",
      "Gradient Descent(696/2999): loss=0.3899174830631654, w0=0.009639042141598587, w1=-0.25474520745006246\n",
      "(250000,)\n",
      "Gradient Descent(697/2999): loss=0.38991748248598884, w0=0.009639036602290232, w1=-0.25474515192645913\n",
      "(250000,)\n",
      "Gradient Descent(698/2999): loss=0.3899174819120992, w0=0.009639030966312696, w1=-0.25474509672292484\n",
      "(250000,)\n",
      "Gradient Descent(699/2999): loss=0.38991748134147736, w0=0.009639025235494092, w1=-0.2547450418365284\n",
      "(250000,)\n",
      "Gradient Descent(700/2999): loss=0.38991748077410504, w0=0.009639019411635433, w1=-0.25474498726437156\n",
      "(250000,)\n",
      "Gradient Descent(701/2999): loss=0.38991748020996353, w0=0.009639013496511048, w1=-0.2547449330035883\n",
      "(250000,)\n",
      "Gradient Descent(702/2999): loss=0.3899174796490348, w0=0.009639007491868948, w1=-0.25474487905134485\n",
      "(250000,)\n",
      "Gradient Descent(703/2999): loss=0.38991747909130003, w0=0.009639001399431232, w1=-0.25474482540483906\n",
      "(250000,)\n",
      "Gradient Descent(704/2999): loss=0.38991747853674114, w0=0.009638995220894446, w1=-0.25474477206130014\n",
      "(250000,)\n",
      "Gradient Descent(705/2999): loss=0.3899174779853403, w0=0.009638988957929967, w1=-0.2547447190179883\n",
      "(250000,)\n",
      "Gradient Descent(706/2999): loss=0.38991747743707933, w0=0.009638982612184347, w1=-0.25474466627219433\n",
      "(250000,)\n",
      "Gradient Descent(707/2999): loss=0.38991747689194045, w0=0.009638976185279697, w1=-0.2547446138212393\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(708/2999): loss=0.38991747634990565, w0=0.00963896967881404, w1=-0.2547445616624743\n",
      "(250000,)\n",
      "Gradient Descent(709/2999): loss=0.3899174758109574, w0=0.00963896309436165, w1=-0.25474450979327984\n",
      "(250000,)\n",
      "Gradient Descent(710/2999): loss=0.38991747527507814, w0=0.009638956433473402, w1=-0.25474445821106584\n",
      "(250000,)\n",
      "Gradient Descent(711/2999): loss=0.38991747474225036, w0=0.009638949697677101, w1=-0.25474440691327105\n",
      "(250000,)\n",
      "Gradient Descent(712/2999): loss=0.3899174742124565, w0=0.009638942888477837, w1=-0.2547443558973627\n",
      "(250000,)\n",
      "Gradient Descent(713/2999): loss=0.38991747368567964, w0=0.009638936007358283, w1=-0.25474430516083646\n",
      "(250000,)\n",
      "Gradient Descent(714/2999): loss=0.3899174731619023, w0=0.009638929055779066, w1=-0.25474425470121576\n",
      "(250000,)\n",
      "Gradient Descent(715/2999): loss=0.3899174726411075, w0=0.009638922035179036, w1=-0.2547442045160517\n",
      "(250000,)\n",
      "Gradient Descent(716/2999): loss=0.3899174721232781, w0=0.009638914946975616, w1=-0.25474415460292255\n",
      "(250000,)\n",
      "Gradient Descent(717/2999): loss=0.3899174716083974, w0=0.009638907792565096, w1=-0.2547441049594337\n",
      "(250000,)\n",
      "Gradient Descent(718/2999): loss=0.3899174710964486, w0=0.00963890057332294, w1=-0.2547440555832171\n",
      "(250000,)\n",
      "Gradient Descent(719/2999): loss=0.3899174705874149, w0=0.00963889329060411, w1=-0.2547440064719311\n",
      "(250000,)\n",
      "Gradient Descent(720/2999): loss=0.3899174700812798, w0=0.009638885945743308, w1=-0.25474395762325985\n",
      "(250000,)\n",
      "Gradient Descent(721/2999): loss=0.3899174695780266, w0=0.00963887854005533, w1=-0.25474390903491356\n",
      "(250000,)\n",
      "Gradient Descent(722/2999): loss=0.3899174690776389, w0=0.009638871074835322, w1=-0.25474386070462773\n",
      "(250000,)\n",
      "Gradient Descent(723/2999): loss=0.3899174685801007, w0=0.009638863551359058, w1=-0.254743812630163\n",
      "(250000,)\n",
      "Gradient Descent(724/2999): loss=0.38991746808539535, w0=0.009638855970883256, w1=-0.2547437648093048\n",
      "(250000,)\n",
      "Gradient Descent(725/2999): loss=0.3899174675935068, w0=0.009638848334645794, w1=-0.25474371723986317\n",
      "(250000,)\n",
      "Gradient Descent(726/2999): loss=0.38991746710441944, w0=0.009638840643866033, w1=-0.2547436699196725\n",
      "(250000,)\n",
      "Gradient Descent(727/2999): loss=0.3899174666181168, w0=0.00963883289974506, w1=-0.2547436228465911\n",
      "(250000,)\n",
      "Gradient Descent(728/2999): loss=0.3899174661345832, w0=0.009638825103465965, w1=-0.25474357601850095\n",
      "(250000,)\n",
      "Gradient Descent(729/2999): loss=0.389917465653803, w0=0.009638817256194093, w1=-0.2547435294333075\n",
      "(250000,)\n",
      "Gradient Descent(730/2999): loss=0.3899174651757601, w0=0.009638809359077297, w1=-0.25474348308893946\n",
      "(250000,)\n",
      "Gradient Descent(731/2999): loss=0.38991746470043953, w0=0.009638801413246193, w1=-0.2547434369833483\n",
      "(250000,)\n",
      "Gradient Descent(732/2999): loss=0.38991746422782514, w0=0.009638793419814405, w1=-0.2547433911145081\n",
      "(250000,)\n",
      "Gradient Descent(733/2999): loss=0.389917463757902, w0=0.009638785379878828, w1=-0.2547433454804155\n",
      "(250000,)\n",
      "Gradient Descent(734/2999): loss=0.3899174632906546, w0=0.009638777294519844, w1=-0.254743300079089\n",
      "(250000,)\n",
      "Gradient Descent(735/2999): loss=0.3899174628260677, w0=0.009638769164801563, w1=-0.2547432549085692\n",
      "(250000,)\n",
      "Gradient Descent(736/2999): loss=0.3899174623641261, w0=0.009638760991772079, w1=-0.254743209966918\n",
      "(250000,)\n",
      "Gradient Descent(737/2999): loss=0.3899174619048147, w0=0.00963875277646367, w1=-0.2547431652522189\n",
      "(250000,)\n",
      "Gradient Descent(738/2999): loss=0.38991746144811884, w0=0.009638744519893055, w1=-0.2547431207625764\n",
      "(250000,)\n",
      "Gradient Descent(739/2999): loss=0.38991746099402314, w0=0.0096387362230616, w1=-0.2547430764961157\n",
      "(250000,)\n",
      "Gradient Descent(740/2999): loss=0.3899174605425131, w0=0.009638727886955542, w1=-0.25474303245098284\n",
      "(250000,)\n",
      "Gradient Descent(741/2999): loss=0.389917460093574, w0=0.00963871951254621, w1=-0.254742988625344\n",
      "(250000,)\n",
      "Gradient Descent(742/2999): loss=0.389917459647191, w0=0.009638711100790232, w1=-0.25474294501738565\n",
      "(250000,)\n",
      "Gradient Descent(743/2999): loss=0.38991745920334975, w0=0.009638702652629743, w1=-0.254742901625314\n",
      "(250000,)\n",
      "Gradient Descent(744/2999): loss=0.38991745876203565, w0=0.009638694168992615, w1=-0.254742858447355\n",
      "(250000,)\n",
      "Gradient Descent(745/2999): loss=0.3899174583232344, w0=0.00963868565079263, w1=-0.2547428154817541\n",
      "(250000,)\n",
      "Gradient Descent(746/2999): loss=0.3899174578869316, w0=0.009638677098929732, w1=-0.25474277272677576\n",
      "(250000,)\n",
      "Gradient Descent(747/2999): loss=0.389917457453113, w0=0.009638668514290167, w1=-0.2547427301807035\n",
      "(250000,)\n",
      "Gradient Descent(748/2999): loss=0.38991745702176467, w0=0.009638659897746713, w1=-0.25474268784183957\n",
      "(250000,)\n",
      "Gradient Descent(749/2999): loss=0.3899174565928723, w0=0.009638651250158866, w1=-0.2547426457085048\n",
      "(250000,)\n",
      "Gradient Descent(750/2999): loss=0.389917456166422, w0=0.00963864257237304, w1=-0.2547426037790383\n",
      "(250000,)\n",
      "Gradient Descent(751/2999): loss=0.3899174557423998, w0=0.009638633865222743, w1=-0.2547425620517972\n",
      "(250000,)\n",
      "Gradient Descent(752/2999): loss=0.38991745532079186, w0=0.009638625129528754, w1=-0.2547425205251566\n",
      "(250000,)\n",
      "Gradient Descent(753/2999): loss=0.3899174549015846, w0=0.009638616366099327, w1=-0.25474247919750925\n",
      "(250000,)\n",
      "Gradient Descent(754/2999): loss=0.3899174544847642, w0=0.009638607575730346, w1=-0.2547424380672654\n",
      "(250000,)\n",
      "Gradient Descent(755/2999): loss=0.389917454070317, w0=0.009638598759205542, w1=-0.25474239713285246\n",
      "(250000,)\n",
      "Gradient Descent(756/2999): loss=0.38991745365822966, w0=0.009638589917296622, w1=-0.254742356392715\n",
      "(250000,)\n",
      "Gradient Descent(757/2999): loss=0.38991745324848875, w0=0.009638581050763468, w1=-0.25474231584531437\n",
      "(250000,)\n",
      "Gradient Descent(758/2999): loss=0.3899174528410808, w0=0.009638572160354304, w1=-0.25474227548912864\n",
      "(250000,)\n",
      "Gradient Descent(759/2999): loss=0.3899174524359925, w0=0.009638563246805842, w1=-0.2547422353226523\n",
      "(250000,)\n",
      "Gradient Descent(760/2999): loss=0.38991745203321065, w0=0.009638554310843483, w1=-0.25474219534439607\n",
      "(250000,)\n",
      "Gradient Descent(761/2999): loss=0.3899174516327223, w0=0.009638545353181458, w1=-0.25474215555288693\n",
      "(250000,)\n",
      "Gradient Descent(762/2999): loss=0.38991745123451405, w0=0.009638536374522976, w1=-0.2547421159466675\n",
      "(250000,)\n",
      "Gradient Descent(763/2999): loss=0.3899174508385732, w0=0.009638527375560414, w1=-0.25474207652429626\n",
      "(250000,)\n",
      "Gradient Descent(764/2999): loss=0.3899174504448867, w0=0.009638518356975441, w1=-0.25474203728434713\n",
      "(250000,)\n",
      "Gradient Descent(765/2999): loss=0.38991745005344186, w0=0.009638509319439192, w1=-0.25474199822540944\n",
      "(250000,)\n",
      "Gradient Descent(766/2999): loss=0.38991744966422565, w0=0.009638500263612395, w1=-0.2547419593460876\n",
      "(250000,)\n",
      "Gradient Descent(767/2999): loss=0.3899174492772257, w0=0.009638491190145555, w1=-0.2547419206450011\n",
      "(250000,)\n",
      "Gradient Descent(768/2999): loss=0.3899174488924292, w0=0.009638482099679073, w1=-0.2547418821207841\n",
      "(250000,)\n",
      "Gradient Descent(769/2999): loss=0.38991744850982357, w0=0.0096384729928434, w1=-0.25474184377208536\n",
      "(250000,)\n",
      "Gradient Descent(770/2999): loss=0.38991744812939644, w0=0.009638463870259184, w1=-0.25474180559756826\n",
      "(250000,)\n",
      "Gradient Descent(771/2999): loss=0.38991744775113535, w0=0.009638454732537386, w1=-0.25474176759591033\n",
      "(250000,)\n",
      "Gradient Descent(772/2999): loss=0.3899174473750279, w0=0.009638445580279454, w1=-0.2547417297658033\n",
      "(250000,)\n",
      "Gradient Descent(773/2999): loss=0.38991744700106196, w0=0.009638436414077443, w1=-0.25474169210595277\n",
      "(250000,)\n",
      "Gradient Descent(774/2999): loss=0.38991744662922534, w0=0.009638427234514151, w1=-0.25474165461507814\n",
      "(250000,)\n",
      "Gradient Descent(775/2999): loss=0.3899174462595058, w0=0.009638418042163234, w1=-0.2547416172919124\n",
      "(250000,)\n",
      "Gradient Descent(776/2999): loss=0.38991744589189126, w0=0.009638408837589365, w1=-0.25474158013520215\n",
      "(250000,)\n",
      "Gradient Descent(777/2999): loss=0.3899174455263699, w0=0.00963839962134834, w1=-0.2547415431437071\n",
      "(250000,)\n",
      "Gradient Descent(778/2999): loss=0.3899174451629297, w0=0.009638390393987223, w1=-0.25474150631620024\n",
      "(250000,)\n",
      "Gradient Descent(779/2999): loss=0.3899174448015588, w0=0.009638381156044457, w1=-0.25474146965146743\n",
      "(250000,)\n",
      "Gradient Descent(780/2999): loss=0.3899174444422455, w0=0.00963837190805, w1=-0.25474143314830744\n",
      "(250000,)\n",
      "Gradient Descent(781/2999): loss=0.38991744408497797, w0=0.009638362650525426, w1=-0.25474139680553176\n",
      "(250000,)\n",
      "Gradient Descent(782/2999): loss=0.3899174437297445, w0=0.00963835338398408, w1=-0.25474136062196434\n",
      "(250000,)\n",
      "Gradient Descent(783/2999): loss=0.38991744337653367, w0=0.009638344108931154, w1=-0.2547413245964415\n",
      "(250000,)\n",
      "Gradient Descent(784/2999): loss=0.38991744302533404, w0=0.00963833482586383, w1=-0.25474128872781193\n",
      "(250000,)\n",
      "Gradient Descent(785/2999): loss=0.38991744267613393, w0=0.0096383255352714, w1=-0.2547412530149363\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(786/2999): loss=0.389917442328922, w0=0.009638316237635341, w1=-0.2547412174566872\n",
      "(250000,)\n",
      "Gradient Descent(787/2999): loss=0.3899174419836872, w0=0.009638306933429474, w1=-0.2547411820519492\n",
      "(250000,)\n",
      "Gradient Descent(788/2999): loss=0.3899174416404178, w0=0.009638297623120044, w1=-0.2547411467996183\n",
      "(250000,)\n",
      "Gradient Descent(789/2999): loss=0.3899174412991029, w0=0.009638288307165833, w1=-0.2547411116986023\n",
      "(250000,)\n",
      "Gradient Descent(790/2999): loss=0.38991744095973147, w0=0.009638278986018281, w1=-0.25474107674782026\n",
      "(250000,)\n",
      "Gradient Descent(791/2999): loss=0.3899174406222922, w0=0.00963826966012157, w1=-0.2547410419462025\n",
      "(250000,)\n",
      "Gradient Descent(792/2999): loss=0.38991744028677416, w0=0.009638260329912747, w1=-0.2547410072926905\n",
      "(250000,)\n",
      "Gradient Descent(793/2999): loss=0.3899174399531665, w0=0.009638250995821806, w1=-0.2547409727862367\n",
      "(250000,)\n",
      "Gradient Descent(794/2999): loss=0.38991743962145836, w0=0.009638241658271816, w1=-0.2547409384258045\n",
      "(250000,)\n",
      "Gradient Descent(795/2999): loss=0.3899174392916387, w0=0.009638232317678984, w1=-0.2547409042103679\n",
      "(250000,)\n",
      "Gradient Descent(796/2999): loss=0.3899174389636969, w0=0.009638222974452788, w1=-0.25474087013891167\n",
      "(250000,)\n",
      "Gradient Descent(797/2999): loss=0.3899174386376224, w0=0.009638213628996055, w1=-0.25474083621043103\n",
      "(250000,)\n",
      "Gradient Descent(798/2999): loss=0.3899174383134044, w0=0.009638204281705061, w1=-0.2547408024239315\n",
      "(250000,)\n",
      "Gradient Descent(799/2999): loss=0.3899174379910324, w0=0.00963819493296962, w1=-0.2547407687784289\n",
      "(250000,)\n",
      "Gradient Descent(800/2999): loss=0.3899174376704959, w0=0.009638185583173182, w1=-0.2547407352729491\n",
      "(250000,)\n",
      "Gradient Descent(801/2999): loss=0.38991743735178425, w0=0.009638176232692933, w1=-0.25474070190652814\n",
      "(250000,)\n",
      "Gradient Descent(802/2999): loss=0.38991743703488735, w0=0.009638166881899855, w1=-0.25474066867821177\n",
      "(250000,)\n",
      "Gradient Descent(803/2999): loss=0.38991743671979473, w0=0.009638157531158847, w1=-0.2547406355870556\n",
      "(250000,)\n",
      "Gradient Descent(804/2999): loss=0.38991743640649607, w0=0.009638148180828785, w1=-0.25474060263212495\n",
      "(250000,)\n",
      "Gradient Descent(805/2999): loss=0.3899174360949813, w0=0.009638138831262641, w1=-0.2547405698124945\n",
      "(250000,)\n",
      "Gradient Descent(806/2999): loss=0.38991743578524013, w0=0.009638129482807541, w1=-0.25474053712724853\n",
      "(250000,)\n",
      "Gradient Descent(807/2999): loss=0.3899174354772626, w0=0.009638120135804841, w1=-0.2547405045754806\n",
      "(250000,)\n",
      "Gradient Descent(808/2999): loss=0.38991743517103844, w0=0.009638110790590253, w1=-0.25474047215629336\n",
      "(250000,)\n",
      "Gradient Descent(809/2999): loss=0.38991743486655783, w0=0.009638101447493873, w1=-0.25474043986879874\n",
      "(250000,)\n",
      "Gradient Descent(810/2999): loss=0.3899174345638108, w0=0.009638092106840298, w1=-0.2547404077121176\n",
      "(250000,)\n",
      "Gradient Descent(811/2999): loss=0.3899174342627876, w0=0.00963808276894869, w1=-0.2547403756853796\n",
      "(250000,)\n",
      "Gradient Descent(812/2999): loss=0.38991743396347817, w0=0.009638073434132867, w1=-0.2547403437877233\n",
      "(250000,)\n",
      "Gradient Descent(813/2999): loss=0.38991743366587295, w0=0.009638064102701353, w1=-0.2547403120182959\n",
      "(250000,)\n",
      "Gradient Descent(814/2999): loss=0.38991743336996215, w0=0.009638054774957478, w1=-0.2547402803762531\n",
      "(250000,)\n",
      "Gradient Descent(815/2999): loss=0.3899174330757361, w0=0.009638045451199442, w1=-0.25474024886075924\n",
      "(250000,)\n",
      "Gradient Descent(816/2999): loss=0.38991743278318536, w0=0.0096380361317204, w1=-0.25474021747098685\n",
      "(250000,)\n",
      "Gradient Descent(817/2999): loss=0.38991743249230015, w0=0.009638026816808531, w1=-0.2547401862061168\n",
      "(250000,)\n",
      "Gradient Descent(818/2999): loss=0.3899174322030713, w0=0.009638017506747077, w1=-0.2547401550653383\n",
      "(250000,)\n",
      "Gradient Descent(819/2999): loss=0.38991743191548905, w0=0.009638008201814463, w1=-0.2547401240478484\n",
      "(250000,)\n",
      "Gradient Descent(820/2999): loss=0.38991743162954423, w0=0.009637998902284343, w1=-0.2547400931528523\n",
      "(250000,)\n",
      "Gradient Descent(821/2999): loss=0.38991743134522755, w0=0.009637989608425662, w1=-0.25474006237956304\n",
      "(250000,)\n",
      "Gradient Descent(822/2999): loss=0.3899174310625295, w0=0.009637980320502736, w1=-0.25474003172720144\n",
      "(250000,)\n",
      "Gradient Descent(823/2999): loss=0.38991743078144114, w0=0.009637971038775308, w1=-0.2547400011949961\n",
      "(250000,)\n",
      "Gradient Descent(824/2999): loss=0.38991743050195316, w0=0.009637961763498636, w1=-0.25473997078218324\n",
      "(250000,)\n",
      "Gradient Descent(825/2999): loss=0.38991743022405645, w0=0.009637952494923527, w1=-0.2547399404880065\n",
      "(250000,)\n",
      "Gradient Descent(826/2999): loss=0.389917429947742, w0=0.00963794323329642, w1=-0.2547399103117171\n",
      "(250000,)\n",
      "Gradient Descent(827/2999): loss=0.3899174296730006, w0=0.009637933978859451, w1=-0.25473988025257355\n",
      "(250000,)\n",
      "Gradient Descent(828/2999): loss=0.3899174293998236, w0=0.009637924731850487, w1=-0.2547398503098416\n",
      "(250000,)\n",
      "Gradient Descent(829/2999): loss=0.38991742912820193, w0=0.009637915492503223, w1=-0.2547398204827942\n",
      "(250000,)\n",
      "Gradient Descent(830/2999): loss=0.3899174288581267, w0=0.009637906261047225, w1=-0.2547397907707115\n",
      "(250000,)\n",
      "Gradient Descent(831/2999): loss=0.3899174285895892, w0=0.009637897037707989, w1=-0.25473976117288055\n",
      "(250000,)\n",
      "Gradient Descent(832/2999): loss=0.38991742832258064, w0=0.009637887822707004, w1=-0.25473973168859537\n",
      "(250000,)\n",
      "Gradient Descent(833/2999): loss=0.38991742805709234, w0=0.009637878616261811, w1=-0.2547397023171568\n",
      "(250000,)\n",
      "Gradient Descent(834/2999): loss=0.38991742779311556, w0=0.009637869418586058, w1=-0.25473967305787254\n",
      "(250000,)\n",
      "Gradient Descent(835/2999): loss=0.3899174275306418, w0=0.009637860229889543, w1=-0.25473964391005693\n",
      "(250000,)\n",
      "Gradient Descent(836/2999): loss=0.38991742726966244, w0=0.00963785105037829, w1=-0.25473961487303093\n",
      "(250000,)\n",
      "Gradient Descent(837/2999): loss=0.3899174270101689, w0=0.0096378418802546, w1=-0.25473958594612206\n",
      "(250000,)\n",
      "Gradient Descent(838/2999): loss=0.3899174267521529, w0=0.009637832719717082, w1=-0.25473955712866425\n",
      "(250000,)\n",
      "Gradient Descent(839/2999): loss=0.389917426495606, w0=0.009637823568960754, w1=-0.25473952841999786\n",
      "(250000,)\n",
      "Gradient Descent(840/2999): loss=0.38991742624051967, w0=0.00963781442817703, w1=-0.2547394998194696\n",
      "(250000,)\n",
      "Gradient Descent(841/2999): loss=0.3899174259868856, w0=0.009637805297553837, w1=-0.2547394713264323\n",
      "(250000,)\n",
      "Gradient Descent(842/2999): loss=0.3899174257346957, w0=0.00963779617727562, w1=-0.25473944294024514\n",
      "(250000,)\n",
      "Gradient Descent(843/2999): loss=0.3899174254839417, w0=0.009637787067523397, w1=-0.25473941466027317\n",
      "(250000,)\n",
      "Gradient Descent(844/2999): loss=0.38991742523461537, w0=0.009637777968474835, w1=-0.2547393864858876\n",
      "(250000,)\n",
      "Gradient Descent(845/2999): loss=0.38991742498670845, w0=0.009637768880304279, w1=-0.25473935841646567\n",
      "(250000,)\n",
      "Gradient Descent(846/2999): loss=0.3899174247402132, w0=0.009637759803182808, w1=-0.2547393304513903\n",
      "(250000,)\n",
      "Gradient Descent(847/2999): loss=0.3899174244951213, w0=0.009637750737278265, w1=-0.2547393025900504\n",
      "(250000,)\n",
      "Gradient Descent(848/2999): loss=0.38991742425142484, w0=0.009637741682755328, w1=-0.25473927483184056\n",
      "(250000,)\n",
      "Gradient Descent(849/2999): loss=0.3899174240091159, w0=0.009637732639775543, w1=-0.25473924717616103\n",
      "(250000,)\n",
      "Gradient Descent(850/2999): loss=0.38991742376818644, w0=0.009637723608497366, w1=-0.2547392196224177\n",
      "(250000,)\n",
      "Gradient Descent(851/2999): loss=0.3899174235286288, w0=0.009637714589076214, w1=-0.254739192170022\n",
      "(250000,)\n",
      "Gradient Descent(852/2999): loss=0.38991742329043516, w0=0.009637705581664505, w1=-0.2547391648183909\n",
      "(250000,)\n",
      "Gradient Descent(853/2999): loss=0.3899174230535976, w0=0.009637696586411719, w1=-0.25473913756694666\n",
      "(250000,)\n",
      "Gradient Descent(854/2999): loss=0.38991742281810854, w0=0.009637687603464406, w1=-0.2547391104151171\n",
      "(250000,)\n",
      "Gradient Descent(855/2999): loss=0.3899174225839602, w0=0.009637678632966268, w1=-0.25473908336233514\n",
      "(250000,)\n",
      "Gradient Descent(856/2999): loss=0.38991742235114507, w0=0.009637669675058164, w1=-0.25473905640803907\n",
      "(250000,)\n",
      "Gradient Descent(857/2999): loss=0.38991742211965535, w0=0.009637660729878173, w1=-0.2547390295516723\n",
      "(250000,)\n",
      "Gradient Descent(858/2999): loss=0.3899174218894837, w0=0.009637651797561647, w1=-0.2547390027926834\n",
      "(250000,)\n",
      "Gradient Descent(859/2999): loss=0.38991742166062243, w0=0.009637642878241209, w1=-0.2547389761305259\n",
      "(250000,)\n",
      "Gradient Descent(860/2999): loss=0.38991742143306446, w0=0.009637633972046823, w1=-0.25473894956465837\n",
      "(250000,)\n",
      "Gradient Descent(861/2999): loss=0.3899174212068019, w0=0.009637625079105835, w1=-0.2547389230945444\n",
      "(250000,)\n",
      "Gradient Descent(862/2999): loss=0.38991742098182763, w0=0.009637616199543009, w1=-0.25473889671965233\n",
      "(250000,)\n",
      "Gradient Descent(863/2999): loss=0.38991742075813424, w0=0.009637607333480538, w1=-0.2547388704394555\n",
      "(250000,)\n",
      "Gradient Descent(864/2999): loss=0.38991742053571454, w0=0.009637598481038108, w1=-0.25473884425343185\n",
      "(250000,)\n",
      "Gradient Descent(865/2999): loss=0.3899174203145611, w0=0.009637589642332942, w1=-0.25473881816106414\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(866/2999): loss=0.38991742009466673, w0=0.009637580817479827, w1=-0.2547387921618397\n",
      "(250000,)\n",
      "Gradient Descent(867/2999): loss=0.38991741987602446, w0=0.009637572006591127, w1=-0.2547387662552506\n",
      "(250000,)\n",
      "Gradient Descent(868/2999): loss=0.38991741965862703, w0=0.009637563209776837, w1=-0.25473874044079337\n",
      "(250000,)\n",
      "Gradient Descent(869/2999): loss=0.3899174194424674, w0=0.009637554427144632, w1=-0.25473871471796905\n",
      "(250000,)\n",
      "Gradient Descent(870/2999): loss=0.38991741922753836, w0=0.009637545658799889, w1=-0.25473868908628317\n",
      "(250000,)\n",
      "Gradient Descent(871/2999): loss=0.3899174190138331, w0=0.009637536904845695, w1=-0.25473866354524566\n",
      "(250000,)\n",
      "Gradient Descent(872/2999): loss=0.38991741880134456, w0=0.009637528165382927, w1=-0.25473863809437075\n",
      "(250000,)\n",
      "Gradient Descent(873/2999): loss=0.3899174185900659, w0=0.009637519440510252, w1=-0.25473861273317705\n",
      "(250000,)\n",
      "Gradient Descent(874/2999): loss=0.38991741837999, w0=0.009637510730324168, w1=-0.2547385874611873\n",
      "(250000,)\n",
      "Gradient Descent(875/2999): loss=0.3899174181711103, w0=0.009637502034919043, w1=-0.25473856227792857\n",
      "(250000,)\n",
      "Gradient Descent(876/2999): loss=0.3899174179634198, w0=0.009637493354387135, w1=-0.254738537182932\n",
      "(250000,)\n",
      "Gradient Descent(877/2999): loss=0.38991741775691185, w0=0.009637484688818636, w1=-0.2547385121757329\n",
      "(250000,)\n",
      "Gradient Descent(878/2999): loss=0.38991741755157966, w0=0.009637476038301685, w1=-0.2547384872558706\n",
      "(250000,)\n",
      "Gradient Descent(879/2999): loss=0.3899174173474165, w0=0.00963746740292242, w1=-0.25473846242288845\n",
      "(250000,)\n",
      "Gradient Descent(880/2999): loss=0.3899174171444158, w0=0.009637458782764976, w1=-0.25473843767633375\n",
      "(250000,)\n",
      "Gradient Descent(881/2999): loss=0.3899174169425708, w0=0.009637450177911558, w1=-0.2547384130157578\n",
      "(250000,)\n",
      "Gradient Descent(882/2999): loss=0.3899174167418751, w0=0.009637441588442429, w1=-0.25473838844071567\n",
      "(250000,)\n",
      "Gradient Descent(883/2999): loss=0.389917416542322, w0=0.009637433014435964, w1=-0.2547383639507663\n",
      "(250000,)\n",
      "Gradient Descent(884/2999): loss=0.3899174163439051, w0=0.009637424455968659, w1=-0.25473833954547254\n",
      "(250000,)\n",
      "Gradient Descent(885/2999): loss=0.38991741614661785, w0=0.009637415913115186, w1=-0.2547383152244008\n",
      "(250000,)\n",
      "Gradient Descent(886/2999): loss=0.38991741595045387, w0=0.00963740738594839, w1=-0.25473829098712136\n",
      "(250000,)\n",
      "Gradient Descent(887/2999): loss=0.38991741575540684, w0=0.00963739887453933, w1=-0.25473826683320805\n",
      "(250000,)\n",
      "Gradient Descent(888/2999): loss=0.3899174155614702, w0=0.009637390378957321, w1=-0.25473824276223833\n",
      "(250000,)\n",
      "Gradient Descent(889/2999): loss=0.3899174153686378, w0=0.009637381899269928, w1=-0.2547382187737933\n",
      "(250000,)\n",
      "Gradient Descent(890/2999): loss=0.3899174151769033, w0=0.009637373435543002, w1=-0.25473819486745763\n",
      "(250000,)\n",
      "Gradient Descent(891/2999): loss=0.3899174149862603, w0=0.009637364987840735, w1=-0.2547381710428194\n",
      "(250000,)\n",
      "Gradient Descent(892/2999): loss=0.3899174147967029, w0=0.00963735655622564, w1=-0.2547381472994701\n",
      "(250000,)\n",
      "Gradient Descent(893/2999): loss=0.38991741460822477, w0=0.009637348140758598, w1=-0.2547381236370048\n",
      "(250000,)\n",
      "Gradient Descent(894/2999): loss=0.3899174144208197, w0=0.009637339741498895, w1=-0.2547381000550219\n",
      "(250000,)\n",
      "Gradient Descent(895/2999): loss=0.3899174142344815, w0=0.009637331358504225, w1=-0.2547380765531231\n",
      "(250000,)\n",
      "Gradient Descent(896/2999): loss=0.3899174140492044, w0=0.009637322991830725, w1=-0.2547380531309134\n",
      "(250000,)\n",
      "Gradient Descent(897/2999): loss=0.3899174138649821, w0=0.009637314641533003, w1=-0.2547380297880012\n",
      "(250000,)\n",
      "Gradient Descent(898/2999): loss=0.38991741368180854, w0=0.009637306307664129, w1=-0.25473800652399803\n",
      "(250000,)\n",
      "Gradient Descent(899/2999): loss=0.3899174134996781, w0=0.009637297990275698, w1=-0.2547379833385186\n",
      "(250000,)\n",
      "Gradient Descent(900/2999): loss=0.38991741331858454, w0=0.009637289689417843, w1=-0.2547379602311808\n",
      "(250000,)\n",
      "Gradient Descent(901/2999): loss=0.389917413138522, w0=0.009637281405139225, w1=-0.25473793720160565\n",
      "(250000,)\n",
      "Gradient Descent(902/2999): loss=0.3899174129594848, w0=0.009637273137487103, w1=-0.25473791424941733\n",
      "(250000,)\n",
      "Gradient Descent(903/2999): loss=0.38991741278146674, w0=0.009637264886507317, w1=-0.254737891374243\n",
      "(250000,)\n",
      "Gradient Descent(904/2999): loss=0.38991741260446233, w0=0.009637256652244335, w1=-0.2547378685757129\n",
      "(250000,)\n",
      "Gradient Descent(905/2999): loss=0.38991741242846584, w0=0.009637248434741267, w1=-0.25473784585346015\n",
      "(250000,)\n",
      "Gradient Descent(906/2999): loss=0.3899174122534712, w0=0.009637240234039876, w1=-0.25473782320712096\n",
      "(250000,)\n",
      "Gradient Descent(907/2999): loss=0.38991741207947295, w0=0.0096372320501806, w1=-0.2547378006363344\n",
      "(250000,)\n",
      "Gradient Descent(908/2999): loss=0.3899174119064652, w0=0.009637223883202579, w1=-0.25473777814074244\n",
      "(250000,)\n",
      "Gradient Descent(909/2999): loss=0.3899174117344428, w0=0.009637215733143679, w1=-0.25473775571998997\n",
      "(250000,)\n",
      "Gradient Descent(910/2999): loss=0.38991741156339965, w0=0.009637207600040495, w1=-0.2547377333737246\n",
      "(250000,)\n",
      "Gradient Descent(911/2999): loss=0.38991741139333025, w0=0.009637199483928388, w1=-0.2547377111015967\n",
      "(250000,)\n",
      "Gradient Descent(912/2999): loss=0.3899174112242291, w0=0.009637191384841495, w1=-0.2547376889032596\n",
      "(250000,)\n",
      "Gradient Descent(913/2999): loss=0.38991741105609107, w0=0.009637183302812737, w1=-0.25473766677836923\n",
      "(250000,)\n",
      "Gradient Descent(914/2999): loss=0.38991741088890997, w0=0.009637175237873854, w1=-0.2547376447265843\n",
      "(250000,)\n",
      "Gradient Descent(915/2999): loss=0.3899174107226808, w0=0.009637167190055427, w1=-0.25473762274756606\n",
      "(250000,)\n",
      "Gradient Descent(916/2999): loss=0.38991741055739815, w0=0.00963715915938689, w1=-0.2547376008409786\n",
      "(250000,)\n",
      "Gradient Descent(917/2999): loss=0.3899174103930566, w0=0.009637151145896531, w1=-0.25473757900648836\n",
      "(250000,)\n",
      "Gradient Descent(918/2999): loss=0.3899174102296506, w0=0.009637143149611527, w1=-0.2547375572437647\n",
      "(250000,)\n",
      "Gradient Descent(919/2999): loss=0.38991741006717495, w0=0.00963713517055796, w1=-0.2547375355524793\n",
      "(250000,)\n",
      "Gradient Descent(920/2999): loss=0.38991740990562446, w0=0.009637127208760827, w1=-0.2547375139323065\n",
      "(250000,)\n",
      "Gradient Descent(921/2999): loss=0.3899174097449938, w0=0.009637119264244065, w1=-0.254737492382923\n",
      "(250000,)\n",
      "Gradient Descent(922/2999): loss=0.3899174095852775, w0=0.009637111337030567, w1=-0.2547374709040081\n",
      "(250000,)\n",
      "Gradient Descent(923/2999): loss=0.3899174094264707, w0=0.009637103427142179, w1=-0.2547374494952436\n",
      "(250000,)\n",
      "Gradient Descent(924/2999): loss=0.38991740926856794, w0=0.009637095534599741, w1=-0.25473742815631345\n",
      "(250000,)\n",
      "Gradient Descent(925/2999): loss=0.38991740911156436, w0=0.009637087659423095, w1=-0.2547374068869044\n",
      "(250000,)\n",
      "Gradient Descent(926/2999): loss=0.38991740895545457, w0=0.0096370798016311, w1=-0.2547373856867052\n",
      "(250000,)\n",
      "Gradient Descent(927/2999): loss=0.3899174088002336, w0=0.009637071961241637, w1=-0.2547373645554072\n",
      "(250000,)\n",
      "Gradient Descent(928/2999): loss=0.38991740864589636, w0=0.009637064138271641, w1=-0.25473734349270394\n",
      "(250000,)\n",
      "Gradient Descent(929/2999): loss=0.38991740849243794, w0=0.009637056332737114, w1=-0.2547373224982913\n",
      "(250000,)\n",
      "Gradient Descent(930/2999): loss=0.38991740833985317, w0=0.009637048544653113, w1=-0.2547373015718674\n",
      "(250000,)\n",
      "Gradient Descent(931/2999): loss=0.38991740818813714, w0=0.009637040774033802, w1=-0.2547372807131327\n",
      "(250000,)\n",
      "Gradient Descent(932/2999): loss=0.3899174080372849, w0=0.009637033020892447, w1=-0.2547372599217897\n",
      "(250000,)\n",
      "Gradient Descent(933/2999): loss=0.38991740788729157, w0=0.009637025285241432, w1=-0.25473723919754326\n",
      "(250000,)\n",
      "Gradient Descent(934/2999): loss=0.3899174077381522, w0=0.009637017567092265, w1=-0.2547372185401004\n",
      "(250000,)\n",
      "Gradient Descent(935/2999): loss=0.389917407589862, w0=0.009637009866455604, w1=-0.2547371979491702\n",
      "(250000,)\n",
      "Gradient Descent(936/2999): loss=0.38991740744241604, w0=0.009637002183341284, w1=-0.25473717742446395\n",
      "(250000,)\n",
      "Gradient Descent(937/2999): loss=0.38991740729580965, w0=0.009636994517758297, w1=-0.254737156965695\n",
      "(250000,)\n",
      "Gradient Descent(938/2999): loss=0.3899174071500378, w0=0.009636986869714823, w1=-0.2547371365725788\n",
      "(250000,)\n",
      "Gradient Descent(939/2999): loss=0.38991740700509603, w0=0.009636979239218234, w1=-0.25473711624483286\n",
      "(250000,)\n",
      "Gradient Descent(940/2999): loss=0.3899174068609795, w0=0.00963697162627513, w1=-0.25473709598217675\n",
      "(250000,)\n",
      "Gradient Descent(941/2999): loss=0.3899174067176834, w0=0.009636964030891324, w1=-0.25473707578433197\n",
      "(250000,)\n",
      "Gradient Descent(942/2999): loss=0.3899174065752033, w0=0.009636956453071862, w1=-0.25473705565102217\n",
      "(250000,)\n",
      "Gradient Descent(943/2999): loss=0.38991740643353434, w0=0.00963694889282104, w1=-0.2547370355819728\n",
      "(250000,)\n",
      "Gradient Descent(944/2999): loss=0.38991740629267196, w0=0.009636941350142432, w1=-0.2547370155769114\n",
      "(250000,)\n",
      "Gradient Descent(945/2999): loss=0.3899174061526115, w0=0.009636933825038873, w1=-0.25473699563556745\n",
      "(250000,)\n",
      "Gradient Descent(946/2999): loss=0.3899174060133487, w0=0.00963692631751248, w1=-0.2547369757576722\n",
      "(250000,)\n",
      "Gradient Descent(947/2999): loss=0.38991740587487866, w0=0.00963691882756466, w1=-0.25473695594295886\n",
      "(250000,)\n",
      "Gradient Descent(948/2999): loss=0.38991740573719696, w0=0.009636911355196147, w1=-0.2547369361911626\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(949/2999): loss=0.3899174056002993, w0=0.00963690390040698, w1=-0.25473691650202035\n",
      "(250000,)\n",
      "Gradient Descent(950/2999): loss=0.38991740546418086, w0=0.009636896463196522, w1=-0.2547368968752709\n",
      "(250000,)\n",
      "Gradient Descent(951/2999): loss=0.38991740532883756, w0=0.009636889043563496, w1=-0.25473687731065486\n",
      "(250000,)\n",
      "Gradient Descent(952/2999): loss=0.3899174051942647, w0=0.009636881641505963, w1=-0.25473685780791466\n",
      "(250000,)\n",
      "Gradient Descent(953/2999): loss=0.3899174050604581, w0=0.009636874257021361, w1=-0.2547368383667944\n",
      "(250000,)\n",
      "Gradient Descent(954/2999): loss=0.3899174049274134, w0=0.009636866890106476, w1=-0.25473681898704004\n",
      "(250000,)\n",
      "Gradient Descent(955/2999): loss=0.38991740479512615, w0=0.009636859540757497, w1=-0.2547367996683993\n",
      "(250000,)\n",
      "Gradient Descent(956/2999): loss=0.3899174046635921, w0=0.009636852208970004, w1=-0.25473678041062153\n",
      "(250000,)\n",
      "Gradient Descent(957/2999): loss=0.3899174045328069, w0=0.009636844894738972, w1=-0.25473676121345784\n",
      "(250000,)\n",
      "Gradient Descent(958/2999): loss=0.38991740440276623, w0=0.009636837598058789, w1=-0.254736742076661\n",
      "(250000,)\n",
      "Gradient Descent(959/2999): loss=0.38991740427346605, w0=0.009636830318923281, w1=-0.2547367229999855\n",
      "(250000,)\n",
      "Gradient Descent(960/2999): loss=0.38991740414490217, w0=0.009636823057325675, w1=-0.25473670398318743\n",
      "(250000,)\n",
      "Gradient Descent(961/2999): loss=0.3899174040170701, w0=0.009636815813258669, w1=-0.2547366850260246\n",
      "(250000,)\n",
      "Gradient Descent(962/2999): loss=0.3899174038899659, w0=0.00963680858671441, w1=-0.25473666612825624\n",
      "(250000,)\n",
      "Gradient Descent(963/2999): loss=0.38991740376358536, w0=0.009636801377684504, w1=-0.25473664728964346\n",
      "(250000,)\n",
      "Gradient Descent(964/2999): loss=0.3899174036379244, w0=0.009636794186160005, w1=-0.2547366285099487\n",
      "(250000,)\n",
      "Gradient Descent(965/2999): loss=0.38991740351297877, w0=0.009636787012131469, w1=-0.25473660978893614\n",
      "(250000,)\n",
      "Gradient Descent(966/2999): loss=0.38991740338874464, w0=0.009636779855588935, w1=-0.2547365911263714\n",
      "(250000,)\n",
      "Gradient Descent(967/2999): loss=0.38991740326521784, w0=0.00963677271652194, w1=-0.2547365725220218\n",
      "(250000,)\n",
      "Gradient Descent(968/2999): loss=0.3899174031423943, w0=0.009636765594919508, w1=-0.254736553975656\n",
      "(250000,)\n",
      "Gradient Descent(969/2999): loss=0.38991740302027006, w0=0.009636758490770203, w1=-0.25473653548704417\n",
      "(250000,)\n",
      "Gradient Descent(970/2999): loss=0.3899174028988411, w0=0.009636751404062092, w1=-0.2547365170559581\n",
      "(250000,)\n",
      "Gradient Descent(971/2999): loss=0.38991740277810355, w0=0.009636744334782772, w1=-0.2547364986821711\n",
      "(250000,)\n",
      "Gradient Descent(972/2999): loss=0.3899174026580534, w0=0.009636737282919378, w1=-0.2547364803654577\n",
      "(250000,)\n",
      "Gradient Descent(973/2999): loss=0.3899174025386868, w0=0.009636730248458614, w1=-0.2547364621055941\n",
      "(250000,)\n",
      "Gradient Descent(974/2999): loss=0.38991740241999967, w0=0.009636723231386703, w1=-0.2547364439023578\n",
      "(250000,)\n",
      "Gradient Descent(975/2999): loss=0.3899174023019885, w0=0.009636716231689447, w1=-0.25473642575552774\n",
      "(250000,)\n",
      "Gradient Descent(976/2999): loss=0.38991740218464904, w0=0.009636709249352218, w1=-0.25473640766488437\n",
      "(250000,)\n",
      "Gradient Descent(977/2999): loss=0.38991740206797776, w0=0.009636702284359967, w1=-0.2547363896302094\n",
      "(250000,)\n",
      "Gradient Descent(978/2999): loss=0.38991740195197083, w0=0.009636695336697214, w1=-0.254736371651286\n",
      "(250000,)\n",
      "Gradient Descent(979/2999): loss=0.3899174018366242, w0=0.009636688406348086, w1=-0.2547363537278986\n",
      "(250000,)\n",
      "Gradient Descent(980/2999): loss=0.3899174017219343, w0=0.009636681493296304, w1=-0.25473633585983313\n",
      "(250000,)\n",
      "Gradient Descent(981/2999): loss=0.3899174016078975, w0=0.009636674597525183, w1=-0.25473631804687674\n",
      "(250000,)\n",
      "Gradient Descent(982/2999): loss=0.38991740149451, w0=0.00963666771901767, w1=-0.25473630028881794\n",
      "(250000,)\n",
      "Gradient Descent(983/2999): loss=0.38991740138176806, w0=0.00963666085775632, w1=-0.25473628258544656\n",
      "(250000,)\n",
      "Gradient Descent(984/2999): loss=0.3899174012696679, w0=0.00963665401372333, w1=-0.25473626493655366\n",
      "(250000,)\n",
      "Gradient Descent(985/2999): loss=0.38991740115820595, w0=0.009636647186900495, w1=-0.2547362473419317\n",
      "(250000,)\n",
      "Gradient Descent(986/2999): loss=0.38991740104737876, w0=0.00963664037726929, w1=-0.2547362298013743\n",
      "(250000,)\n",
      "Gradient Descent(987/2999): loss=0.3899174009371824, w0=0.009636633584810811, w1=-0.25473621231467636\n",
      "(250000,)\n",
      "Gradient Descent(988/2999): loss=0.3899174008276136, w0=0.00963662680950582, w1=-0.2547361948816341\n",
      "(250000,)\n",
      "Gradient Descent(989/2999): loss=0.38991740071866865, w0=0.009636620051334734, w1=-0.25473617750204497\n",
      "(250000,)\n",
      "Gradient Descent(990/2999): loss=0.3899174006103439, w0=0.009636613310277627, w1=-0.2547361601757075\n",
      "(250000,)\n",
      "Gradient Descent(991/2999): loss=0.3899174005026359, w0=0.009636606586314263, w1=-0.2547361429024216\n",
      "(250000,)\n",
      "Gradient Descent(992/2999): loss=0.389917400395541, w0=0.00963659987942407, w1=-0.2547361256819883\n",
      "(250000,)\n",
      "Gradient Descent(993/2999): loss=0.3899174002890561, w0=0.009636593189586162, w1=-0.2547361085142098\n",
      "(250000,)\n",
      "Gradient Descent(994/2999): loss=0.38991740018317744, w0=0.009636586516779339, w1=-0.25473609139888953\n",
      "(250000,)\n",
      "Gradient Descent(995/2999): loss=0.38991740007790154, w0=0.009636579860982118, w1=-0.25473607433583206\n",
      "(250000,)\n",
      "Gradient Descent(996/2999): loss=0.38991739997322494, w0=0.00963657322217268, w1=-0.25473605732484306\n",
      "(250000,)\n",
      "Gradient Descent(997/2999): loss=0.38991739986914437, w0=0.009636566600328949, w1=-0.25473604036572944\n",
      "(250000,)\n",
      "Gradient Descent(998/2999): loss=0.3899173997656565, w0=0.009636559995428541, w1=-0.2547360234582992\n",
      "(250000,)\n",
      "Gradient Descent(999/2999): loss=0.3899173996627576, w0=0.009636553407448796, w1=-0.25473600660236145\n",
      "(250000,)\n",
      "Gradient Descent(1000/2999): loss=0.38991739956044486, w0=0.009636546836366777, w1=-0.25473598979772644\n",
      "(250000,)\n",
      "Gradient Descent(1001/2999): loss=0.38991739945871434, w0=0.00963654028215927, w1=-0.2547359730442055\n",
      "(250000,)\n",
      "Gradient Descent(1002/2999): loss=0.3899173993575632, w0=0.0096365337448028, w1=-0.25473595634161095\n",
      "(250000,)\n",
      "Gradient Descent(1003/2999): loss=0.3899173992569879, w0=0.009636527224273644, w1=-0.2547359396897564\n",
      "(250000,)\n",
      "Gradient Descent(1004/2999): loss=0.38991739915698526, w0=0.009636520720547805, w1=-0.25473592308845633\n",
      "(250000,)\n",
      "Gradient Descent(1005/2999): loss=0.38991739905755196, w0=0.009636514233601037, w1=-0.2547359065375264\n",
      "(250000,)\n",
      "Gradient Descent(1006/2999): loss=0.3899173989586848, w0=0.00963650776340886, w1=-0.2547358900367834\n",
      "(250000,)\n",
      "Gradient Descent(1007/2999): loss=0.38991739886038057, w0=0.009636501309946543, w1=-0.2547358735860448\n",
      "(250000,)\n",
      "Gradient Descent(1008/2999): loss=0.3899173987626359, w0=0.009636494873189115, w1=-0.2547358571851296\n",
      "(250000,)\n",
      "Gradient Descent(1009/2999): loss=0.38991739866544795, w0=0.0096364884531114, w1=-0.25473584083385736\n",
      "(250000,)\n",
      "Gradient Descent(1010/2999): loss=0.3899173985688132, w0=0.009636482049687964, w1=-0.254735824532049\n",
      "(250000,)\n",
      "Gradient Descent(1011/2999): loss=0.3899173984727287, w0=0.00963647566289317, w1=-0.25473580827952624\n",
      "(250000,)\n",
      "Gradient Descent(1012/2999): loss=0.38991739837719125, w0=0.009636469292701157, w1=-0.25473579207611186\n",
      "(250000,)\n",
      "Gradient Descent(1013/2999): loss=0.3899173982821976, w0=0.009636462939085858, w1=-0.25473577592162955\n",
      "(250000,)\n",
      "Gradient Descent(1014/2999): loss=0.38991739818774507, w0=0.009636456602020997, w1=-0.25473575981590413\n",
      "(250000,)\n",
      "Gradient Descent(1015/2999): loss=0.3899173980938301, w0=0.00963645028148008, w1=-0.25473574375876123\n",
      "(250000,)\n",
      "Gradient Descent(1016/2999): loss=0.38991739800045, w0=0.009636443977436427, w1=-0.25473572775002756\n",
      "(250000,)\n",
      "Gradient Descent(1017/2999): loss=0.3899173979076013, w0=0.009636437689863156, w1=-0.25473571178953064\n",
      "(250000,)\n",
      "Gradient Descent(1018/2999): loss=0.38991739781528145, w0=0.009636431418733191, w1=-0.25473569587709904\n",
      "(250000,)\n",
      "Gradient Descent(1019/2999): loss=0.38991739772348716, w0=0.009636425164019278, w1=-0.25473568001256225\n",
      "(250000,)\n",
      "Gradient Descent(1020/2999): loss=0.3899173976322155, w0=0.009636418925693984, w1=-0.25473566419575067\n",
      "(250000,)\n",
      "Gradient Descent(1021/2999): loss=0.3899173975414635, w0=0.009636412703729688, w1=-0.25473564842649554\n",
      "(250000,)\n",
      "Gradient Descent(1022/2999): loss=0.3899173974512281, w0=0.009636406498098599, w1=-0.2547356327046291\n",
      "(250000,)\n",
      "Gradient Descent(1023/2999): loss=0.3899173973615067, w0=0.00963640030877275, w1=-0.25473561702998454\n",
      "(250000,)\n",
      "Gradient Descent(1024/2999): loss=0.38991739727229596, w0=0.009636394135724006, w1=-0.25473560140239576\n",
      "(250000,)\n",
      "Gradient Descent(1025/2999): loss=0.38991739718359314, w0=0.009636387978924073, w1=-0.25473558582169764\n",
      "(250000,)\n",
      "Gradient Descent(1026/2999): loss=0.38991739709539536, w0=0.009636381838344496, w1=-0.254735570287726\n",
      "(250000,)\n",
      "Gradient Descent(1027/2999): loss=0.3899173970076997, w0=0.009636375713956657, w1=-0.2547355548003174\n",
      "(250000,)\n",
      "Gradient Descent(1028/2999): loss=0.38991739692050337, w0=0.009636369605731792, w1=-0.25473553935930937\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1029/2999): loss=0.3899173968338035, w0=0.00963636351364099, w1=-0.2547355239645402\n",
      "(250000,)\n",
      "Gradient Descent(1030/2999): loss=0.38991739674759723, w0=0.00963635743765519, w1=-0.25473550861584904\n",
      "(250000,)\n",
      "Gradient Descent(1031/2999): loss=0.3899173966618818, w0=0.009636351377745194, w1=-0.254735493313076\n",
      "(250000,)\n",
      "Gradient Descent(1032/2999): loss=0.3899173965766545, w0=0.009636345333881665, w1=-0.25473547805606184\n",
      "(250000,)\n",
      "Gradient Descent(1033/2999): loss=0.3899173964919123, w0=0.009636339306035123, w1=-0.2547354628446482\n",
      "(250000,)\n",
      "Gradient Descent(1034/2999): loss=0.3899173964076526, w0=0.009636333294175964, w1=-0.2547354476786777\n",
      "(250000,)\n",
      "Gradient Descent(1035/2999): loss=0.38991739632387273, w0=0.009636327298274454, w1=-0.25473543255799347\n",
      "(250000,)\n",
      "Gradient Descent(1036/2999): loss=0.38991739624056976, w0=0.009636321318300731, w1=-0.25473541748243966\n",
      "(250000,)\n",
      "Gradient Descent(1037/2999): loss=0.38991739615774107, w0=0.009636315354224805, w1=-0.25473540245186116\n",
      "(250000,)\n",
      "Gradient Descent(1038/2999): loss=0.38991739607538395, w0=0.00963630940601658, w1=-0.2547353874661036\n",
      "(250000,)\n",
      "Gradient Descent(1039/2999): loss=0.38991739599349584, w0=0.009636303473645828, w1=-0.25473537252501344\n",
      "(250000,)\n",
      "Gradient Descent(1040/2999): loss=0.38991739591207386, w0=0.009636297557082224, w1=-0.2547353576284379\n",
      "(250000,)\n",
      "Gradient Descent(1041/2999): loss=0.3899173958311155, w0=0.009636291656295327, w1=-0.254735342776225\n",
      "(250000,)\n",
      "Gradient Descent(1042/2999): loss=0.38991739575061807, w0=0.00963628577125458, w1=-0.2547353279682235\n",
      "(250000,)\n",
      "Gradient Descent(1043/2999): loss=0.3899173956705789, w0=0.00963627990192934, w1=-0.2547353132042829\n",
      "(250000,)\n",
      "Gradient Descent(1044/2999): loss=0.3899173955909955, w0=0.009636274048288844, w1=-0.25473529848425336\n",
      "(250000,)\n",
      "Gradient Descent(1045/2999): loss=0.38991739551186533, w0=0.009636268210302256, w1=-0.25473528380798593\n",
      "(250000,)\n",
      "Gradient Descent(1046/2999): loss=0.3899173954331855, w0=0.00963626238793861, w1=-0.2547352691753324\n",
      "(250000,)\n",
      "Gradient Descent(1047/2999): loss=0.3899173953549537, w0=0.009636256581166862, w1=-0.25473525458614515\n",
      "(250000,)\n",
      "Gradient Descent(1048/2999): loss=0.3899173952771672, w0=0.009636250789955888, w1=-0.2547352400402774\n",
      "(250000,)\n",
      "Gradient Descent(1049/2999): loss=0.38991739519982377, w0=0.009636245014274456, w1=-0.25473522553758304\n",
      "(250000,)\n",
      "Gradient Descent(1050/2999): loss=0.38991739512292056, w0=0.009636239254091267, w1=-0.2547352110779167\n",
      "(250000,)\n",
      "Gradient Descent(1051/2999): loss=0.3899173950464552, w0=0.009636233509374925, w1=-0.25473519666113364\n",
      "(250000,)\n",
      "Gradient Descent(1052/2999): loss=0.38991739497042527, w0=0.00963622778009396, w1=-0.25473518228708997\n",
      "(250000,)\n",
      "Gradient Descent(1053/2999): loss=0.3899173948948282, w0=0.009636222066216841, w1=-0.2547351679556423\n",
      "(250000,)\n",
      "Gradient Descent(1054/2999): loss=0.3899173948196615, w0=0.009636216367711932, w1=-0.2547351536666481\n",
      "(250000,)\n",
      "Gradient Descent(1055/2999): loss=0.3899173947449228, w0=0.009636210684547536, w1=-0.2547351394199654\n",
      "(250000,)\n",
      "Gradient Descent(1056/2999): loss=0.3899173946706096, w0=0.009636205016691891, w1=-0.254735125215453\n",
      "(250000,)\n",
      "Gradient Descent(1057/2999): loss=0.38991739459671954, w0=0.00963619936411316, w1=-0.25473511105297025\n",
      "(250000,)\n",
      "Gradient Descent(1058/2999): loss=0.3899173945232501, w0=0.009636193726779443, w1=-0.25473509693237734\n",
      "(250000,)\n",
      "Gradient Descent(1059/2999): loss=0.3899173944501991, w0=0.009636188104658777, w1=-0.25473508285353497\n",
      "(250000,)\n",
      "Gradient Descent(1060/2999): loss=0.38991739437756384, w0=0.009636182497719145, w1=-0.2547350688163046\n",
      "(250000,)\n",
      "Gradient Descent(1061/2999): loss=0.3899173943053423, w0=0.009636176905928462, w1=-0.25473505482054826\n",
      "(250000,)\n",
      "Gradient Descent(1062/2999): loss=0.3899173942335318, w0=0.009636171329254591, w1=-0.2547350408661287\n",
      "(250000,)\n",
      "Gradient Descent(1063/2999): loss=0.3899173941621302, w0=0.009636165767665324, w1=-0.2547350269529092\n",
      "(250000,)\n",
      "Gradient Descent(1064/2999): loss=0.3899173940911351, w0=0.009636160221128412, w1=-0.2547350130807538\n",
      "(250000,)\n",
      "Gradient Descent(1065/2999): loss=0.3899173940205444, w0=0.009636154689611574, w1=-0.2547349992495271\n",
      "(250000,)\n",
      "Gradient Descent(1066/2999): loss=0.38991739395035535, w0=0.009636149173082446, w1=-0.2547349854590943\n",
      "(250000,)\n",
      "Gradient Descent(1067/2999): loss=0.389917393880566, w0=0.009636143671508642, w1=-0.2547349717093213\n",
      "(250000,)\n",
      "Gradient Descent(1068/2999): loss=0.3899173938111741, w0=0.009636138184857725, w1=-0.25473495800007456\n",
      "(250000,)\n",
      "Gradient Descent(1069/2999): loss=0.38991739374217715, w0=0.00963613271309721, w1=-0.2547349443312212\n",
      "(250000,)\n",
      "Gradient Descent(1070/2999): loss=0.38991739367357314, w0=0.00963612725619457, w1=-0.25473493070262887\n",
      "(250000,)\n",
      "Gradient Descent(1071/2999): loss=0.3899173936053597, w0=0.00963612181411725, w1=-0.25473491711416585\n",
      "(250000,)\n",
      "Gradient Descent(1072/2999): loss=0.38991739353753463, w0=0.009636116386832662, w1=-0.25473490356570105\n",
      "(250000,)\n",
      "Gradient Descent(1073/2999): loss=0.38991739347009563, w0=0.009636110974308162, w1=-0.254734890057104\n",
      "(250000,)\n",
      "Gradient Descent(1074/2999): loss=0.3899173934030407, w0=0.00963610557651109, w1=-0.2547348765882447\n",
      "(250000,)\n",
      "Gradient Descent(1075/2999): loss=0.3899173933363675, w0=0.009636100193408742, w1=-0.2547348631589938\n",
      "(250000,)\n",
      "Gradient Descent(1076/2999): loss=0.389917393270074, w0=0.009636094824968394, w1=-0.25473484976922256\n",
      "(250000,)\n",
      "Gradient Descent(1077/2999): loss=0.38991739320415775, w0=0.009636089471157292, w1=-0.2547348364188028\n",
      "(250000,)\n",
      "Gradient Descent(1078/2999): loss=0.389917393138617, w0=0.00963608413194265, w1=-0.25473482310760687\n",
      "(250000,)\n",
      "Gradient Descent(1079/2999): loss=0.38991739307344936, w0=0.00963607880729166, w1=-0.2547348098355077\n",
      "(250000,)\n",
      "Gradient Descent(1080/2999): loss=0.3899173930086526, w0=0.009636073497171497, w1=-0.25473479660237885\n",
      "(250000,)\n",
      "Gradient Descent(1081/2999): loss=0.3899173929442249, w0=0.009636068201549311, w1=-0.2547347834080943\n",
      "(250000,)\n",
      "Gradient Descent(1082/2999): loss=0.38991739288016397, w0=0.009636062920392222, w1=-0.25473477025252866\n",
      "(250000,)\n",
      "Gradient Descent(1083/2999): loss=0.3899173928164678, w0=0.00963605765366735, w1=-0.25473475713555715\n",
      "(250000,)\n",
      "Gradient Descent(1084/2999): loss=0.38991739275313436, w0=0.009636052401341787, w1=-0.25473474405705543\n",
      "(250000,)\n",
      "Gradient Descent(1085/2999): loss=0.3899173926901614, w0=0.009636047163382608, w1=-0.25473473101689975\n",
      "(250000,)\n",
      "Gradient Descent(1086/2999): loss=0.38991739262754693, w0=0.009636041939756881, w1=-0.2547347180149669\n",
      "(250000,)\n",
      "Gradient Descent(1087/2999): loss=0.38991739256528907, w0=0.009636036730431648, w1=-0.25473470505113416\n",
      "(250000,)\n",
      "Gradient Descent(1088/2999): loss=0.38991739250338575, w0=0.009636031535373969, w1=-0.25473469212527944\n",
      "(250000,)\n",
      "Gradient Descent(1089/2999): loss=0.3899173924418347, w0=0.009636026354550868, w1=-0.2547346792372811\n",
      "(250000,)\n",
      "Gradient Descent(1090/2999): loss=0.38991739238063416, w0=0.009636021187929366, w1=-0.25473466638701797\n",
      "(250000,)\n",
      "Gradient Descent(1091/2999): loss=0.3899173923197821, w0=0.009636016035476482, w1=-0.25473465357436953\n",
      "(250000,)\n",
      "Gradient Descent(1092/2999): loss=0.38991739225927646, w0=0.00963601089715923, w1=-0.25473464079921565\n",
      "(250000,)\n",
      "Gradient Descent(1093/2999): loss=0.38991739219911525, w0=0.00963600577294461, w1=-0.2547346280614368\n",
      "(250000,)\n",
      "Gradient Descent(1094/2999): loss=0.38991739213929655, w0=0.009636000662799644, w1=-0.2547346153609139\n",
      "(250000,)\n",
      "Gradient Descent(1095/2999): loss=0.3899173920798185, w0=0.009635995566691327, w1=-0.2547346026975284\n",
      "(250000,)\n",
      "Gradient Descent(1096/2999): loss=0.3899173920206791, w0=0.009635990484586656, w1=-0.2547345900711622\n",
      "(250000,)\n",
      "Gradient Descent(1097/2999): loss=0.3899173919618764, w0=0.009635985416452646, w1=-0.2547345774816978\n",
      "(250000,)\n",
      "Gradient Descent(1098/2999): loss=0.38991739190340857, w0=0.009635980362256297, w1=-0.2547345649290181\n",
      "(250000,)\n",
      "Gradient Descent(1099/2999): loss=0.3899173918452734, w0=0.00963597532196463, w1=-0.25473455241300647\n",
      "(250000,)\n",
      "Gradient Descent(1100/2999): loss=0.3899173917874695, w0=0.009635970295544654, w1=-0.2547345399335469\n",
      "(250000,)\n",
      "Gradient Descent(1101/2999): loss=0.3899173917299946, w0=0.00963596528296339, w1=-0.2547345274905237\n",
      "(250000,)\n",
      "Gradient Descent(1102/2999): loss=0.3899173916728469, w0=0.00963596028418787, w1=-0.25473451508382183\n",
      "(250000,)\n",
      "Gradient Descent(1103/2999): loss=0.3899173916160245, w0=0.00963595529918513, w1=-0.25473450271332654\n",
      "(250000,)\n",
      "Gradient Descent(1104/2999): loss=0.3899173915595258, w0=0.009635950327922213, w1=-0.25473449037892365\n",
      "(250000,)\n",
      "Gradient Descent(1105/2999): loss=0.3899173915033487, w0=0.009635945370366175, w1=-0.2547344780804995\n",
      "(250000,)\n",
      "Gradient Descent(1106/2999): loss=0.3899173914474913, w0=0.009635940426484092, w1=-0.2547344658179408\n",
      "(250000,)\n",
      "Gradient Descent(1107/2999): loss=0.38991739139195214, w0=0.009635935496243047, w1=-0.2547344535911347\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1108/2999): loss=0.38991739133672904, w0=0.00963593057961013, w1=-0.254734441399969\n",
      "(250000,)\n",
      "Gradient Descent(1109/2999): loss=0.3899173912818205, w0=0.009635925676552444, w1=-0.2547344292443317\n",
      "(250000,)\n",
      "Gradient Descent(1110/2999): loss=0.3899173912272246, w0=0.009635920787037117, w1=-0.2547344171241115\n",
      "(250000,)\n",
      "Gradient Descent(1111/2999): loss=0.3899173911729394, w0=0.009635915911031308, w1=-0.2547344050391973\n",
      "(250000,)\n",
      "Gradient Descent(1112/2999): loss=0.38991739111896323, w0=0.009635911048502149, w1=-0.2547343929894787\n",
      "(250000,)\n",
      "Gradient Descent(1113/2999): loss=0.3899173910652945, w0=0.009635906199416842, w1=-0.2547343809748455\n",
      "(250000,)\n",
      "Gradient Descent(1114/2999): loss=0.38991739101193124, w0=0.009635901363742573, w1=-0.2547343689951882\n",
      "(250000,)\n",
      "Gradient Descent(1115/2999): loss=0.38991739095887185, w0=0.009635896541446566, w1=-0.2547343570503975\n",
      "(250000,)\n",
      "Gradient Descent(1116/2999): loss=0.3899173909061146, w0=0.009635891732496052, w1=-0.25473434514036464\n",
      "(250000,)\n",
      "Gradient Descent(1117/2999): loss=0.3899173908536577, w0=0.009635886936858294, w1=-0.2547343332649813\n",
      "(250000,)\n",
      "Gradient Descent(1118/2999): loss=0.38991739080149934, w0=0.00963588215450057, w1=-0.2547343214241396\n",
      "(250000,)\n",
      "Gradient Descent(1119/2999): loss=0.38991739074963816, w0=0.009635877385390205, w1=-0.254734309617732\n",
      "(250000,)\n",
      "Gradient Descent(1120/2999): loss=0.3899173906980721, w0=0.009635872629494524, w1=-0.25473429784565155\n",
      "(250000,)\n",
      "Gradient Descent(1121/2999): loss=0.3899173906467997, w0=0.00963586788678088, w1=-0.25473428610779153\n",
      "(250000,)\n",
      "Gradient Descent(1122/2999): loss=0.3899173905958192, w0=0.009635863157216672, w1=-0.2547342744040458\n",
      "(250000,)\n",
      "Gradient Descent(1123/2999): loss=0.3899173905451289, w0=0.009635858440769304, w1=-0.2547342627343085\n",
      "(250000,)\n",
      "Gradient Descent(1124/2999): loss=0.3899173904947273, w0=0.009635853737406223, w1=-0.25473425109847425\n",
      "(250000,)\n",
      "Gradient Descent(1125/2999): loss=0.38991739044461254, w0=0.0096358490470949, w1=-0.25473423949643814\n",
      "(250000,)\n",
      "Gradient Descent(1126/2999): loss=0.3899173903947832, w0=0.009635844369802828, w1=-0.2547342279280956\n",
      "(250000,)\n",
      "Gradient Descent(1127/2999): loss=0.38991739034523754, w0=0.00963583970549754, w1=-0.25473421639334237\n",
      "(250000,)\n",
      "Gradient Descent(1128/2999): loss=0.3899173902959739, w0=0.0096358350541466, w1=-0.2547342048920748\n",
      "(250000,)\n",
      "Gradient Descent(1129/2999): loss=0.38991739024699096, w0=0.009635830415717597, w1=-0.25473419342418957\n",
      "(250000,)\n",
      "Gradient Descent(1130/2999): loss=0.3899173901982866, w0=0.009635825790178156, w1=-0.25473418198958364\n",
      "(250000,)\n",
      "Gradient Descent(1131/2999): loss=0.38991739014985977, w0=0.009635821177495936, w1=-0.25473417058815445\n",
      "(250000,)\n",
      "Gradient Descent(1132/2999): loss=0.3899173901017086, w0=0.009635816577638633, w1=-0.25473415921979986\n",
      "(250000,)\n",
      "Gradient Descent(1133/2999): loss=0.3899173900538315, w0=0.009635811990573983, w1=-0.2547341478844181\n",
      "(250000,)\n",
      "Gradient Descent(1134/2999): loss=0.38991739000622716, w0=0.009635807416269732, w1=-0.25473413658190774\n",
      "(250000,)\n",
      "Gradient Descent(1135/2999): loss=0.38991738995889375, w0=0.009635802854693691, w1=-0.2547341253121678\n",
      "(250000,)\n",
      "Gradient Descent(1136/2999): loss=0.3899173899118298, w0=0.009635798305813693, w1=-0.2547341140750977\n",
      "(250000,)\n",
      "Gradient Descent(1137/2999): loss=0.3899173898650339, w0=0.009635793769597603, w1=-0.25473410287059717\n",
      "(250000,)\n",
      "Gradient Descent(1138/2999): loss=0.3899173898185043, w0=0.009635789246013339, w1=-0.2547340916985663\n",
      "(250000,)\n",
      "Gradient Descent(1139/2999): loss=0.38991738977223983, w0=0.009635784735028852, w1=-0.25473408055890573\n",
      "(250000,)\n",
      "Gradient Descent(1140/2999): loss=0.3899173897262385, w0=0.009635780236612128, w1=-0.2547340694515162\n",
      "(250000,)\n",
      "Gradient Descent(1141/2999): loss=0.38991738968049927, w0=0.009635775750731195, w1=-0.2547340583762991\n",
      "(250000,)\n",
      "Gradient Descent(1142/2999): loss=0.3899173896350203, w0=0.009635771277354113, w1=-0.25473404733315597\n",
      "(250000,)\n",
      "Gradient Descent(1143/2999): loss=0.3899173895898003, w0=0.009635766816449008, w1=-0.2547340363219889\n",
      "(250000,)\n",
      "Gradient Descent(1144/2999): loss=0.38991738954483796, w0=0.009635762367984012, w1=-0.2547340253427002\n",
      "(250000,)\n",
      "Gradient Descent(1145/2999): loss=0.3899173895001314, w0=0.009635757931927313, w1=-0.25473401439519255\n",
      "(250000,)\n",
      "Gradient Descent(1146/2999): loss=0.38991738945567955, w0=0.009635753508247167, w1=-0.2547340034793691\n",
      "(250000,)\n",
      "Gradient Descent(1147/2999): loss=0.3899173894114806, w0=0.009635749096911835, w1=-0.2547339925951333\n",
      "(250000,)\n",
      "Gradient Descent(1148/2999): loss=0.3899173893675335, w0=0.009635744697889638, w1=-0.2547339817423889\n",
      "(250000,)\n",
      "Gradient Descent(1149/2999): loss=0.38991738932383646, w0=0.009635740311148949, w1=-0.25473397092104016\n",
      "(250000,)\n",
      "Gradient Descent(1150/2999): loss=0.3899173892803882, w0=0.009635735936658167, w1=-0.25473396013099153\n",
      "(250000,)\n",
      "Gradient Descent(1151/2999): loss=0.3899173892371873, w0=0.009635731574385749, w1=-0.2547339493721479\n",
      "(250000,)\n",
      "Gradient Descent(1152/2999): loss=0.3899173891942325, w0=0.009635727224300198, w1=-0.2547339386444144\n",
      "(250000,)\n",
      "Gradient Descent(1153/2999): loss=0.38991738915152224, w0=0.009635722886370058, w1=-0.25473392794769667\n",
      "(250000,)\n",
      "Gradient Descent(1154/2999): loss=0.38991738910905505, w0=0.009635718560563914, w1=-0.2547339172819006\n",
      "(250000,)\n",
      "Gradient Descent(1155/2999): loss=0.38991738906682966, w0=0.00963571424685041, w1=-0.2547339066469324\n",
      "(250000,)\n",
      "Gradient Descent(1156/2999): loss=0.38991738902484474, w0=0.009635709945198226, w1=-0.25473389604269875\n",
      "(250000,)\n",
      "Gradient Descent(1157/2999): loss=0.3899173889830989, w0=0.0096357056555761, w1=-0.25473388546910647\n",
      "(250000,)\n",
      "Gradient Descent(1158/2999): loss=0.3899173889415906, w0=0.009635701377952808, w1=-0.25473387492606286\n",
      "(250000,)\n",
      "Gradient Descent(1159/2999): loss=0.3899173889003187, w0=0.009635697112297179, w1=-0.2547338644134755\n",
      "(250000,)\n",
      "Gradient Descent(1160/2999): loss=0.38991738885928173, w0=0.009635692858578083, w1=-0.25473385393125236\n",
      "(250000,)\n",
      "Gradient Descent(1161/2999): loss=0.38991738881847854, w0=0.00963568861676446, w1=-0.25473384347930167\n",
      "(250000,)\n",
      "Gradient Descent(1162/2999): loss=0.38991738877790755, w0=0.009635684386825275, w1=-0.254733833057532\n",
      "(250000,)\n",
      "Gradient Descent(1163/2999): loss=0.3899173887375676, w0=0.009635680168729555, w1=-0.2547338226658523\n",
      "(250000,)\n",
      "Gradient Descent(1164/2999): loss=0.3899173886974573, w0=0.009635675962446388, w1=-0.2547338123041718\n",
      "(250000,)\n",
      "Gradient Descent(1165/2999): loss=0.38991738865757536, w0=0.009635671767944894, w1=-0.25473380197240003\n",
      "(250000,)\n",
      "Gradient Descent(1166/2999): loss=0.3899173886179206, w0=0.00963566758519425, w1=-0.25473379167044696\n",
      "(250000,)\n",
      "Gradient Descent(1167/2999): loss=0.3899173885784915, w0=0.009635663414163678, w1=-0.2547337813982227\n",
      "(250000,)\n",
      "Gradient Descent(1168/2999): loss=0.3899173885392868, w0=0.009635659254822473, w1=-0.2547337711556379\n",
      "(250000,)\n",
      "Gradient Descent(1169/2999): loss=0.38991738850030544, w0=0.009635655107139958, w1=-0.2547337609426033\n",
      "(250000,)\n",
      "Gradient Descent(1170/2999): loss=0.389917388461546, w0=0.009635650971085529, w1=-0.25473375075903015\n",
      "(250000,)\n",
      "Gradient Descent(1171/2999): loss=0.38991738842300727, w0=0.009635646846628612, w1=-0.25473374060482984\n",
      "(250000,)\n",
      "Gradient Descent(1172/2999): loss=0.389917388384688, w0=0.009635642733738705, w1=-0.2547337304799142\n",
      "(250000,)\n",
      "Gradient Descent(1173/2999): loss=0.38991738834658674, w0=0.009635638632385347, w1=-0.25473372038419534\n",
      "(250000,)\n",
      "Gradient Descent(1174/2999): loss=0.38991738830870254, w0=0.009635634542538138, w1=-0.2547337103175856\n",
      "(250000,)\n",
      "Gradient Descent(1175/2999): loss=0.389917388271034, w0=0.00963563046416673, w1=-0.25473370027999775\n",
      "(250000,)\n",
      "Gradient Descent(1176/2999): loss=0.38991738823357985, w0=0.00963562639724083, w1=-0.2547336902713448\n",
      "(250000,)\n",
      "Gradient Descent(1177/2999): loss=0.38991738819633903, w0=0.009635622341730203, w1=-0.25473368029154003\n",
      "(250000,)\n",
      "Gradient Descent(1178/2999): loss=0.3899173881593103, w0=0.009635618297604648, w1=-0.2547336703404971\n",
      "(250000,)\n",
      "Gradient Descent(1179/2999): loss=0.3899173881224923, w0=0.009635614264834051, w1=-0.2547336604181299\n",
      "(250000,)\n",
      "Gradient Descent(1180/2999): loss=0.38991738808588394, w0=0.009635610243388333, w1=-0.25473365052435265\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1181/2999): loss=0.38991738804948395, w0=0.009635606233237474, w1=-0.25473364065907994\n",
      "(250000,)\n",
      "Gradient Descent(1182/2999): loss=0.3899173880132913, w0=0.009635602234351509, w1=-0.2547336308222265\n",
      "(250000,)\n",
      "Gradient Descent(1183/2999): loss=0.38991738797730474, w0=0.009635598246700528, w1=-0.2547336210137075\n",
      "(250000,)\n",
      "Gradient Descent(1184/2999): loss=0.3899173879415229, w0=0.009635594270254672, w1=-0.2547336112334383\n",
      "(250000,)\n",
      "Gradient Descent(1185/2999): loss=0.38991738790594493, w0=0.009635590304984155, w1=-0.2547336014813346\n",
      "(250000,)\n",
      "Gradient Descent(1186/2999): loss=0.3899173878705695, w0=0.009635586350859239, w1=-0.2547335917573124\n",
      "(250000,)\n",
      "Gradient Descent(1187/2999): loss=0.3899173878353954, w0=0.009635582407850238, w1=-0.254733582061288\n",
      "(250000,)\n",
      "Gradient Descent(1188/2999): loss=0.3899173878004217, w0=0.00963557847592753, w1=-0.25473357239317784\n",
      "(250000,)\n",
      "Gradient Descent(1189/2999): loss=0.3899173877656471, w0=0.00963557455506155, w1=-0.2547335627528989\n",
      "(250000,)\n",
      "Gradient Descent(1190/2999): loss=0.3899173877310704, w0=0.009635570645222767, w1=-0.2547335531403682\n",
      "(250000,)\n",
      "Gradient Descent(1191/2999): loss=0.38991738769669054, w0=0.009635566746381753, w1=-0.2547335435555032\n",
      "(250000,)\n",
      "Gradient Descent(1192/2999): loss=0.38991738766250655, w0=0.009635562858509096, w1=-0.25473353399822163\n",
      "(250000,)\n",
      "Gradient Descent(1193/2999): loss=0.38991738762851713, w0=0.009635558981575471, w1=-0.2547335244684414\n",
      "(250000,)\n",
      "Gradient Descent(1194/2999): loss=0.38991738759472117, w0=0.00963555511555159, w1=-0.25473351496608077\n",
      "(250000,)\n",
      "Gradient Descent(1195/2999): loss=0.38991738756111777, w0=0.009635551260408241, w1=-0.2547335054910583\n",
      "(250000,)\n",
      "Gradient Descent(1196/2999): loss=0.38991738752770555, w0=0.009635547416116249, w1=-0.2547334960432927\n",
      "(250000,)\n",
      "Gradient Descent(1197/2999): loss=0.38991738749448357, w0=0.009635543582646516, w1=-0.2547334866227031\n",
      "(250000,)\n",
      "Gradient Descent(1198/2999): loss=0.3899173874614507, w0=0.009635539759970004, w1=-0.2547334772292089\n",
      "(250000,)\n",
      "Gradient Descent(1199/2999): loss=0.3899173874286061, w0=0.00963553594805773, w1=-0.2547334678627297\n",
      "(250000,)\n",
      "Gradient Descent(1200/2999): loss=0.3899173873959484, w0=0.00963553214688076, w1=-0.25473345852318535\n",
      "(250000,)\n",
      "Gradient Descent(1201/2999): loss=0.38991738736347664, w0=0.009635528356410223, w1=-0.254733449210496\n",
      "(250000,)\n",
      "Gradient Descent(1202/2999): loss=0.38991738733118975, w0=0.009635524576617314, w1=-0.25473343992458214\n",
      "(250000,)\n",
      "Gradient Descent(1203/2999): loss=0.3899173872990866, w0=0.00963552080747328, w1=-0.2547334306653644\n",
      "(250000,)\n",
      "Gradient Descent(1204/2999): loss=0.38991738726716624, w0=0.009635517048949431, w1=-0.2547334214327638\n",
      "(250000,)\n",
      "Gradient Descent(1205/2999): loss=0.38991738723542774, w0=0.009635513301017149, w1=-0.2547334122267015\n",
      "(250000,)\n",
      "Gradient Descent(1206/2999): loss=0.3899173872038699, w0=0.009635509563647856, w1=-0.254733403047099\n",
      "(250000,)\n",
      "Gradient Descent(1207/2999): loss=0.38991738717249175, w0=0.009635505836813045, w1=-0.25473339389387806\n",
      "(250000,)\n",
      "Gradient Descent(1208/2999): loss=0.3899173871412922, w0=0.00963550212048426, w1=-0.25473338476696067\n",
      "(250000,)\n",
      "Gradient Descent(1209/2999): loss=0.3899173871102703, w0=0.009635498414633115, w1=-0.25473337566626914\n",
      "(250000,)\n",
      "Gradient Descent(1210/2999): loss=0.389917387079425, w0=0.009635494719231292, w1=-0.25473336659172596\n",
      "(250000,)\n",
      "Gradient Descent(1211/2999): loss=0.38991738704875545, w0=0.009635491034250513, w1=-0.25473335754325394\n",
      "(250000,)\n",
      "Gradient Descent(1212/2999): loss=0.3899173870182604, w0=0.009635487359662576, w1=-0.25473334852077606\n",
      "(250000,)\n",
      "Gradient Descent(1213/2999): loss=0.389917386987939, w0=0.009635483695439335, w1=-0.2547333395242156\n",
      "(250000,)\n",
      "Gradient Descent(1214/2999): loss=0.3899173869577902, w0=0.0096354800415527, w1=-0.2547333305534962\n",
      "(250000,)\n",
      "Gradient Descent(1215/2999): loss=0.389917386927813, w0=0.009635476397974643, w1=-0.2547333216085415\n",
      "(250000,)\n",
      "Gradient Descent(1216/2999): loss=0.38991738689800665, w0=0.009635472764677197, w1=-0.2547333126892757\n",
      "(250000,)\n",
      "Gradient Descent(1217/2999): loss=0.38991738686836985, w0=0.009635469141632463, w1=-0.254733303795623\n",
      "(250000,)\n",
      "Gradient Descent(1218/2999): loss=0.38991738683890187, w0=0.009635465528812594, w1=-0.25473329492750796\n",
      "(250000,)\n",
      "Gradient Descent(1219/2999): loss=0.3899173868096016, w0=0.009635461926189819, w1=-0.2547332860848554\n",
      "(250000,)\n",
      "Gradient Descent(1220/2999): loss=0.3899173867804682, w0=0.009635458333736411, w1=-0.2547332772675903\n",
      "(250000,)\n",
      "Gradient Descent(1221/2999): loss=0.3899173867515006, w0=0.009635454751424702, w1=-0.254733268475638\n",
      "(250000,)\n",
      "Gradient Descent(1222/2999): loss=0.38991738672269804, w0=0.009635451179227096, w1=-0.25473325970892396\n",
      "(250000,)\n",
      "Gradient Descent(1223/2999): loss=0.3899173866940594, w0=0.00963544761711607, w1=-0.254733250967374\n",
      "(250000,)\n",
      "Gradient Descent(1224/2999): loss=0.3899173866655838, w0=0.009635444065064136, w1=-0.25473324225091404\n",
      "(250000,)\n",
      "Gradient Descent(1225/2999): loss=0.38991738663727044, w0=0.009635440523043883, w1=-0.2547332335594704\n",
      "(250000,)\n",
      "Gradient Descent(1226/2999): loss=0.3899173866091181, w0=0.009635436991027958, w1=-0.2547332248929696\n",
      "(250000,)\n",
      "Gradient Descent(1227/2999): loss=0.3899173865811262, w0=0.009635433468989072, w1=-0.2547332162513383\n",
      "(250000,)\n",
      "Gradient Descent(1228/2999): loss=0.3899173865532936, w0=0.009635429956899998, w1=-0.2547332076345035\n",
      "(250000,)\n",
      "Gradient Descent(1229/2999): loss=0.38991738652561947, w0=0.009635426454733564, w1=-0.25473319904239244\n",
      "(250000,)\n",
      "Gradient Descent(1230/2999): loss=0.38991738649810304, w0=0.00963542296246267, w1=-0.2547331904749324\n",
      "(250000,)\n",
      "Gradient Descent(1231/2999): loss=0.3899173864707431, w0=0.009635419480060265, w1=-0.2547331819320512\n",
      "(250000,)\n",
      "Gradient Descent(1232/2999): loss=0.3899173864435389, w0=0.009635416007499372, w1=-0.2547331734136767\n",
      "(250000,)\n",
      "Gradient Descent(1233/2999): loss=0.38991738641648976, w0=0.009635412544753067, w1=-0.254733164919737\n",
      "(250000,)\n",
      "Gradient Descent(1234/2999): loss=0.38991738638959444, w0=0.009635409091794493, w1=-0.2547331564501605\n",
      "(250000,)\n",
      "Gradient Descent(1235/2999): loss=0.3899173863628523, w0=0.009635405648596855, w1=-0.2547331480048758\n",
      "(250000,)\n",
      "Gradient Descent(1236/2999): loss=0.38991738633626255, w0=0.009635402215133409, w1=-0.25473313958381166\n",
      "(250000,)\n",
      "Gradient Descent(1237/2999): loss=0.3899173863098241, w0=0.00963539879137749, w1=-0.2547331311868972\n",
      "(250000,)\n",
      "Gradient Descent(1238/2999): loss=0.38991738628353617, w0=0.009635395377302479, w1=-0.2547331228140617\n",
      "(250000,)\n",
      "Gradient Descent(1239/2999): loss=0.3899173862573978, w0=0.009635391972881826, w1=-0.2547331144652346\n",
      "(250000,)\n",
      "Gradient Descent(1240/2999): loss=0.3899173862314085, w0=0.00963538857808905, w1=-0.2547331061403457\n",
      "(250000,)\n",
      "Gradient Descent(1241/2999): loss=0.38991738620556693, w0=0.009635385192897725, w1=-0.25473309783932496\n",
      "(250000,)\n",
      "Gradient Descent(1242/2999): loss=0.38991738617987265, w0=0.009635381817281484, w1=-0.2547330895621025\n",
      "(250000,)\n",
      "Gradient Descent(1243/2999): loss=0.38991738615432453, w0=0.009635378451214021, w1=-0.2547330813086088\n",
      "(250000,)\n",
      "Gradient Descent(1244/2999): loss=0.38991738612892196, w0=0.009635375094669099, w1=-0.25473307307877435\n",
      "(250000,)\n",
      "Gradient Descent(1245/2999): loss=0.38991738610366405, w0=0.009635371747620539, w1=-0.2547330648725301\n",
      "(250000,)\n",
      "Gradient Descent(1246/2999): loss=0.38991738607854987, w0=0.009635368410042222, w1=-0.2547330566898071\n",
      "(250000,)\n",
      "Gradient Descent(1247/2999): loss=0.38991738605357873, w0=0.009635365081908097, w1=-0.25473304853053663\n",
      "(250000,)\n",
      "Gradient Descent(1248/2999): loss=0.3899173860287498, w0=0.009635361763192164, w1=-0.25473304039465017\n",
      "(250000,)\n",
      "Gradient Descent(1249/2999): loss=0.3899173860040621, w0=0.0096353584538685, w1=-0.25473303228207944\n",
      "(250000,)\n",
      "Gradient Descent(1250/2999): loss=0.3899173859795151, w0=0.009635355153911236, w1=-0.2547330241927564\n",
      "(250000,)\n",
      "Gradient Descent(1251/2999): loss=0.38991738595510766, w0=0.00963535186329456, w1=-0.25473301612661314\n",
      "(250000,)\n",
      "Gradient Descent(1252/2999): loss=0.3899173859308393, w0=0.009635348581992723, w1=-0.25473300808358207\n",
      "(250000,)\n",
      "Gradient Descent(1253/2999): loss=0.3899173859067092, w0=0.00963534530998005, w1=-0.2547330000635957\n",
      "(250000,)\n",
      "Gradient Descent(1254/2999): loss=0.38991738588271635, w0=0.009635342047230907, w1=-0.25473299206658695\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1255/2999): loss=0.38991738585886027, w0=0.009635338793719741, w1=-0.2547329840924887\n",
      "(250000,)\n",
      "Gradient Descent(1256/2999): loss=0.3899173858351397, w0=0.00963533554942105, w1=-0.2547329761412342\n",
      "(250000,)\n",
      "Gradient Descent(1257/2999): loss=0.3899173858115546, w0=0.009635332314309396, w1=-0.2547329682127568\n",
      "(250000,)\n",
      "Gradient Descent(1258/2999): loss=0.38991738578810353, w0=0.009635329088359406, w1=-0.2547329603069903\n",
      "(250000,)\n",
      "Gradient Descent(1259/2999): loss=0.38991738576478596, w0=0.009635325871545761, w1=-0.25473295242386834\n",
      "(250000,)\n",
      "Gradient Descent(1260/2999): loss=0.38991738574160123, w0=0.009635322663843216, w1=-0.25473294456332507\n",
      "(250000,)\n",
      "Gradient Descent(1261/2999): loss=0.3899173857185486, w0=0.00963531946522658, w1=-0.2547329367252947\n",
      "(250000,)\n",
      "Gradient Descent(1262/2999): loss=0.389917385695627, w0=0.009635316275670712, w1=-0.25473292890971166\n",
      "(250000,)\n",
      "Gradient Descent(1263/2999): loss=0.38991738567283607, w0=0.00963531309515055, w1=-0.2547329211165107\n",
      "(250000,)\n",
      "Gradient Descent(1264/2999): loss=0.3899173856501749, w0=0.009635309923641089, w1=-0.2547329133456266\n",
      "(250000,)\n",
      "Gradient Descent(1265/2999): loss=0.3899173856276426, w0=0.00963530676111739, w1=-0.25473290559699446\n",
      "(250000,)\n",
      "Gradient Descent(1266/2999): loss=0.3899173856052388, w0=0.009635303607554564, w1=-0.2547328978705496\n",
      "(250000,)\n",
      "Gradient Descent(1267/2999): loss=0.38991738558296235, w0=0.009635300462927786, w1=-0.2547328901662274\n",
      "(250000,)\n",
      "Gradient Descent(1268/2999): loss=0.38991738556081285, w0=0.009635297327212298, w1=-0.2547328824839636\n",
      "(250000,)\n",
      "Gradient Descent(1269/2999): loss=0.3899173855387893, w0=0.009635294200383396, w1=-0.254732874823694\n",
      "(250000,)\n",
      "Gradient Descent(1270/2999): loss=0.3899173855168913, w0=0.009635291082416435, w1=-0.2547328671853547\n",
      "(250000,)\n",
      "Gradient Descent(1271/2999): loss=0.389917385495118, w0=0.009635287973286843, w1=-0.25473285956888203\n",
      "(250000,)\n",
      "Gradient Descent(1272/2999): loss=0.3899173854734685, w0=0.009635284872970107, w1=-0.25473285197421236\n",
      "(250000,)\n",
      "Gradient Descent(1273/2999): loss=0.3899173854519424, w0=0.009635281781441773, w1=-0.2547328444012824\n",
      "(250000,)\n",
      "Gradient Descent(1274/2999): loss=0.38991738543053883, w0=0.009635278698677445, w1=-0.254732836850029\n",
      "(250000,)\n",
      "Gradient Descent(1275/2999): loss=0.38991738540925713, w0=0.009635275624652785, w1=-0.25473282932038926\n",
      "(250000,)\n",
      "Gradient Descent(1276/2999): loss=0.3899173853880966, w0=0.009635272559343518, w1=-0.2547328218123004\n",
      "(250000,)\n",
      "Gradient Descent(1277/2999): loss=0.38991738536705656, w0=0.00963526950272543, w1=-0.25473281432569983\n",
      "(250000,)\n",
      "Gradient Descent(1278/2999): loss=0.38991738534613607, w0=0.009635266454774371, w1=-0.2547328068605252\n",
      "(250000,)\n",
      "Gradient Descent(1279/2999): loss=0.389917385325335, w0=0.009635263415466248, w1=-0.25473279941671434\n",
      "(250000,)\n",
      "Gradient Descent(1280/2999): loss=0.38991738530465236, w0=0.009635260384777031, w1=-0.2547327919942053\n",
      "(250000,)\n",
      "Gradient Descent(1281/2999): loss=0.3899173852840874, w0=0.009635257362682764, w1=-0.25473278459293625\n",
      "(250000,)\n",
      "Gradient Descent(1282/2999): loss=0.38991738526363945, w0=0.00963525434915952, w1=-0.2547327772128456\n",
      "(250000,)\n",
      "Gradient Descent(1283/2999): loss=0.38991738524330793, w0=0.009635251344183455, w1=-0.25473276985387194\n",
      "(250000,)\n",
      "Gradient Descent(1284/2999): loss=0.38991738522309227, w0=0.009635248347730769, w1=-0.25473276251595406\n",
      "(250000,)\n",
      "Gradient Descent(1285/2999): loss=0.3899173852029915, w0=0.009635245359777748, w1=-0.25473275519903094\n",
      "(250000,)\n",
      "Gradient Descent(1286/2999): loss=0.38991738518300545, w0=0.009635242380300718, w1=-0.25473274790304173\n",
      "(250000,)\n",
      "Gradient Descent(1287/2999): loss=0.38991738516313307, w0=0.009635239409276072, w1=-0.2547327406279257\n",
      "(250000,)\n",
      "Gradient Descent(1288/2999): loss=0.3899173851433738, w0=0.009635236446680254, w1=-0.2547327333736225\n",
      "(250000,)\n",
      "Gradient Descent(1289/2999): loss=0.3899173851237272, w0=0.009635233492489778, w1=-0.25473272614007175\n",
      "(250000,)\n",
      "Gradient Descent(1290/2999): loss=0.3899173851041922, w0=0.009635230546681221, w1=-0.2547327189272134\n",
      "(250000,)\n",
      "Gradient Descent(1291/2999): loss=0.3899173850847686, w0=0.00963522760923121, w1=-0.25473271173498746\n",
      "(250000,)\n",
      "Gradient Descent(1292/2999): loss=0.3899173850654555, w0=0.009635224680116438, w1=-0.25473270456333424\n",
      "(250000,)\n",
      "Gradient Descent(1293/2999): loss=0.3899173850462524, w0=0.009635221759313648, w1=-0.2547326974121942\n",
      "(250000,)\n",
      "Gradient Descent(1294/2999): loss=0.38991738502715856, w0=0.009635218846799647, w1=-0.2547326902815079\n",
      "(250000,)\n",
      "Gradient Descent(1295/2999): loss=0.3899173850081735, w0=0.009635215942551312, w1=-0.2547326831712162\n",
      "(250000,)\n",
      "Gradient Descent(1296/2999): loss=0.38991738498929646, w0=0.009635213046545568, w1=-0.2547326760812601\n",
      "(250000,)\n",
      "Gradient Descent(1297/2999): loss=0.389917384970527, w0=0.009635210158759405, w1=-0.2547326690115807\n",
      "(250000,)\n",
      "Gradient Descent(1298/2999): loss=0.38991738495186434, w0=0.009635207279169866, w1=-0.2547326619621194\n",
      "(250000,)\n",
      "Gradient Descent(1299/2999): loss=0.3899173849333081, w0=0.009635204407754065, w1=-0.25473265493281777\n",
      "(250000,)\n",
      "Gradient Descent(1300/2999): loss=0.38991738491485733, w0=0.009635201544489156, w1=-0.25473264792361744\n",
      "(250000,)\n",
      "Gradient Descent(1301/2999): loss=0.3899173848965116, w0=0.009635198689352382, w1=-0.2547326409344603\n",
      "(250000,)\n",
      "Gradient Descent(1302/2999): loss=0.3899173848782705, w0=0.00963519584232101, w1=-0.2547326339652884\n",
      "(250000,)\n",
      "Gradient Descent(1303/2999): loss=0.38991738486013305, w0=0.009635193003372377, w1=-0.254732627016044\n",
      "(250000,)\n",
      "Gradient Descent(1304/2999): loss=0.3899173848420989, w0=0.009635190172483885, w1=-0.25473262008666947\n",
      "(250000,)\n",
      "Gradient Descent(1305/2999): loss=0.38991738482416755, w0=0.009635187349632998, w1=-0.25473261317710744\n",
      "(250000,)\n",
      "Gradient Descent(1306/2999): loss=0.38991738480633814, w0=0.009635184534797234, w1=-0.2547326062873006\n",
      "(250000,)\n",
      "Gradient Descent(1307/2999): loss=0.3899173847886104, w0=0.00963518172795417, w1=-0.25473259941719195\n",
      "(250000,)\n",
      "Gradient Descent(1308/2999): loss=0.3899173847709834, w0=0.009635178929081436, w1=-0.25473259256672454\n",
      "(250000,)\n",
      "Gradient Descent(1309/2999): loss=0.38991738475345694, w0=0.009635176138156731, w1=-0.2547325857358417\n",
      "(250000,)\n",
      "Gradient Descent(1310/2999): loss=0.3899173847360302, w0=0.009635173355157801, w1=-0.25473257892448675\n",
      "(250000,)\n",
      "Gradient Descent(1311/2999): loss=0.38991738471870263, w0=0.009635170580062443, w1=-0.2547325721326034\n",
      "(250000,)\n",
      "Gradient Descent(1312/2999): loss=0.38991738470147375, w0=0.00963516781284854, w1=-0.25473256536013544\n",
      "(250000,)\n",
      "Gradient Descent(1313/2999): loss=0.389917384684343, w0=0.009635165053494005, w1=-0.2547325586070268\n",
      "(250000,)\n",
      "Gradient Descent(1314/2999): loss=0.3899173846673098, w0=0.009635162301976826, w1=-0.25473255187322164\n",
      "(250000,)\n",
      "Gradient Descent(1315/2999): loss=0.38991738465037346, w0=0.009635159558275045, w1=-0.2547325451586642\n",
      "(250000,)\n",
      "Gradient Descent(1316/2999): loss=0.38991738463353365, w0=0.009635156822366755, w1=-0.254732538463299\n",
      "(250000,)\n",
      "Gradient Descent(1317/2999): loss=0.3899173846167897, w0=0.009635154094230124, w1=-0.2547325317870706\n",
      "(250000,)\n",
      "Gradient Descent(1318/2999): loss=0.38991738460014114, w0=0.009635151373843347, w1=-0.25473252512992384\n",
      "(250000,)\n",
      "Gradient Descent(1319/2999): loss=0.38991738458358727, w0=0.009635148661184697, w1=-0.25473251849180367\n",
      "(250000,)\n",
      "Gradient Descent(1320/2999): loss=0.3899173845671276, w0=0.009635145956232504, w1=-0.2547325118726552\n",
      "(250000,)\n",
      "Gradient Descent(1321/2999): loss=0.3899173845507617, w0=0.00963514325896516, w1=-0.25473250527242375\n",
      "(250000,)\n",
      "Gradient Descent(1322/2999): loss=0.389917384534489, w0=0.009635140569361092, w1=-0.2547324986910547\n",
      "(250000,)\n",
      "Gradient Descent(1323/2999): loss=0.38991738451830904, w0=0.009635137887398797, w1=-0.2547324921284937\n",
      "(250000,)\n",
      "Gradient Descent(1324/2999): loss=0.389917384502221, w0=0.009635135213056832, w1=-0.2547324855846866\n",
      "(250000,)\n",
      "Gradient Descent(1325/2999): loss=0.38991738448622465, w0=0.009635132546313803, w1=-0.25473247905957924\n",
      "(250000,)\n",
      "Gradient Descent(1326/2999): loss=0.3899173844703195, w0=0.009635129887148385, w1=-0.2547324725531178\n",
      "(250000,)\n",
      "Gradient Descent(1327/2999): loss=0.3899173844545048, w0=0.009635127235539304, w1=-0.2547324660652485\n",
      "(250000,)\n",
      "Gradient Descent(1328/2999): loss=0.38991738443878005, w0=0.009635124591465333, w1=-0.25473245959591784\n",
      "(250000,)\n",
      "Gradient Descent(1329/2999): loss=0.3899173844231449, w0=0.009635121954905318, w1=-0.2547324531450723\n",
      "(250000,)\n",
      "Gradient Descent(1330/2999): loss=0.3899173844075989, w0=0.009635119325838144, w1=-0.2547324467126587\n",
      "(250000,)\n",
      "Gradient Descent(1331/2999): loss=0.3899173843921412, w0=0.00963511670424276, w1=-0.2547324402986239\n",
      "(250000,)\n",
      "Gradient Descent(1332/2999): loss=0.3899173843767716, w0=0.00963511409009817, w1=-0.254732433902915\n",
      "(250000,)\n",
      "Gradient Descent(1333/2999): loss=0.38991738436148954, w0=0.009635111483383441, w1=-0.2547324275254792\n",
      "(250000,)\n",
      "Gradient Descent(1334/2999): loss=0.3899173843462945, w0=0.009635108884077679, w1=-0.2547324211662639\n",
      "(250000,)\n",
      "Gradient Descent(1335/2999): loss=0.38991738433118595, w0=0.009635106292160066, w1=-0.25473241482521664\n",
      "(250000,)\n",
      "Gradient Descent(1336/2999): loss=0.38991738431616335, w0=0.009635103707609831, w1=-0.25473240850228507\n",
      "(250000,)\n",
      "Gradient Descent(1337/2999): loss=0.38991738430122636, w0=0.009635101130406254, w1=-0.2547324021974171\n",
      "(250000,)\n",
      "Gradient Descent(1338/2999): loss=0.3899173842863744, w0=0.009635098560528674, w1=-0.2547323959105607\n",
      "(250000,)\n",
      "Gradient Descent(1339/2999): loss=0.38991738427160694, w0=0.009635095997956469, w1=-0.254732389641664\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1340/2999): loss=0.3899173842569236, w0=0.009635093442669098, w1=-0.25473238339067533\n",
      "(250000,)\n",
      "Gradient Descent(1341/2999): loss=0.3899173842423239, w0=0.009635090894646063, w1=-0.2547323771575432\n",
      "(250000,)\n",
      "Gradient Descent(1342/2999): loss=0.38991738422780725, w0=0.00963508835386693, w1=-0.2547323709422162\n",
      "(250000,)\n",
      "Gradient Descent(1343/2999): loss=0.3899173842133734, w0=0.009635085820311315, w1=-0.25473236474464306\n",
      "(250000,)\n",
      "Gradient Descent(1344/2999): loss=0.38991738419902155, w0=0.009635083293958872, w1=-0.2547323585647728\n",
      "(250000,)\n",
      "Gradient Descent(1345/2999): loss=0.38991738418475147, w0=0.00963508077478932, w1=-0.25473235240255443\n",
      "(250000,)\n",
      "Gradient Descent(1346/2999): loss=0.3899173841705626, w0=0.00963507826278245, w1=-0.2547323462579372\n",
      "(250000,)\n",
      "Gradient Descent(1347/2999): loss=0.3899173841564546, w0=0.00963507575791808, w1=-0.2547323401308705\n",
      "(250000,)\n",
      "Gradient Descent(1348/2999): loss=0.389917384142427, w0=0.009635073260176106, w1=-0.2547323340213038\n",
      "(250000,)\n",
      "Gradient Descent(1349/2999): loss=0.3899173841284791, w0=0.009635070769536458, w1=-0.2547323279291868\n",
      "(250000,)\n",
      "Gradient Descent(1350/2999): loss=0.3899173841146106, w0=0.009635068285979133, w1=-0.2547323218544694\n",
      "(250000,)\n",
      "Gradient Descent(1351/2999): loss=0.3899173841008211, w0=0.00963506580948418, w1=-0.25473231579710154\n",
      "(250000,)\n",
      "Gradient Descent(1352/2999): loss=0.38991738408711013, w0=0.009635063340031697, w1=-0.25473230975703326\n",
      "(250000,)\n",
      "Gradient Descent(1353/2999): loss=0.38991738407347726, w0=0.009635060877601836, w1=-0.2547323037342149\n",
      "(250000,)\n",
      "Gradient Descent(1354/2999): loss=0.38991738405992205, w0=0.009635058422174813, w1=-0.2547322977285969\n",
      "(250000,)\n",
      "Gradient Descent(1355/2999): loss=0.3899173840464439, w0=0.00963505597373088, w1=-0.2547322917401298\n",
      "(250000,)\n",
      "Gradient Descent(1356/2999): loss=0.3899173840330425, w0=0.009635053532250356, w1=-0.25473228576876433\n",
      "(250000,)\n",
      "Gradient Descent(1357/2999): loss=0.3899173840197175, w0=0.009635051097713612, w1=-0.25473227981445135\n",
      "(250000,)\n",
      "Gradient Descent(1358/2999): loss=0.3899173840064683, w0=0.009635048670101063, w1=-0.2547322738771418\n",
      "(250000,)\n",
      "Gradient Descent(1359/2999): loss=0.38991738399329456, w0=0.009635046249393191, w1=-0.2547322679567869\n",
      "(250000,)\n",
      "Gradient Descent(1360/2999): loss=0.3899173839801957, w0=0.009635043835570513, w1=-0.2547322620533379\n",
      "(250000,)\n",
      "Gradient Descent(1361/2999): loss=0.3899173839671715, w0=0.009635041428613617, w1=-0.2547322561667462\n",
      "(250000,)\n",
      "Gradient Descent(1362/2999): loss=0.3899173839542215, w0=0.009635039028503123, w1=-0.2547322502969635\n",
      "(250000,)\n",
      "Gradient Descent(1363/2999): loss=0.3899173839413453, w0=0.009635036635219725, w1=-0.25473224444394144\n",
      "(250000,)\n",
      "Gradient Descent(1364/2999): loss=0.3899173839285423, w0=0.009635034248744158, w1=-0.2547322386076319\n",
      "(250000,)\n",
      "Gradient Descent(1365/2999): loss=0.3899173839158122, w0=0.009635031869057207, w1=-0.25473223278798685\n",
      "(250000,)\n",
      "Gradient Descent(1366/2999): loss=0.3899173839031545, w0=0.009635029496139725, w1=-0.25473222698495845\n",
      "(250000,)\n",
      "Gradient Descent(1367/2999): loss=0.38991738389056896, w0=0.009635027129972594, w1=-0.25473222119849903\n",
      "(250000,)\n",
      "Gradient Descent(1368/2999): loss=0.3899173838780551, w0=0.009635024770536762, w1=-0.254732215428561\n",
      "(250000,)\n",
      "Gradient Descent(1369/2999): loss=0.3899173838656125, w0=0.00963502241781323, w1=-0.2547322096750969\n",
      "(250000,)\n",
      "Gradient Descent(1370/2999): loss=0.38991738385324065, w0=0.00963502007178305, w1=-0.2547322039380595\n",
      "(250000,)\n",
      "Gradient Descent(1371/2999): loss=0.3899173838409393, w0=0.009635017732427314, w1=-0.25473219821740156\n",
      "(250000,)\n",
      "Gradient Descent(1372/2999): loss=0.3899173838287079, w0=0.009635015399727175, w1=-0.25473219251307616\n",
      "(250000,)\n",
      "Gradient Descent(1373/2999): loss=0.38991738381654634, w0=0.009635013073663838, w1=-0.25473218682503634\n",
      "(250000,)\n",
      "Gradient Descent(1374/2999): loss=0.3899173838044538, w0=0.009635010754218565, w1=-0.2547321811532354\n",
      "(250000,)\n",
      "Gradient Descent(1375/2999): loss=0.3899173837924304, w0=0.009635008441372661, w1=-0.2547321754976268\n",
      "(250000,)\n",
      "Gradient Descent(1376/2999): loss=0.389917383780475, w0=0.009635006135107477, w1=-0.25473216985816394\n",
      "(250000,)\n",
      "Gradient Descent(1377/2999): loss=0.389917383768588, w0=0.009635003835404419, w1=-0.2547321642348006\n",
      "(250000,)\n",
      "Gradient Descent(1378/2999): loss=0.3899173837567686, w0=0.009635001542244948, w1=-0.25473215862749055\n",
      "(250000,)\n",
      "Gradient Descent(1379/2999): loss=0.38991738374501644, w0=0.00963499925561057, w1=-0.25473215303618774\n",
      "(250000,)\n",
      "Gradient Descent(1380/2999): loss=0.3899173837333314, w0=0.009634996975482844, w1=-0.25473214746084627\n",
      "(250000,)\n",
      "Gradient Descent(1381/2999): loss=0.38991738372171275, w0=0.009634994701843385, w1=-0.2547321419014203\n",
      "(250000,)\n",
      "Gradient Descent(1382/2999): loss=0.38991738371016016, w0=0.009634992434673847, w1=-0.25473213635786424\n",
      "(250000,)\n",
      "Gradient Descent(1383/2999): loss=0.3899173836986734, w0=0.00963499017395595, w1=-0.2547321308301325\n",
      "(250000,)\n",
      "Gradient Descent(1384/2999): loss=0.3899173836872521, w0=0.00963498791967146, w1=-0.2547321253181798\n",
      "(250000,)\n",
      "Gradient Descent(1385/2999): loss=0.38991738367589585, w0=0.009634985671802172, w1=-0.2547321198219608\n",
      "(250000,)\n",
      "Gradient Descent(1386/2999): loss=0.3899173836646041, w0=0.009634983430329952, w1=-0.25473211434143045\n",
      "(250000,)\n",
      "Gradient Descent(1387/2999): loss=0.3899173836533768, w0=0.009634981195236718, w1=-0.2547321088765437\n",
      "(250000,)\n",
      "Gradient Descent(1388/2999): loss=0.38991738364221334, w0=0.009634978966504426, w1=-0.25473210342725566\n",
      "(250000,)\n",
      "Gradient Descent(1389/2999): loss=0.38991738363111345, w0=0.009634976744115085, w1=-0.2547320979935217\n",
      "(250000,)\n",
      "Gradient Descent(1390/2999): loss=0.3899173836200768, w0=0.009634974528050755, w1=-0.2547320925752972\n",
      "(250000,)\n",
      "Gradient Descent(1391/2999): loss=0.3899173836091029, w0=0.009634972318293545, w1=-0.2547320871725377\n",
      "(250000,)\n",
      "Gradient Descent(1392/2999): loss=0.3899173835981915, w0=0.009634970114825605, w1=-0.25473208178519885\n",
      "(250000,)\n",
      "Gradient Descent(1393/2999): loss=0.38991738358734235, w0=0.009634967917629148, w1=-0.2547320764132364\n",
      "(250000,)\n",
      "Gradient Descent(1394/2999): loss=0.3899173835765548, w0=0.009634965726686427, w1=-0.2547320710566064\n",
      "(250000,)\n",
      "Gradient Descent(1395/2999): loss=0.38991738356582883, w0=0.009634963541979746, w1=-0.2547320657152648\n",
      "(250000,)\n",
      "Gradient Descent(1396/2999): loss=0.38991738355516387, w0=0.00963496136349145, w1=-0.25473206038916785\n",
      "(250000,)\n",
      "Gradient Descent(1397/2999): loss=0.3899173835445596, w0=0.009634959191203947, w1=-0.2547320550782718\n",
      "(250000,)\n",
      "Gradient Descent(1398/2999): loss=0.3899173835340157, w0=0.0096349570250997, w1=-0.2547320497825331\n",
      "(250000,)\n",
      "Gradient Descent(1399/2999): loss=0.38991738352353184, w0=0.009634954865161189, w1=-0.25473204450190834\n",
      "(250000,)\n",
      "Gradient Descent(1400/2999): loss=0.3899173835131076, w0=0.009634952711370964, w1=-0.25473203923635424\n",
      "(250000,)\n",
      "Gradient Descent(1401/2999): loss=0.38991738350274285, w0=0.009634950563711631, w1=-0.25473203398582756\n",
      "(250000,)\n",
      "Gradient Descent(1402/2999): loss=0.389917383492437, w0=0.009634948422165824, w1=-0.2547320287502853\n",
      "(250000,)\n",
      "Gradient Descent(1403/2999): loss=0.38991738348218996, w0=0.009634946286716227, w1=-0.25473202352968444\n",
      "(250000,)\n",
      "Gradient Descent(1404/2999): loss=0.38991738347200106, w0=0.009634944157345591, w1=-0.2547320183239823\n",
      "(250000,)\n",
      "Gradient Descent(1405/2999): loss=0.3899173834618703, w0=0.009634942034036701, w1=-0.2547320131331361\n",
      "(250000,)\n",
      "Gradient Descent(1406/2999): loss=0.3899173834517973, w0=0.009634939916772386, w1=-0.2547320079571033\n",
      "(250000,)\n",
      "Gradient Descent(1407/2999): loss=0.3899173834417815, w0=0.00963493780553553, w1=-0.2547320027958415\n",
      "(250000,)\n",
      "Gradient Descent(1408/2999): loss=0.38991738343182275, w0=0.009634935700309053, w1=-0.25473199764930843\n",
      "(250000,)\n",
      "Gradient Descent(1409/2999): loss=0.3899173834219207, w0=0.009634933601075934, w1=-0.2547319925174618\n",
      "(250000,)\n",
      "Gradient Descent(1410/2999): loss=0.3899173834120751, w0=0.009634931507819204, w1=-0.25473198740025965\n",
      "(250000,)\n",
      "Gradient Descent(1411/2999): loss=0.38991738340228554, w0=0.00963492942052192, w1=-0.25473198229766\n",
      "(250000,)\n",
      "Gradient Descent(1412/2999): loss=0.38991738339255166, w0=0.009634927339167208, w1=-0.254731977209621\n",
      "(250000,)\n",
      "Gradient Descent(1413/2999): loss=0.38991738338287324, w0=0.009634925263738228, w1=-0.254731972136101\n",
      "(250000,)\n",
      "Gradient Descent(1414/2999): loss=0.38991738337324994, w0=0.009634923194218194, w1=-0.2547319670770584\n",
      "(250000,)\n",
      "Gradient Descent(1415/2999): loss=0.38991738336368137, w0=0.009634921130590359, w1=-0.2547319620324518\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1416/2999): loss=0.3899173833541673, w0=0.00963491907283802, w1=-0.2547319570022398\n",
      "(250000,)\n",
      "Gradient Descent(1417/2999): loss=0.3899173833447075, w0=0.009634917020944534, w1=-0.25473195198638116\n",
      "(250000,)\n",
      "Gradient Descent(1418/2999): loss=0.38991738333530135, w0=0.009634914974893294, w1=-0.25473194698483487\n",
      "(250000,)\n",
      "Gradient Descent(1419/2999): loss=0.38991738332594894, w0=0.00963491293466775, w1=-0.25473194199755994\n",
      "(250000,)\n",
      "Gradient Descent(1420/2999): loss=0.38991738331664977, w0=0.00963491090025137, w1=-0.2547319370245155\n",
      "(250000,)\n",
      "Gradient Descent(1421/2999): loss=0.38991738330740344, w0=0.009634908871627702, w1=-0.25473193206566086\n",
      "(250000,)\n",
      "Gradient Descent(1422/2999): loss=0.38991738329820985, w0=0.009634906848780316, w1=-0.2547319271209553\n",
      "(250000,)\n",
      "Gradient Descent(1423/2999): loss=0.3899173832890685, w0=0.009634904831692844, w1=-0.25473192219035845\n",
      "(250000,)\n",
      "Gradient Descent(1424/2999): loss=0.3899173832799792, w0=0.009634902820348956, w1=-0.2547319172738298\n",
      "(250000,)\n",
      "Gradient Descent(1425/2999): loss=0.38991738327094183, w0=0.00963490081473236, w1=-0.2547319123713292\n",
      "(250000,)\n",
      "Gradient Descent(1426/2999): loss=0.38991738326195563, w0=0.009634898814826818, w1=-0.25473190748281643\n",
      "(250000,)\n",
      "Gradient Descent(1427/2999): loss=0.3899173832530207, w0=0.009634896820616135, w1=-0.25473190260825146\n",
      "(250000,)\n",
      "Gradient Descent(1428/2999): loss=0.3899173832441368, w0=0.009634894832084169, w1=-0.2547318977475944\n",
      "(250000,)\n",
      "Gradient Descent(1429/2999): loss=0.3899173832353034, w0=0.009634892849214802, w1=-0.25473189290080545\n",
      "(250000,)\n",
      "Gradient Descent(1430/2999): loss=0.38991738322652025, w0=0.009634890871991982, w1=-0.2547318880678449\n",
      "(250000,)\n",
      "Gradient Descent(1431/2999): loss=0.389917383217787, w0=0.009634888900399692, w1=-0.25473188324867324\n",
      "(250000,)\n",
      "Gradient Descent(1432/2999): loss=0.38991738320910363, w0=0.009634886934421964, w1=-0.25473187844325096\n",
      "(250000,)\n",
      "Gradient Descent(1433/2999): loss=0.38991738320046976, w0=0.009634884974042867, w1=-0.25473187365153876\n",
      "(250000,)\n",
      "Gradient Descent(1434/2999): loss=0.389917383191885, w0=0.009634883019246528, w1=-0.25473186887349736\n",
      "(250000,)\n",
      "Gradient Descent(1435/2999): loss=0.38991738318334895, w0=0.0096348810700171, w1=-0.2547318641090877\n",
      "(250000,)\n",
      "Gradient Descent(1436/2999): loss=0.38991738317486163, w0=0.009634879126338803, w1=-0.25473185935827075\n",
      "(250000,)\n",
      "Gradient Descent(1437/2999): loss=0.38991738316642266, w0=0.009634877188195884, w1=-0.25473185462100767\n",
      "(250000,)\n",
      "Gradient Descent(1438/2999): loss=0.38991738315803165, w0=0.009634875255572621, w1=-0.25473184989725967\n",
      "(250000,)\n",
      "Gradient Descent(1439/2999): loss=0.3899173831496885, w0=0.009634873328453374, w1=-0.25473184518698805\n",
      "(250000,)\n",
      "Gradient Descent(1440/2999): loss=0.3899173831413928, w0=0.009634871406822516, w1=-0.2547318404901543\n",
      "(250000,)\n",
      "Gradient Descent(1441/2999): loss=0.3899173831331443, w0=0.009634869490664475, w1=-0.25473183580671993\n",
      "(250000,)\n",
      "Gradient Descent(1442/2999): loss=0.38991738312494284, w0=0.009634867579963714, w1=-0.25473183113664666\n",
      "(250000,)\n",
      "Gradient Descent(1443/2999): loss=0.3899173831167881, w0=0.009634865674704748, w1=-0.2547318264798963\n",
      "(250000,)\n",
      "Gradient Descent(1444/2999): loss=0.38991738310867974, w0=0.009634863774872132, w1=-0.25473182183643067\n",
      "(250000,)\n",
      "Gradient Descent(1445/2999): loss=0.3899173831006174, w0=0.009634861880450474, w1=-0.25473181720621185\n",
      "(250000,)\n",
      "Gradient Descent(1446/2999): loss=0.3899173830926012, w0=0.009634859991424409, w1=-0.2547318125892019\n",
      "(250000,)\n",
      "Gradient Descent(1447/2999): loss=0.38991738308463053, w0=0.009634858107778612, w1=-0.25473180798536316\n",
      "(250000,)\n",
      "Gradient Descent(1448/2999): loss=0.38991738307670526, w0=0.009634856229497826, w1=-0.25473180339465784\n",
      "(250000,)\n",
      "Gradient Descent(1449/2999): loss=0.38991738306882506, w0=0.009634854356566813, w1=-0.2547317988170484\n",
      "(250000,)\n",
      "Gradient Descent(1450/2999): loss=0.3899173830609898, w0=0.009634852488970384, w1=-0.2547317942524974\n",
      "(250000,)\n",
      "Gradient Descent(1451/2999): loss=0.3899173830531991, w0=0.009634850626693399, w1=-0.25473178970096755\n",
      "(250000,)\n",
      "Gradient Descent(1452/2999): loss=0.3899173830454527, w0=0.009634848769720754, w1=-0.25473178516242156\n",
      "(250000,)\n",
      "Gradient Descent(1453/2999): loss=0.3899173830377504, w0=0.009634846918037385, w1=-0.25473178063682234\n",
      "(250000,)\n",
      "Gradient Descent(1454/2999): loss=0.38991738303009216, w0=0.009634845071628276, w1=-0.25473177612413284\n",
      "(250000,)\n",
      "Gradient Descent(1455/2999): loss=0.3899173830224773, w0=0.009634843230478453, w1=-0.2547317716243162\n",
      "(250000,)\n",
      "Gradient Descent(1456/2999): loss=0.38991738301490597, w0=0.009634841394572985, w1=-0.2547317671373356\n",
      "(250000,)\n",
      "Gradient Descent(1457/2999): loss=0.3899173830073777, w0=0.009634839563896977, w1=-0.25473176266315434\n",
      "(250000,)\n",
      "Gradient Descent(1458/2999): loss=0.3899173829998922, w0=0.009634837738435566, w1=-0.2547317582017358\n",
      "(250000,)\n",
      "Gradient Descent(1459/2999): loss=0.38991738299244943, w0=0.00963483591817396, w1=-0.2547317537530435\n",
      "(250000,)\n",
      "Gradient Descent(1460/2999): loss=0.3899173829850489, w0=0.009634834103097385, w1=-0.2547317493170411\n",
      "(250000,)\n",
      "Gradient Descent(1461/2999): loss=0.38991738297769063, w0=0.009634832293191113, w1=-0.2547317448936923\n",
      "(250000,)\n",
      "Gradient Descent(1462/2999): loss=0.38991738297037415, w0=0.009634830488440466, w1=-0.25473174048296093\n",
      "(250000,)\n",
      "Gradient Descent(1463/2999): loss=0.3899173829630995, w0=0.009634828688830794, w1=-0.2547317360848109\n",
      "(250000,)\n",
      "Gradient Descent(1464/2999): loss=0.38991738295586614, w0=0.009634826894347494, w1=-0.2547317316992063\n",
      "(250000,)\n",
      "Gradient Descent(1465/2999): loss=0.3899173829486739, w0=0.009634825104976003, w1=-0.25473172732611127\n",
      "(250000,)\n",
      "Gradient Descent(1466/2999): loss=0.3899173829415227, w0=0.0096348233207018, w1=-0.25473172296549\n",
      "(250000,)\n",
      "Gradient Descent(1467/2999): loss=0.3899173829344122, w0=0.0096348215415104, w1=-0.2547317186173069\n",
      "(250000,)\n",
      "Gradient Descent(1468/2999): loss=0.3899173829273423, w0=0.009634819767387373, w1=-0.2547317142815264\n",
      "(250000,)\n",
      "Gradient Descent(1469/2999): loss=0.3899173829203125, w0=0.009634817998318326, w1=-0.25473170995811306\n",
      "(250000,)\n",
      "Gradient Descent(1470/2999): loss=0.3899173829133227, w0=0.009634816234288886, w1=-0.2547317056470315\n",
      "(250000,)\n",
      "Gradient Descent(1471/2999): loss=0.3899173829063728, w0=0.009634814475284742, w1=-0.2547317013482465\n",
      "(250000,)\n",
      "Gradient Descent(1472/2999): loss=0.3899173828994625, w0=0.009634812721291613, w1=-0.25473169706172294\n",
      "(250000,)\n",
      "Gradient Descent(1473/2999): loss=0.38991738289259137, w0=0.009634810972295257, w1=-0.25473169278742575\n",
      "(250000,)\n",
      "Gradient Descent(1474/2999): loss=0.38991738288575956, w0=0.00963480922828148, w1=-0.25473168852531997\n",
      "(250000,)\n",
      "Gradient Descent(1475/2999): loss=0.3899173828789665, w0=0.009634807489236112, w1=-0.25473168427537085\n",
      "(250000,)\n",
      "Gradient Descent(1476/2999): loss=0.3899173828722121, w0=0.009634805755145049, w1=-0.2547316800375436\n",
      "(250000,)\n",
      "Gradient Descent(1477/2999): loss=0.38991738286549626, w0=0.009634804025994209, w1=-0.25473167581180356\n",
      "(250000,)\n",
      "Gradient Descent(1478/2999): loss=0.3899173828588186, w0=0.00963480230176955, w1=-0.2547316715981162\n",
      "(250000,)\n",
      "Gradient Descent(1479/2999): loss=0.389917382852179, w0=0.00963480058245707, w1=-0.25473166739644715\n",
      "(250000,)\n",
      "Gradient Descent(1480/2999): loss=0.3899173828455772, w0=0.009634798868042818, w1=-0.254731663206762\n",
      "(250000,)\n",
      "Gradient Descent(1481/2999): loss=0.38991738283901306, w0=0.009634797158512861, w1=-0.2547316590290265\n",
      "(250000,)\n",
      "Gradient Descent(1482/2999): loss=0.3899173828324861, w0=0.00963479545385332, w1=-0.2547316548632066\n",
      "(250000,)\n",
      "Gradient Descent(1483/2999): loss=0.3899173828259965, w0=0.009634793754050358, w1=-0.2547316507092682\n",
      "(250000,)\n",
      "Gradient Descent(1484/2999): loss=0.3899173828195436, w0=0.009634792059090163, w1=-0.2547316465671773\n",
      "(250000,)\n",
      "Gradient Descent(1485/2999): loss=0.3899173828131277, w0=0.009634790368958972, w1=-0.2547316424369001\n",
      "(250000,)\n",
      "Gradient Descent(1486/2999): loss=0.38991738280674815, w0=0.009634788683643057, w1=-0.2547316383184029\n",
      "(250000,)\n",
      "Gradient Descent(1487/2999): loss=0.3899173828004049, w0=0.00963478700312873, w1=-0.25473163421165196\n",
      "(250000,)\n",
      "Gradient Descent(1488/2999): loss=0.389917382794098, w0=0.009634785327402346, w1=-0.2547316301166138\n",
      "(250000,)\n",
      "Gradient Descent(1489/2999): loss=0.3899173827878268, w0=0.009634783656450293, w1=-0.25473162603325494\n",
      "(250000,)\n",
      "Gradient Descent(1490/2999): loss=0.3899173827815914, w0=0.00963478199025899, w1=-0.254731621961542\n",
      "(250000,)\n",
      "Gradient Descent(1491/2999): loss=0.38991738277539145, w0=0.009634780328814905, w1=-0.2547316179014417\n",
      "(250000,)\n",
      "Gradient Descent(1492/2999): loss=0.38991738276922683, w0=0.009634778672104546, w1=-0.2547316138529209\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1493/2999): loss=0.38991738276309734, w0=0.009634777020114442, w1=-0.25473160981594656\n",
      "(250000,)\n",
      "Gradient Descent(1494/2999): loss=0.3899173827570026, w0=0.00963477537283118, w1=-0.25473160579048565\n",
      "(250000,)\n",
      "Gradient Descent(1495/2999): loss=0.3899173827509428, w0=0.009634773730241377, w1=-0.2547316017765053\n",
      "(250000,)\n",
      "Gradient Descent(1496/2999): loss=0.3899173827449173, w0=0.009634772092331689, w1=-0.25473159777397264\n",
      "(250000,)\n",
      "Gradient Descent(1497/2999): loss=0.3899173827389261, w0=0.0096347704590888, w1=-0.2547315937828551\n",
      "(250000,)\n",
      "Gradient Descent(1498/2999): loss=0.3899173827329692, w0=0.00963476883049945, w1=-0.25473158980312\n",
      "(250000,)\n",
      "Gradient Descent(1499/2999): loss=0.38991738272704607, w0=0.009634767206550404, w1=-0.2547315858347349\n",
      "(250000,)\n",
      "Gradient Descent(1500/2999): loss=0.38991738272115667, w0=0.009634765587228462, w1=-0.25473158187766726\n",
      "(250000,)\n",
      "Gradient Descent(1501/2999): loss=0.38991738271530085, w0=0.009634763972520471, w1=-0.2547315779318849\n",
      "(250000,)\n",
      "Gradient Descent(1502/2999): loss=0.38991738270947834, w0=0.0096347623624133, w1=-0.2547315739973555\n",
      "(250000,)\n",
      "Gradient Descent(1503/2999): loss=0.389917382703689, w0=0.009634760756893875, w1=-0.25473157007404695\n",
      "(250000,)\n",
      "Gradient Descent(1504/2999): loss=0.38991738269793264, w0=0.009634759155949144, w1=-0.2547315661619272\n",
      "(250000,)\n",
      "Gradient Descent(1505/2999): loss=0.38991738269220905, w0=0.009634757559566104, w1=-0.25473156226096433\n",
      "(250000,)\n",
      "Gradient Descent(1506/2999): loss=0.389917382686518, w0=0.00963475596773177, w1=-0.25473155837112643\n",
      "(250000,)\n",
      "Gradient Descent(1507/2999): loss=0.38991738268085935, w0=0.009634754380433215, w1=-0.2547315544923818\n",
      "(250000,)\n",
      "Gradient Descent(1508/2999): loss=0.38991738267523285, w0=0.00963475279765753, w1=-0.25473155062469865\n",
      "(250000,)\n",
      "Gradient Descent(1509/2999): loss=0.38991738266963855, w0=0.009634751219391854, w1=-0.2547315467680455\n",
      "(250000,)\n",
      "Gradient Descent(1510/2999): loss=0.3899173826640761, w0=0.009634749645623357, w1=-0.2547315429223908\n",
      "(250000,)\n",
      "Gradient Descent(1511/2999): loss=0.3899173826585452, w0=0.009634748076339242, w1=-0.2547315390877032\n",
      "(250000,)\n",
      "Gradient Descent(1512/2999): loss=0.3899173826530459, w0=0.009634746511526765, w1=-0.25473153526395126\n",
      "(250000,)\n",
      "Gradient Descent(1513/2999): loss=0.38991738264757764, w0=0.00963474495117319, w1=-0.2547315314511039\n",
      "(250000,)\n",
      "Gradient Descent(1514/2999): loss=0.3899173826421408, w0=0.009634743395265849, w1=-0.2547315276491299\n",
      "(250000,)\n",
      "Gradient Descent(1515/2999): loss=0.3899173826367349, w0=0.009634741843792082, w1=-0.2547315238579982\n",
      "(250000,)\n",
      "Gradient Descent(1516/2999): loss=0.38991738263135967, w0=0.009634740296739275, w1=-0.2547315200776779\n",
      "(250000,)\n",
      "Gradient Descent(1517/2999): loss=0.3899173826260151, w0=0.009634738754094856, w1=-0.2547315163081382\n",
      "(250000,)\n",
      "Gradient Descent(1518/2999): loss=0.38991738262070097, w0=0.009634737215846288, w1=-0.25473151254934817\n",
      "(250000,)\n",
      "Gradient Descent(1519/2999): loss=0.389917382615417, w0=0.00963473568198106, w1=-0.25473150880127715\n",
      "(250000,)\n",
      "Gradient Descent(1520/2999): loss=0.38991738261016323, w0=0.0096347341524867, w1=-0.2547315050638946\n",
      "(250000,)\n",
      "Gradient Descent(1521/2999): loss=0.38991738260493924, w0=0.009634732627350776, w1=-0.25473150133717\n",
      "(250000,)\n",
      "Gradient Descent(1522/2999): loss=0.3899173825997452, w0=0.009634731106560894, w1=-0.25473149762107283\n",
      "(250000,)\n",
      "Gradient Descent(1523/2999): loss=0.38991738259458053, w0=0.009634729590104673, w1=-0.25473149391557287\n",
      "(250000,)\n",
      "Gradient Descent(1524/2999): loss=0.3899173825894454, w0=0.00963472807796979, w1=-0.2547314902206398\n",
      "(250000,)\n",
      "Gradient Descent(1525/2999): loss=0.38991738258433944, w0=0.009634726570143947, w1=-0.25473148653624345\n",
      "(250000,)\n",
      "Gradient Descent(1526/2999): loss=0.38991738257926256, w0=0.009634725066614884, w1=-0.25473148286235375\n",
      "(250000,)\n",
      "Gradient Descent(1527/2999): loss=0.3899173825742146, w0=0.009634723567370378, w1=-0.25473147919894074\n",
      "(250000,)\n",
      "Gradient Descent(1528/2999): loss=0.38991738256919534, w0=0.009634722072398237, w1=-0.2547314755459745\n",
      "(250000,)\n",
      "Gradient Descent(1529/2999): loss=0.3899173825642048, w0=0.009634720581686301, w1=-0.25473147190342516\n",
      "(250000,)\n",
      "Gradient Descent(1530/2999): loss=0.38991738255924246, w0=0.009634719095222451, w1=-0.25473146827126303\n",
      "(250000,)\n",
      "Gradient Descent(1531/2999): loss=0.38991738255430847, w0=0.0096347176129946, w1=-0.25473146464945845\n",
      "(250000,)\n",
      "Gradient Descent(1532/2999): loss=0.3899173825494026, w0=0.009634716134990683, w1=-0.25473146103798183\n",
      "(250000,)\n",
      "Gradient Descent(1533/2999): loss=0.3899173825445246, w0=0.009634714661198682, w1=-0.2547314574368037\n",
      "(250000,)\n",
      "Gradient Descent(1534/2999): loss=0.38991738253967445, w0=0.009634713191606617, w1=-0.2547314538458947\n",
      "(250000,)\n",
      "Gradient Descent(1535/2999): loss=0.3899173825348518, w0=0.009634711726202538, w1=-0.2547314502652255\n",
      "(250000,)\n",
      "Gradient Descent(1536/2999): loss=0.38991738253005676, w0=0.009634710264974514, w1=-0.2547314466947668\n",
      "(250000,)\n",
      "Gradient Descent(1537/2999): loss=0.38991738252528885, w0=0.009634708807910673, w1=-0.2547314431344896\n",
      "(250000,)\n",
      "Gradient Descent(1538/2999): loss=0.38991738252054825, w0=0.009634707354999158, w1=-0.2547314395843647\n",
      "(250000,)\n",
      "Gradient Descent(1539/2999): loss=0.38991738251583463, w0=0.009634705906228159, w1=-0.2547314360443632\n",
      "(250000,)\n",
      "Gradient Descent(1540/2999): loss=0.3899173825111478, w0=0.009634704461585871, w1=-0.2547314325144562\n",
      "(250000,)\n",
      "Gradient Descent(1541/2999): loss=0.38991738250648744, w0=0.009634703021060554, w1=-0.2547314289946148\n",
      "(250000,)\n",
      "Gradient Descent(1542/2999): loss=0.3899173825018539, w0=0.009634701584640498, w1=-0.2547314254848104\n",
      "(250000,)\n",
      "Gradient Descent(1543/2999): loss=0.38991738249724667, w0=0.009634700152314008, w1=-0.2547314219850143\n",
      "(250000,)\n",
      "Gradient Descent(1544/2999): loss=0.3899173824926657, w0=0.009634698724069437, w1=-0.2547314184951979\n",
      "(250000,)\n",
      "Gradient Descent(1545/2999): loss=0.38991738248811075, w0=0.009634697299895162, w1=-0.2547314150153328\n",
      "(250000,)\n",
      "Gradient Descent(1546/2999): loss=0.3899173824835817, w0=0.0096346958797796, w1=-0.25473141154539053\n",
      "(250000,)\n",
      "Gradient Descent(1547/2999): loss=0.3899173824790785, w0=0.009634694463711202, w1=-0.2547314080853428\n",
      "(250000,)\n",
      "Gradient Descent(1548/2999): loss=0.389917382474601, w0=0.00963469305167844, w1=-0.25473140463516136\n",
      "(250000,)\n",
      "Gradient Descent(1549/2999): loss=0.3899173824701489, w0=0.00963469164366983, w1=-0.2547314011948181\n",
      "(250000,)\n",
      "Gradient Descent(1550/2999): loss=0.38991738246572216, w0=0.009634690239673921, w1=-0.25473139776428483\n",
      "(250000,)\n",
      "Gradient Descent(1551/2999): loss=0.38991738246132057, w0=0.00963468883967928, w1=-0.25473139434353365\n",
      "(250000,)\n",
      "Gradient Descent(1552/2999): loss=0.38991738245694424, w0=0.009634687443674527, w1=-0.2547313909325366\n",
      "(250000,)\n",
      "Gradient Descent(1553/2999): loss=0.3899173824525927, w0=0.009634686051648305, w1=-0.25473138753126584\n",
      "(250000,)\n",
      "Gradient Descent(1554/2999): loss=0.3899173824482659, w0=0.009634684663589285, w1=-0.25473138413969365\n",
      "(250000,)\n",
      "Gradient Descent(1555/2999): loss=0.3899173824439636, w0=0.00963468327948617, w1=-0.25473138075779234\n",
      "(250000,)\n",
      "Gradient Descent(1556/2999): loss=0.3899173824396861, w0=0.009634681899327707, w1=-0.2547313773855343\n",
      "(250000,)\n",
      "Gradient Descent(1557/2999): loss=0.3899173824354328, w0=0.009634680523102655, w1=-0.25473137402289203\n",
      "(250000,)\n",
      "Gradient Descent(1558/2999): loss=0.3899173824312038, w0=0.009634679150799817, w1=-0.25473137066983803\n",
      "(250000,)\n",
      "Gradient Descent(1559/2999): loss=0.3899173824269987, w0=0.009634677782408035, w1=-0.254731367326345\n",
      "(250000,)\n",
      "Gradient Descent(1560/2999): loss=0.3899173824228176, w0=0.009634676417916163, w1=-0.25473136399238566\n",
      "(250000,)\n",
      "Gradient Descent(1561/2999): loss=0.38991738241866036, w0=0.009634675057313101, w1=-0.2547313606679328\n",
      "(250000,)\n",
      "Gradient Descent(1562/2999): loss=0.3899173824145268, w0=0.009634673700587784, w1=-0.25473135735295926\n",
      "(250000,)\n",
      "Gradient Descent(1563/2999): loss=0.38991738241041674, w0=0.009634672347729172, w1=-0.254731354047438\n",
      "(250000,)\n",
      "Gradient Descent(1564/2999): loss=0.3899173824063301, w0=0.009634670998726249, w1=-0.25473135075134207\n",
      "(250000,)\n",
      "Gradient Descent(1565/2999): loss=0.38991738240226675, w0=0.009634669653568038, w1=-0.25473134746464454\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1566/2999): loss=0.3899173823982265, w0=0.009634668312243591, w1=-0.25473134418731863\n",
      "(250000,)\n",
      "Gradient Descent(1567/2999): loss=0.3899173823942092, w0=0.009634666974741996, w1=-0.2547313409193376\n",
      "(250000,)\n",
      "Gradient Descent(1568/2999): loss=0.3899173823902148, w0=0.009634665641052363, w1=-0.25473133766067474\n",
      "(250000,)\n",
      "Gradient Descent(1569/2999): loss=0.3899173823862432, w0=0.009634664311163846, w1=-0.2547313344113035\n",
      "(250000,)\n",
      "Gradient Descent(1570/2999): loss=0.3899173823822943, w0=0.009634662985065616, w1=-0.2547313311711974\n",
      "(250000,)\n",
      "Gradient Descent(1571/2999): loss=0.3899173823783677, w0=0.009634661662746883, w1=-0.25473132794032993\n",
      "(250000,)\n",
      "Gradient Descent(1572/2999): loss=0.3899173823744635, w0=0.009634660344196886, w1=-0.25473132471867477\n",
      "(250000,)\n",
      "Gradient Descent(1573/2999): loss=0.38991738237058154, w0=0.00963465902940489, w1=-0.25473132150620564\n",
      "(250000,)\n",
      "Gradient Descent(1574/2999): loss=0.38991738236672185, w0=0.009634657718360196, w1=-0.25473131830289636\n",
      "(250000,)\n",
      "Gradient Descent(1575/2999): loss=0.3899173823628838, w0=0.009634656411052132, w1=-0.2547313151087207\n",
      "(250000,)\n",
      "Gradient Descent(1576/2999): loss=0.3899173823590678, w0=0.009634655107470061, w1=-0.25473131192365267\n",
      "(250000,)\n",
      "Gradient Descent(1577/2999): loss=0.38991738235527346, w0=0.009634653807603368, w1=-0.2547313087476663\n",
      "(250000,)\n",
      "Gradient Descent(1578/2999): loss=0.38991738235150086, w0=0.009634652511441471, w1=-0.25473130558073565\n",
      "(250000,)\n",
      "Gradient Descent(1579/2999): loss=0.3899173823477497, w0=0.009634651218973826, w1=-0.25473130242283487\n",
      "(250000,)\n",
      "Gradient Descent(1580/2999): loss=0.3899173823440197, w0=0.00963464993018991, w1=-0.2547312992739382\n",
      "(250000,)\n",
      "Gradient Descent(1581/2999): loss=0.3899173823403112, w0=0.00963464864507924, w1=-0.25473129613401996\n",
      "(250000,)\n",
      "Gradient Descent(1582/2999): loss=0.38991738233662365, w0=0.009634647363631338, w1=-0.2547312930030545\n",
      "(250000,)\n",
      "Gradient Descent(1583/2999): loss=0.3899173823329572, w0=0.009634646085835785, w1=-0.2547312898810164\n",
      "(250000,)\n",
      "Gradient Descent(1584/2999): loss=0.3899173823293115, w0=0.009634644811682193, w1=-0.25473128676788004\n",
      "(250000,)\n",
      "Gradient Descent(1585/2999): loss=0.3899173823256866, w0=0.00963464354116017, w1=-0.2547312836636201\n",
      "(250000,)\n",
      "Gradient Descent(1586/2999): loss=0.38991738232208245, w0=0.009634642274259385, w1=-0.2547312805682112\n",
      "(250000,)\n",
      "Gradient Descent(1587/2999): loss=0.38991738231849865, w0=0.009634641010969511, w1=-0.2547312774816282\n",
      "(250000,)\n",
      "Gradient Descent(1588/2999): loss=0.3899173823149354, w0=0.009634639751280268, w1=-0.25473127440384585\n",
      "(250000,)\n",
      "Gradient Descent(1589/2999): loss=0.38991738231139234, w0=0.009634638495181414, w1=-0.254731271334839\n",
      "(250000,)\n",
      "Gradient Descent(1590/2999): loss=0.38991738230786954, w0=0.009634637242662714, w1=-0.2547312682745827\n",
      "(250000,)\n",
      "Gradient Descent(1591/2999): loss=0.3899173823043668, w0=0.009634635993713968, w1=-0.25473126522305195\n",
      "(250000,)\n",
      "Gradient Descent(1592/2999): loss=0.3899173823008839, w0=0.009634634748325011, w1=-0.25473126218022185\n",
      "(250000,)\n",
      "Gradient Descent(1593/2999): loss=0.38991738229742084, w0=0.009634633506485716, w1=-0.2547312591460676\n",
      "(250000,)\n",
      "Gradient Descent(1594/2999): loss=0.3899173822939776, w0=0.009634632268185953, w1=-0.2547312561205644\n",
      "(250000,)\n",
      "Gradient Descent(1595/2999): loss=0.3899173822905539, w0=0.009634631033415657, w1=-0.2547312531036876\n",
      "(250000,)\n",
      "Gradient Descent(1596/2999): loss=0.3899173822871495, w0=0.00963462980216477, w1=-0.2547312500954126\n",
      "(250000,)\n",
      "Gradient Descent(1597/2999): loss=0.3899173822837648, w0=0.009634628574423269, w1=-0.2547312470957149\n",
      "(250000,)\n",
      "Gradient Descent(1598/2999): loss=0.3899173822803993, w0=0.009634627350181155, w1=-0.25473124410456993\n",
      "(250000,)\n",
      "Gradient Descent(1599/2999): loss=0.3899173822770528, w0=0.009634626129428461, w1=-0.2547312411219534\n",
      "(250000,)\n",
      "Gradient Descent(1600/2999): loss=0.3899173822737256, w0=0.00963462491215526, w1=-0.2547312381478409\n",
      "(250000,)\n",
      "Gradient Descent(1601/2999): loss=0.3899173822704171, w0=0.009634623698351632, w1=-0.2547312351822082\n",
      "(250000,)\n",
      "Gradient Descent(1602/2999): loss=0.3899173822671275, w0=0.009634622488007682, w1=-0.2547312322250311\n",
      "(250000,)\n",
      "Gradient Descent(1603/2999): loss=0.3899173822638568, w0=0.00963462128111357, w1=-0.2547312292762855\n",
      "(250000,)\n",
      "Gradient Descent(1604/2999): loss=0.3899173822606045, w0=0.009634620077659466, w1=-0.25473122633594736\n",
      "(250000,)\n",
      "Gradient Descent(1605/2999): loss=0.3899173822573708, w0=0.009634618877635568, w1=-0.2547312234039927\n",
      "(250000,)\n",
      "Gradient Descent(1606/2999): loss=0.38991738225415556, w0=0.00963461768103211, w1=-0.25473122048039754\n",
      "(250000,)\n",
      "Gradient Descent(1607/2999): loss=0.38991738225095857, w0=0.00963461648783936, w1=-0.25473121756513806\n",
      "(250000,)\n",
      "Gradient Descent(1608/2999): loss=0.3899173822477798, w0=0.00963461529804758, w1=-0.25473121465819054\n",
      "(250000,)\n",
      "Gradient Descent(1609/2999): loss=0.3899173822446192, w0=0.0096346141116471, w1=-0.25473121175953123\n",
      "(250000,)\n",
      "Gradient Descent(1610/2999): loss=0.38991738224147654, w0=0.00963461292862825, w1=-0.25473120886913647\n",
      "(250000,)\n",
      "Gradient Descent(1611/2999): loss=0.3899173822383517, w0=0.009634611748981404, w1=-0.25473120598698273\n",
      "(250000,)\n",
      "Gradient Descent(1612/2999): loss=0.38991738223524464, w0=0.009634610572696945, w1=-0.2547312031130465\n",
      "(250000,)\n",
      "Gradient Descent(1613/2999): loss=0.38991738223215533, w0=0.009634609399765308, w1=-0.2547312002473043\n",
      "(250000,)\n",
      "Gradient Descent(1614/2999): loss=0.3899173822290836, w0=0.009634608230176938, w1=-0.2547311973897327\n",
      "(250000,)\n",
      "Gradient Descent(1615/2999): loss=0.38991738222602945, w0=0.009634607063922308, w1=-0.25473119454030857\n",
      "(250000,)\n",
      "Gradient Descent(1616/2999): loss=0.3899173822229926, w0=0.00963460590099192, w1=-0.25473119169900854\n",
      "(250000,)\n",
      "Gradient Descent(1617/2999): loss=0.389917382219973, w0=0.009634604741376299, w1=-0.2547311888658095\n",
      "(250000,)\n",
      "Gradient Descent(1618/2999): loss=0.3899173822169707, w0=0.009634603585066012, w1=-0.25473118604068823\n",
      "(250000,)\n",
      "Gradient Descent(1619/2999): loss=0.3899173822139855, w0=0.009634602432051642, w1=-0.2547311832236218\n",
      "(250000,)\n",
      "Gradient Descent(1620/2999): loss=0.3899173822110172, w0=0.009634601282323803, w1=-0.25473118041458725\n",
      "(250000,)\n",
      "Gradient Descent(1621/2999): loss=0.3899173822080658, w0=0.009634600135873125, w1=-0.25473117761356157\n",
      "(250000,)\n",
      "Gradient Descent(1622/2999): loss=0.3899173822051312, w0=0.009634598992690284, w1=-0.254731174820522\n",
      "(250000,)\n",
      "Gradient Descent(1623/2999): loss=0.38991738220221345, w0=0.00963459785276596, w1=-0.2547311720354457\n",
      "(250000,)\n",
      "Gradient Descent(1624/2999): loss=0.38991738219931216, w0=0.009634596716090866, w1=-0.25473116925831\n",
      "(250000,)\n",
      "Gradient Descent(1625/2999): loss=0.38991738219642735, w0=0.009634595582655756, w1=-0.2547311664890923\n",
      "(250000,)\n",
      "Gradient Descent(1626/2999): loss=0.3899173821935591, w0=0.00963459445245139, w1=-0.25473116372776994\n",
      "(250000,)\n",
      "Gradient Descent(1627/2999): loss=0.3899173821907071, w0=0.009634593325468576, w1=-0.2547311609743204\n",
      "(250000,)\n",
      "Gradient Descent(1628/2999): loss=0.3899173821878714, w0=0.009634592201698114, w1=-0.2547311582287213\n",
      "(250000,)\n",
      "Gradient Descent(1629/2999): loss=0.3899173821850518, w0=0.009634591081130869, w1=-0.25473115549095016\n",
      "(250000,)\n",
      "Gradient Descent(1630/2999): loss=0.38991738218224825, w0=0.009634589963757715, w1=-0.2547311527609847\n",
      "(250000,)\n",
      "Gradient Descent(1631/2999): loss=0.3899173821794607, w0=0.009634588849569551, w1=-0.2547311500388026\n",
      "(250000,)\n",
      "Gradient Descent(1632/2999): loss=0.38991738217668903, w0=0.009634587738557299, w1=-0.25473114732438173\n",
      "(250000,)\n",
      "Gradient Descent(1633/2999): loss=0.38991738217393307, w0=0.009634586630711909, w1=-0.25473114461769997\n",
      "(250000,)\n",
      "Gradient Descent(1634/2999): loss=0.38991738217119287, w0=0.009634585526024376, w1=-0.25473114191873514\n",
      "(250000,)\n",
      "Gradient Descent(1635/2999): loss=0.38991738216846833, w0=0.009634584424485683, w1=-0.2547311392274653\n",
      "(250000,)\n",
      "Gradient Descent(1636/2999): loss=0.38991738216575916, w0=0.009634583326086866, w1=-0.25473113654386853\n",
      "(250000,)\n",
      "Gradient Descent(1637/2999): loss=0.38991738216306543, w0=0.009634582230818985, w1=-0.25473113386792295\n",
      "(250000,)\n",
      "Gradient Descent(1638/2999): loss=0.3899173821603871, w0=0.009634581138673111, w1=-0.25473113119960666\n",
      "(250000,)\n",
      "Gradient Descent(1639/2999): loss=0.389917382157724, w0=0.009634580049640351, w1=-0.25473112853889796\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1640/2999): loss=0.389917382155076, w0=0.009634578963711832, w1=-0.2547311258857751\n",
      "(250000,)\n",
      "Gradient Descent(1641/2999): loss=0.3899173821524432, w0=0.009634577880878716, w1=-0.2547311232402166\n",
      "(250000,)\n",
      "Gradient Descent(1642/2999): loss=0.3899173821498252, w0=0.009634576801132173, w1=-0.25473112060220066\n",
      "(250000,)\n",
      "Gradient Descent(1643/2999): loss=0.3899173821472223, w0=0.009634575724463421, w1=-0.25473111797170594\n",
      "(250000,)\n",
      "Gradient Descent(1644/2999): loss=0.3899173821446341, w0=0.009634574650863686, w1=-0.2547311153487109\n",
      "(250000,)\n",
      "Gradient Descent(1645/2999): loss=0.3899173821420608, w0=0.00963457358032422, w1=-0.2547311127331942\n",
      "(250000,)\n",
      "Gradient Descent(1646/2999): loss=0.38991738213950194, w0=0.00963457251283631, w1=-0.2547311101251345\n",
      "(250000,)\n",
      "Gradient Descent(1647/2999): loss=0.3899173821369578, w0=0.009634571448391263, w1=-0.2547311075245105\n",
      "(250000,)\n",
      "Gradient Descent(1648/2999): loss=0.3899173821344281, w0=0.009634570386980403, w1=-0.254731104931301\n",
      "(250000,)\n",
      "Gradient Descent(1649/2999): loss=0.38991738213191285, w0=0.009634569328595084, w1=-0.2547311023454849\n",
      "(250000,)\n",
      "Gradient Descent(1650/2999): loss=0.3899173821294118, w0=0.009634568273226693, w1=-0.2547310997670411\n",
      "(250000,)\n",
      "Gradient Descent(1651/2999): loss=0.38991738212692506, w0=0.00963456722086662, w1=-0.25473109719594855\n",
      "(250000,)\n",
      "Gradient Descent(1652/2999): loss=0.38991738212445237, w0=0.009634566171506303, w1=-0.2547310946321863\n",
      "(250000,)\n",
      "Gradient Descent(1653/2999): loss=0.38991738212199395, w0=0.009634565125137188, w1=-0.25473109207573347\n",
      "(250000,)\n",
      "Gradient Descent(1654/2999): loss=0.38991738211954946, w0=0.009634564081750762, w1=-0.2547310895265692\n",
      "(250000,)\n",
      "Gradient Descent(1655/2999): loss=0.3899173821171188, w0=0.00963456304133852, w1=-0.25473108698467267\n",
      "(250000,)\n",
      "Gradient Descent(1656/2999): loss=0.3899173821147021, w0=0.00963456200389199, w1=-0.2547310844500232\n",
      "(250000,)\n",
      "Gradient Descent(1657/2999): loss=0.3899173821122991, w0=0.00963456096940271, w1=-0.2547310819226001\n",
      "(250000,)\n",
      "Gradient Descent(1658/2999): loss=0.3899173821099098, w0=0.009634559937862268, w1=-0.2547310794023827\n",
      "(250000,)\n",
      "Gradient Descent(1659/2999): loss=0.38991738210753407, w0=0.009634558909262257, w1=-0.25473107688935054\n",
      "(250000,)\n",
      "Gradient Descent(1660/2999): loss=0.3899173821051719, w0=0.009634557883594293, w1=-0.2547310743834831\n",
      "(250000,)\n",
      "Gradient Descent(1661/2999): loss=0.38991738210282323, w0=0.00963455686085002, w1=-0.2547310718847599\n",
      "(250000,)\n",
      "Gradient Descent(1662/2999): loss=0.3899173821004878, w0=0.009634555841021117, w1=-0.2547310693931607\n",
      "(250000,)\n",
      "Gradient Descent(1663/2999): loss=0.3899173820981656, w0=0.00963455482409927, w1=-0.2547310669086651\n",
      "(250000,)\n",
      "Gradient Descent(1664/2999): loss=0.38991738209585686, w0=0.0096345538100762, w1=-0.2547310644312528\n",
      "(250000,)\n",
      "Gradient Descent(1665/2999): loss=0.3899173820935612, w0=0.009634552798943648, w1=-0.25473106196090367\n",
      "(250000,)\n",
      "Gradient Descent(1666/2999): loss=0.3899173820912785, w0=0.009634551790693374, w1=-0.2547310594975975\n",
      "(250000,)\n",
      "Gradient Descent(1667/2999): loss=0.3899173820890089, w0=0.009634550785317175, w1=-0.25473105704131427\n",
      "(250000,)\n",
      "Gradient Descent(1668/2999): loss=0.38991738208675214, w0=0.009634549782806846, w1=-0.25473105459203393\n",
      "(250000,)\n",
      "Gradient Descent(1669/2999): loss=0.38991738208450827, w0=0.009634548783154217, w1=-0.2547310521497365\n",
      "(250000,)\n",
      "Gradient Descent(1670/2999): loss=0.3899173820822773, w0=0.009634547786351158, w1=-0.2547310497144021\n",
      "(250000,)\n",
      "Gradient Descent(1671/2999): loss=0.3899173820800588, w0=0.009634546792389545, w1=-0.2547310472860108\n",
      "(250000,)\n",
      "Gradient Descent(1672/2999): loss=0.3899173820778531, w0=0.009634545801261286, w1=-0.25473104486454284\n",
      "(250000,)\n",
      "Gradient Descent(1673/2999): loss=0.3899173820756599, w0=0.009634544812958298, w1=-0.2547310424499785\n",
      "(250000,)\n",
      "Gradient Descent(1674/2999): loss=0.3899173820734791, w0=0.009634543827472527, w1=-0.2547310400422981\n",
      "(250000,)\n",
      "Gradient Descent(1675/2999): loss=0.38991738207131094, w0=0.009634542844795951, w1=-0.25473103764148197\n",
      "(250000,)\n",
      "Gradient Descent(1676/2999): loss=0.3899173820691549, w0=0.00963454186492056, w1=-0.25473103524751056\n",
      "(250000,)\n",
      "Gradient Descent(1677/2999): loss=0.38991738206701126, w0=0.009634540887838383, w1=-0.25473103286036436\n",
      "(250000,)\n",
      "Gradient Descent(1678/2999): loss=0.3899173820648798, w0=0.009634539913541452, w1=-0.2547310304800239\n",
      "(250000,)\n",
      "Gradient Descent(1679/2999): loss=0.3899173820627604, w0=0.009634538942021835, w1=-0.2547310281064697\n",
      "(250000,)\n",
      "Gradient Descent(1680/2999): loss=0.38991738206065313, w0=0.009634537973271611, w1=-0.2547310257396825\n",
      "(250000,)\n",
      "Gradient Descent(1681/2999): loss=0.3899173820585579, w0=0.009634537007282886, w1=-0.254731023379643\n",
      "(250000,)\n",
      "Gradient Descent(1682/2999): loss=0.3899173820564745, w0=0.009634536044047792, w1=-0.2547310210263319\n",
      "(250000,)\n",
      "Gradient Descent(1683/2999): loss=0.3899173820544031, w0=0.009634535083558484, w1=-0.25473101867973\n",
      "(250000,)\n",
      "Gradient Descent(1684/2999): loss=0.38991738205234333, w0=0.009634534125807141, w1=-0.2547310163398183\n",
      "(250000,)\n",
      "Gradient Descent(1685/2999): loss=0.38991738205029547, w0=0.009634533170785962, w1=-0.2547310140065776\n",
      "(250000,)\n",
      "Gradient Descent(1686/2999): loss=0.3899173820482591, w0=0.009634532218487158, w1=-0.2547310116799889\n",
      "(250000,)\n",
      "Gradient Descent(1687/2999): loss=0.38991738204623444, w0=0.009634531268902976, w1=-0.2547310093600333\n",
      "(250000,)\n",
      "Gradient Descent(1688/2999): loss=0.38991738204422133, w0=0.009634530322025676, w1=-0.25473100704669177\n",
      "(250000,)\n",
      "Gradient Descent(1689/2999): loss=0.3899173820422195, w0=0.009634529377847537, w1=-0.25473100473994553\n",
      "(250000,)\n",
      "Gradient Descent(1690/2999): loss=0.38991738204022924, w0=0.009634528436360883, w1=-0.25473100243977576\n",
      "(250000,)\n",
      "Gradient Descent(1691/2999): loss=0.38991738203825027, w0=0.009634527497558037, w1=-0.25473100014616373\n",
      "(250000,)\n",
      "Gradient Descent(1692/2999): loss=0.38991738203628257, w0=0.009634526561431347, w1=-0.2547309978590907\n",
      "(250000,)\n",
      "Gradient Descent(1693/2999): loss=0.38991738203432597, w0=0.009634525627973196, w1=-0.254730995578538\n",
      "(250000,)\n",
      "Gradient Descent(1694/2999): loss=0.3899173820323806, w0=0.009634524697175968, w1=-0.2547309933044871\n",
      "(250000,)\n",
      "Gradient Descent(1695/2999): loss=0.3899173820304463, w0=0.00963452376903209, w1=-0.25473099103691943\n",
      "(250000,)\n",
      "Gradient Descent(1696/2999): loss=0.38991738202852294, w0=0.00963452284353399, w1=-0.2547309887758165\n",
      "(250000,)\n",
      "Gradient Descent(1697/2999): loss=0.38991738202661064, w0=0.009634521920674134, w1=-0.2547309865211599\n",
      "(250000,)\n",
      "Gradient Descent(1698/2999): loss=0.3899173820247092, w0=0.009634521000445003, w1=-0.2547309842729312\n",
      "(250000,)\n",
      "Gradient Descent(1699/2999): loss=0.38991738202281867, w0=0.0096345200828391, w1=-0.25473098203111216\n",
      "(250000,)\n",
      "Gradient Descent(1700/2999): loss=0.3899173820209388, w0=0.009634519167848951, w1=-0.25473097979568443\n",
      "(250000,)\n",
      "Gradient Descent(1701/2999): loss=0.3899173820190696, w0=0.009634518255467088, w1=-0.25473097756662977\n",
      "(250000,)\n",
      "Gradient Descent(1702/2999): loss=0.38991738201721104, w0=0.00963451734568609, w1=-0.25473097534393\n",
      "(250000,)\n",
      "Gradient Descent(1703/2999): loss=0.3899173820153631, w0=0.009634516438498534, w1=-0.2547309731275671\n",
      "(250000,)\n",
      "Gradient Descent(1704/2999): loss=0.38991738201352566, w0=0.009634515533897046, w1=-0.25473097091752295\n",
      "(250000,)\n",
      "Gradient Descent(1705/2999): loss=0.3899173820116988, w0=0.00963451463187424, w1=-0.2547309687137795\n",
      "(250000,)\n",
      "Gradient Descent(1706/2999): loss=0.38991738200988235, w0=0.009634513732422769, w1=-0.25473096651631877\n",
      "(250000,)\n",
      "Gradient Descent(1707/2999): loss=0.389917382008076, w0=0.009634512835535314, w1=-0.2547309643251229\n",
      "(250000,)\n",
      "Gradient Descent(1708/2999): loss=0.3899173820062801, w0=0.009634511941204564, w1=-0.254730962140174\n",
      "(250000,)\n",
      "Gradient Descent(1709/2999): loss=0.38991738200449433, w0=0.00963451104942323, w1=-0.2547309599614543\n",
      "(250000,)\n",
      "Gradient Descent(1710/2999): loss=0.38991738200271886, w0=0.00963451016018405, w1=-0.25473095778894594\n",
      "(250000,)\n",
      "Gradient Descent(1711/2999): loss=0.38991738200095344, w0=0.009634509273479775, w1=-0.2547309556226313\n",
      "(250000,)\n",
      "Gradient Descent(1712/2999): loss=0.3899173819991981, w0=0.00963450838930318, w1=-0.2547309534624927\n",
      "(250000,)\n",
      "Gradient Descent(1713/2999): loss=0.3899173819974527, w0=0.009634507507647064, w1=-0.2547309513085125\n",
      "(250000,)\n",
      "Gradient Descent(1714/2999): loss=0.3899173819957173, w0=0.009634506628504232, w1=-0.2547309491606731\n",
      "(250000,)\n",
      "Gradient Descent(1715/2999): loss=0.3899173819939918, w0=0.009634505751867537, w1=-0.25473094701895715\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1716/2999): loss=0.38991738199227594, w0=0.009634504877729828, w1=-0.2547309448833471\n",
      "(250000,)\n",
      "Gradient Descent(1717/2999): loss=0.38991738199056997, w0=0.009634504006083991, w1=-0.25473094275382546\n",
      "(250000,)\n",
      "Gradient Descent(1718/2999): loss=0.38991738198887377, w0=0.009634503136922915, w1=-0.254730940630375\n",
      "(250000,)\n",
      "Gradient Descent(1719/2999): loss=0.38991738198718706, w0=0.00963450227023952, w1=-0.2547309385129783\n",
      "(250000,)\n",
      "Gradient Descent(1720/2999): loss=0.38991738198551024, w0=0.009634501406026744, w1=-0.25473093640161815\n",
      "(250000,)\n",
      "Gradient Descent(1721/2999): loss=0.3899173819838427, w0=0.00963450054427755, w1=-0.25473093429627736\n",
      "(250000,)\n",
      "Gradient Descent(1722/2999): loss=0.38991738198218473, w0=0.009634499684984914, w1=-0.2547309321969387\n",
      "(250000,)\n",
      "Gradient Descent(1723/2999): loss=0.3899173819805362, w0=0.009634498828141831, w1=-0.25473093010358516\n",
      "(250000,)\n",
      "Gradient Descent(1724/2999): loss=0.38991738197889714, w0=0.009634497973741326, w1=-0.25473092801619956\n",
      "(250000,)\n",
      "Gradient Descent(1725/2999): loss=0.38991738197726733, w0=0.009634497121776428, w1=-0.25473092593476493\n",
      "(250000,)\n",
      "Gradient Descent(1726/2999): loss=0.3899173819756468, w0=0.009634496272240207, w1=-0.2547309238592643\n",
      "(250000,)\n",
      "Gradient Descent(1727/2999): loss=0.3899173819740355, w0=0.009634495425125726, w1=-0.2547309217896808\n",
      "(250000,)\n",
      "Gradient Descent(1728/2999): loss=0.38991738197243353, w0=0.009634494580426091, w1=-0.25473091972599754\n",
      "(250000,)\n",
      "Gradient Descent(1729/2999): loss=0.3899173819708404, w0=0.009634493738134428, w1=-0.2547309176681976\n",
      "(250000,)\n",
      "Gradient Descent(1730/2999): loss=0.3899173819692565, w0=0.009634492898243863, w1=-0.2547309156162643\n",
      "(250000,)\n",
      "Gradient Descent(1731/2999): loss=0.38991738196768155, w0=0.009634492060747556, w1=-0.2547309135701809\n",
      "(250000,)\n",
      "Gradient Descent(1732/2999): loss=0.3899173819661157, w0=0.009634491225638687, w1=-0.2547309115299307\n",
      "(250000,)\n",
      "Gradient Descent(1733/2999): loss=0.3899173819645587, w0=0.009634490392910434, w1=-0.2547309094954971\n",
      "(250000,)\n",
      "Gradient Descent(1734/2999): loss=0.3899173819630105, w0=0.009634489562556029, w1=-0.2547309074668635\n",
      "(250000,)\n",
      "Gradient Descent(1735/2999): loss=0.3899173819614711, w0=0.00963448873456869, w1=-0.25473090544401333\n",
      "(250000,)\n",
      "Gradient Descent(1736/2999): loss=0.3899173819599405, w0=0.009634487908941686, w1=-0.25473090342693017\n",
      "(250000,)\n",
      "Gradient Descent(1737/2999): loss=0.38991738195841863, w0=0.009634487085668284, w1=-0.2547309014155975\n",
      "(250000,)\n",
      "Gradient Descent(1738/2999): loss=0.3899173819569055, w0=0.009634486264741773, w1=-0.254730899409999\n",
      "(250000,)\n",
      "Gradient Descent(1739/2999): loss=0.3899173819554008, w0=0.009634485446155468, w1=-0.25473089741011823\n",
      "(250000,)\n",
      "Gradient Descent(1740/2999): loss=0.38991738195390496, w0=0.009634484629902705, w1=-0.2547308954159389\n",
      "(250000,)\n",
      "Gradient Descent(1741/2999): loss=0.3899173819524174, w0=0.009634483815976825, w1=-0.2547308934274448\n",
      "(250000,)\n",
      "Gradient Descent(1742/2999): loss=0.3899173819509383, w0=0.009634483004371204, w1=-0.25473089144461974\n",
      "(250000,)\n",
      "Gradient Descent(1743/2999): loss=0.38991738194946773, w0=0.009634482195079225, w1=-0.25473088946744754\n",
      "(250000,)\n",
      "Gradient Descent(1744/2999): loss=0.38991738194800546, w0=0.009634481388094295, w1=-0.25473088749591205\n",
      "(250000,)\n",
      "Gradient Descent(1745/2999): loss=0.3899173819465517, w0=0.009634480583409826, w1=-0.25473088552999723\n",
      "(250000,)\n",
      "Gradient Descent(1746/2999): loss=0.3899173819451059, w0=0.009634479781019271, w1=-0.25473088356968704\n",
      "(250000,)\n",
      "Gradient Descent(1747/2999): loss=0.3899173819436686, w0=0.009634478980916092, w1=-0.2547308816149655\n",
      "(250000,)\n",
      "Gradient Descent(1748/2999): loss=0.3899173819422393, w0=0.009634478183093759, w1=-0.25473087966581665\n",
      "(250000,)\n",
      "Gradient Descent(1749/2999): loss=0.38991738194081826, w0=0.009634477387545777, w1=-0.25473087772222464\n",
      "(250000,)\n",
      "Gradient Descent(1750/2999): loss=0.3899173819394053, w0=0.009634476594265666, w1=-0.2547308757841736\n",
      "(250000,)\n",
      "Gradient Descent(1751/2999): loss=0.38991738193800024, w0=0.00963447580324696, w1=-0.2547308738516477\n",
      "(250000,)\n",
      "Gradient Descent(1752/2999): loss=0.38991738193660336, w0=0.009634475014483211, w1=-0.25473087192463134\n",
      "(250000,)\n",
      "Gradient Descent(1753/2999): loss=0.3899173819352144, w0=0.009634474227968001, w1=-0.2547308700031087\n",
      "(250000,)\n",
      "Gradient Descent(1754/2999): loss=0.38991738193383335, w0=0.009634473443694926, w1=-0.254730868087064\n",
      "(250000,)\n",
      "Gradient Descent(1755/2999): loss=0.38991738193246, w0=0.009634472661657578, w1=-0.25473086617648183\n",
      "(250000,)\n",
      "Gradient Descent(1756/2999): loss=0.38991738193109454, w0=0.009634471881849587, w1=-0.2547308642713465\n",
      "(250000,)\n",
      "Gradient Descent(1757/2999): loss=0.389917381929737, w0=0.009634471104264604, w1=-0.2547308623716425\n",
      "(250000,)\n",
      "Gradient Descent(1758/2999): loss=0.3899173819283871, w0=0.009634470328896288, w1=-0.2547308604773544\n",
      "(250000,)\n",
      "Gradient Descent(1759/2999): loss=0.38991738192704495, w0=0.009634469555738327, w1=-0.25473085858846667\n",
      "(250000,)\n",
      "Gradient Descent(1760/2999): loss=0.3899173819257103, w0=0.009634468784784415, w1=-0.254730856704964\n",
      "(250000,)\n",
      "Gradient Descent(1761/2999): loss=0.3899173819243833, w0=0.009634468016028271, w1=-0.25473085482683094\n",
      "(250000,)\n",
      "Gradient Descent(1762/2999): loss=0.38991738192306397, w0=0.00963446724946364, w1=-0.25473085295405223\n",
      "(250000,)\n",
      "Gradient Descent(1763/2999): loss=0.38991738192175207, w0=0.009634466485084259, w1=-0.2547308510866126\n",
      "(250000,)\n",
      "Gradient Descent(1764/2999): loss=0.38991738192044756, w0=0.009634465722883909, w1=-0.2547308492244968\n",
      "(250000,)\n",
      "Gradient Descent(1765/2999): loss=0.3899173819191506, w0=0.00963446496285638, w1=-0.2547308473676897\n",
      "(250000,)\n",
      "Gradient Descent(1766/2999): loss=0.3899173819178609, w0=0.009634464204995469, w1=-0.25473084551617614\n",
      "(250000,)\n",
      "Gradient Descent(1767/2999): loss=0.3899173819165787, w0=0.009634463449295003, w1=-0.25473084366994103\n",
      "(250000,)\n",
      "Gradient Descent(1768/2999): loss=0.38991738191530373, w0=0.009634462695748828, w1=-0.2547308418289693\n",
      "(250000,)\n",
      "Gradient Descent(1769/2999): loss=0.3899173819140359, w0=0.009634461944350806, w1=-0.254730839993246\n",
      "(250000,)\n",
      "Gradient Descent(1770/2999): loss=0.38991738191277536, w0=0.009634461195094814, w1=-0.25473083816275616\n",
      "(250000,)\n",
      "Gradient Descent(1771/2999): loss=0.38991738191152203, w0=0.009634460447974759, w1=-0.25473083633748483\n",
      "(250000,)\n",
      "Gradient Descent(1772/2999): loss=0.38991738191027586, w0=0.009634459702984535, w1=-0.2547308345174171\n",
      "(250000,)\n",
      "Gradient Descent(1773/2999): loss=0.3899173819090368, w0=0.009634458960118074, w1=-0.2547308327025382\n",
      "(250000,)\n",
      "Gradient Descent(1774/2999): loss=0.3899173819078047, w0=0.009634458219369316, w1=-0.25473083089283327\n",
      "(250000,)\n",
      "Gradient Descent(1775/2999): loss=0.3899173819065798, w0=0.009634457480732236, w1=-0.2547308290882876\n",
      "(250000,)\n",
      "Gradient Descent(1776/2999): loss=0.38991738190536157, w0=0.009634456744200803, w1=-0.25473082728888646\n",
      "(250000,)\n",
      "Gradient Descent(1777/2999): loss=0.38991738190415065, w0=0.009634456009769027, w1=-0.25473082549461523\n",
      "(250000,)\n",
      "Gradient Descent(1778/2999): loss=0.38991738190294634, w0=0.009634455277430919, w1=-0.25473082370545924\n",
      "(250000,)\n",
      "Gradient Descent(1779/2999): loss=0.38991738190174896, w0=0.009634454547180506, w1=-0.2547308219214039\n",
      "(250000,)\n",
      "Gradient Descent(1780/2999): loss=0.38991738190055847, w0=0.009634453819011847, w1=-0.25473082014243464\n",
      "(250000,)\n",
      "Gradient Descent(1781/2999): loss=0.38991738189937464, w0=0.009634453092919005, w1=-0.25473081836853706\n",
      "(250000,)\n",
      "Gradient Descent(1782/2999): loss=0.38991738189819763, w0=0.009634452368896065, w1=-0.2547308165996966\n",
      "(250000,)\n",
      "Gradient Descent(1783/2999): loss=0.3899173818970273, w0=0.00963445164693711, w1=-0.2547308148358989\n",
      "(250000,)\n",
      "Gradient Descent(1784/2999): loss=0.38991738189586356, w0=0.00963445092703627, w1=-0.2547308130771295\n",
      "(250000,)\n",
      "Gradient Descent(1785/2999): loss=0.3899173818947065, w0=0.009634450209187677, w1=-0.2547308113233742\n",
      "(250000,)\n",
      "Gradient Descent(1786/2999): loss=0.3899173818935561, w0=0.009634449493385485, w1=-0.25473080957461863\n",
      "(250000,)\n",
      "Gradient Descent(1787/2999): loss=0.38991738189241226, w0=0.009634448779623864, w1=-0.25473080783084856\n",
      "(250000,)\n",
      "Gradient Descent(1788/2999): loss=0.3899173818912748, w0=0.009634448067896996, w1=-0.25473080609204973\n",
      "(250000,)\n",
      "Gradient Descent(1789/2999): loss=0.3899173818901439, w0=0.009634447358199068, w1=-0.254730804358208\n",
      "(250000,)\n",
      "Gradient Descent(1790/2999): loss=0.38991738188901937, w0=0.009634446650524297, w1=-0.2547308026293092\n",
      "(250000,)\n",
      "Gradient Descent(1791/2999): loss=0.38991738188790137, w0=0.009634445944866926, w1=-0.2547308009053393\n",
      "(250000,)\n",
      "Gradient Descent(1792/2999): loss=0.38991738188678954, w0=0.009634445241221206, w1=-0.25473079918628416\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1793/2999): loss=0.3899173818856842, w0=0.009634444539581403, w1=-0.25473079747212984\n",
      "(250000,)\n",
      "Gradient Descent(1794/2999): loss=0.3899173818845852, w0=0.009634443839941791, w1=-0.25473079576286234\n",
      "(250000,)\n",
      "Gradient Descent(1795/2999): loss=0.3899173818834923, w0=0.009634443142296674, w1=-0.25473079405846777\n",
      "(250000,)\n",
      "Gradient Descent(1796/2999): loss=0.3899173818824057, w0=0.00963444244664037, w1=-0.2547307923589322\n",
      "(250000,)\n",
      "Gradient Descent(1797/2999): loss=0.38991738188132524, w0=0.009634441752967206, w1=-0.25473079066424176\n",
      "(250000,)\n",
      "Gradient Descent(1798/2999): loss=0.3899173818802511, w0=0.009634441061271525, w1=-0.25473078897438267\n",
      "(250000,)\n",
      "Gradient Descent(1799/2999): loss=0.38991738187918284, w0=0.009634440371547693, w1=-0.2547307872893411\n",
      "(250000,)\n",
      "Gradient Descent(1800/2999): loss=0.3899173818781208, w0=0.009634439683790083, w1=-0.2547307856091034\n",
      "(250000,)\n",
      "Gradient Descent(1801/2999): loss=0.38991738187706476, w0=0.009634438997993103, w1=-0.2547307839336558\n",
      "(250000,)\n",
      "Gradient Descent(1802/2999): loss=0.3899173818760149, w0=0.009634438314151157, w1=-0.2547307822629847\n",
      "(250000,)\n",
      "Gradient Descent(1803/2999): loss=0.38991738187497066, w0=0.00963443763225867, w1=-0.25473078059707643\n",
      "(250000,)\n",
      "Gradient Descent(1804/2999): loss=0.38991738187393266, w0=0.009634436952310097, w1=-0.25473077893591745\n",
      "(250000,)\n",
      "Gradient Descent(1805/2999): loss=0.38991738187290037, w0=0.009634436274299886, w1=-0.25473077727949417\n",
      "(250000,)\n",
      "Gradient Descent(1806/2999): loss=0.3899173818718742, w0=0.009634435598222515, w1=-0.25473077562779317\n",
      "(250000,)\n",
      "Gradient Descent(1807/2999): loss=0.3899173818708536, w0=0.009634434924072465, w1=-0.2547307739808009\n",
      "(250000,)\n",
      "Gradient Descent(1808/2999): loss=0.38991738186983904, w0=0.009634434251844253, w1=-0.254730772338504\n",
      "(250000,)\n",
      "Gradient Descent(1809/2999): loss=0.3899173818688302, w0=0.00963443358153241, w1=-0.25473077070088906\n",
      "(250000,)\n",
      "Gradient Descent(1810/2999): loss=0.389917381867827, w0=0.009634432913131447, w1=-0.25473076906794273\n",
      "(250000,)\n",
      "Gradient Descent(1811/2999): loss=0.38991738186682967, w0=0.009634432246635934, w1=-0.25473076743965173\n",
      "(250000,)\n",
      "Gradient Descent(1812/2999): loss=0.38991738186583785, w0=0.009634431582040439, w1=-0.25473076581600274\n",
      "(250000,)\n",
      "Gradient Descent(1813/2999): loss=0.38991738186485186, w0=0.009634430919339541, w1=-0.25473076419698254\n",
      "(250000,)\n",
      "Gradient Descent(1814/2999): loss=0.38991738186387126, w0=0.009634430258527848, w1=-0.25473076258257793\n",
      "(250000,)\n",
      "Gradient Descent(1815/2999): loss=0.3899173818628964, w0=0.009634429599599969, w1=-0.25473076097277575\n",
      "(250000,)\n",
      "Gradient Descent(1816/2999): loss=0.38991738186192704, w0=0.009634428942550535, w1=-0.2547307593675629\n",
      "(250000,)\n",
      "Gradient Descent(1817/2999): loss=0.3899173818609633, w0=0.009634428287374184, w1=-0.2547307577669263\n",
      "(250000,)\n",
      "Gradient Descent(1818/2999): loss=0.3899173818600049, w0=0.009634427634065582, w1=-0.25473075617085283\n",
      "(250000,)\n",
      "Gradient Descent(1819/2999): loss=0.389917381859052, w0=0.009634426982619413, w1=-0.2547307545793296\n",
      "(250000,)\n",
      "Gradient Descent(1820/2999): loss=0.38991738185810454, w0=0.009634426333030362, w1=-0.25473075299234355\n",
      "(250000,)\n",
      "Gradient Descent(1821/2999): loss=0.3899173818571625, w0=0.009634425685293135, w1=-0.2547307514098817\n",
      "(250000,)\n",
      "Gradient Descent(1822/2999): loss=0.3899173818562258, w0=0.009634425039402454, w1=-0.2547307498319313\n",
      "(250000,)\n",
      "Gradient Descent(1823/2999): loss=0.3899173818552944, w0=0.009634424395353053, w1=-0.25473074825847936\n",
      "(250000,)\n",
      "Gradient Descent(1824/2999): loss=0.3899173818543683, w0=0.009634423753139687, w1=-0.2547307466895131\n",
      "(250000,)\n",
      "Gradient Descent(1825/2999): loss=0.3899173818534475, w0=0.009634423112757112, w1=-0.25473074512501975\n",
      "(250000,)\n",
      "Gradient Descent(1826/2999): loss=0.389917381852532, w0=0.009634422474200115, w1=-0.25473074356498654\n",
      "(250000,)\n",
      "Gradient Descent(1827/2999): loss=0.38991738185162167, w0=0.0096344218374635, w1=-0.25473074200940077\n",
      "(250000,)\n",
      "Gradient Descent(1828/2999): loss=0.38991738185071656, w0=0.009634421202542072, w1=-0.2547307404582497\n",
      "(250000,)\n",
      "Gradient Descent(1829/2999): loss=0.3899173818498165, w0=0.009634420569430655, w1=-0.2547307389115208\n",
      "(250000,)\n",
      "Gradient Descent(1830/2999): loss=0.3899173818489216, w0=0.009634419938124085, w1=-0.2547307373692014\n",
      "(250000,)\n",
      "Gradient Descent(1831/2999): loss=0.3899173818480319, w0=0.009634419308617227, w1=-0.25473073583127886\n",
      "(250000,)\n",
      "Gradient Descent(1832/2999): loss=0.3899173818471472, w0=0.00963441868090494, w1=-0.25473073429774074\n",
      "(250000,)\n",
      "Gradient Descent(1833/2999): loss=0.3899173818462674, w0=0.009634418054982115, w1=-0.2547307327685745\n",
      "(250000,)\n",
      "Gradient Descent(1834/2999): loss=0.3899173818453928, w0=0.009634417430843655, w1=-0.25473073124376766\n",
      "(250000,)\n",
      "Gradient Descent(1835/2999): loss=0.38991738184452307, w0=0.009634416808484467, w1=-0.25473072972330785\n",
      "(250000,)\n",
      "Gradient Descent(1836/2999): loss=0.38991738184365826, w0=0.009634416187899487, w1=-0.2547307282071826\n",
      "(250000,)\n",
      "Gradient Descent(1837/2999): loss=0.38991738184279856, w0=0.009634415569083649, w1=-0.25473072669537966\n",
      "(250000,)\n",
      "Gradient Descent(1838/2999): loss=0.3899173818419436, w0=0.009634414952031913, w1=-0.2547307251878866\n",
      "(250000,)\n",
      "Gradient Descent(1839/2999): loss=0.38991738184109354, w0=0.009634414336739247, w1=-0.2547307236846912\n",
      "(250000,)\n",
      "Gradient Descent(1840/2999): loss=0.3899173818402484, w0=0.00963441372320064, w1=-0.25473072218578113\n",
      "(250000,)\n",
      "Gradient Descent(1841/2999): loss=0.38991738183940794, w0=0.009634413111411092, w1=-0.2547307206911443\n",
      "(250000,)\n",
      "Gradient Descent(1842/2999): loss=0.38991738183857233, w0=0.00963441250136562, w1=-0.2547307192007684\n",
      "(250000,)\n",
      "Gradient Descent(1843/2999): loss=0.38991738183774144, w0=0.009634411893059244, w1=-0.25473071771464134\n",
      "(250000,)\n",
      "Gradient Descent(1844/2999): loss=0.3899173818369153, w0=0.00963441128648702, w1=-0.254730716232751\n",
      "(250000,)\n",
      "Gradient Descent(1845/2999): loss=0.38991738183609387, w0=0.009634410681643987, w1=-0.2547307147550853\n",
      "(250000,)\n",
      "Gradient Descent(1846/2999): loss=0.38991738183527724, w0=0.009634410078525222, w1=-0.2547307132816322\n",
      "(250000,)\n",
      "Gradient Descent(1847/2999): loss=0.3899173818344651, w0=0.009634409477125825, w1=-0.2547307118123797\n",
      "(250000,)\n",
      "Gradient Descent(1848/2999): loss=0.38991738183365754, w0=0.009634408877440877, w1=-0.2547307103473158\n",
      "(250000,)\n",
      "Gradient Descent(1849/2999): loss=0.3899173818328548, w0=0.009634408279465496, w1=-0.25473070888642857\n",
      "(250000,)\n",
      "Gradient Descent(1850/2999): loss=0.38991738183205643, w0=0.009634407683194806, w1=-0.2547307074297061\n",
      "(250000,)\n",
      "Gradient Descent(1851/2999): loss=0.38991738183126273, w0=0.00963440708862395, w1=-0.25473070597713654\n",
      "(250000,)\n",
      "Gradient Descent(1852/2999): loss=0.3899173818304734, w0=0.009634406495748085, w1=-0.254730704528708\n",
      "(250000,)\n",
      "Gradient Descent(1853/2999): loss=0.38991738182968877, w0=0.009634405904562366, w1=-0.2547307030844087\n",
      "(250000,)\n",
      "Gradient Descent(1854/2999): loss=0.3899173818289085, w0=0.009634405315061996, w1=-0.2547307016442269\n",
      "(250000,)\n",
      "Gradient Descent(1855/2999): loss=0.38991738182813257, w0=0.009634404727242162, w1=-0.2547307002081508\n",
      "(250000,)\n",
      "Gradient Descent(1856/2999): loss=0.3899173818273612, w0=0.00963440414109807, w1=-0.2547306987761688\n",
      "(250000,)\n",
      "Gradient Descent(1857/2999): loss=0.38991738182659424, w0=0.009634403556624942, w1=-0.2547306973482691\n",
      "(250000,)\n",
      "Gradient Descent(1858/2999): loss=0.38991738182583147, w0=0.009634402973818016, w1=-0.2547306959244401\n",
      "(250000,)\n",
      "Gradient Descent(1859/2999): loss=0.389917381825073, w0=0.00963440239267254, w1=-0.25473069450467023\n",
      "(250000,)\n",
      "Gradient Descent(1860/2999): loss=0.38991738182431906, w0=0.009634401813183778, w1=-0.2547306930889479\n",
      "(250000,)\n",
      "Gradient Descent(1861/2999): loss=0.38991738182356933, w0=0.009634401235347012, w1=-0.2547306916772616\n",
      "(250000,)\n",
      "Gradient Descent(1862/2999): loss=0.389917381822824, w0=0.009634400659157534, w1=-0.25473069026959977\n",
      "(250000,)\n",
      "Gradient Descent(1863/2999): loss=0.3899173818220827, w0=0.009634400084610644, w1=-0.25473068886595096\n",
      "(250000,)\n",
      "Gradient Descent(1864/2999): loss=0.3899173818213459, w0=0.009634399511701664, w1=-0.25473068746630373\n",
      "(250000,)\n",
      "Gradient Descent(1865/2999): loss=0.38991738182061303, w0=0.00963439894042593, w1=-0.25473068607064664\n",
      "(250000,)\n",
      "Gradient Descent(1866/2999): loss=0.3899173818198844, w0=0.009634398370778776, w1=-0.25473068467896837\n",
      "(250000,)\n",
      "Gradient Descent(1867/2999): loss=0.3899173818191599, w0=0.009634397802755554, w1=-0.25473068329125753\n",
      "(250000,)\n",
      "Gradient Descent(1868/2999): loss=0.3899173818184396, w0=0.00963439723635165, w1=-0.25473068190750286\n",
      "(250000,)\n",
      "Gradient Descent(1869/2999): loss=0.38991738181772345, w0=0.009634396671562438, w1=-0.25473068052769304\n",
      "(250000,)\n",
      "Gradient Descent(1870/2999): loss=0.3899173818170113, w0=0.009634396108383316, w1=-0.25473067915181685\n",
      "(250000,)\n",
      "Gradient Descent(1871/2999): loss=0.3899173818163032, w0=0.0096343955468097, w1=-0.2547306777798631\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1872/2999): loss=0.38991738181559904, w0=0.009634394986837012, w1=-0.2547306764118205\n",
      "(250000,)\n",
      "Gradient Descent(1873/2999): loss=0.389917381814899, w0=0.009634394428460683, w1=-0.254730675047678\n",
      "(250000,)\n",
      "Gradient Descent(1874/2999): loss=0.389917381814203, w0=0.00963439387167616, w1=-0.2547306736874244\n",
      "(250000,)\n",
      "Gradient Descent(1875/2999): loss=0.3899173818135109, w0=0.009634393316478916, w1=-0.25473067233104874\n",
      "(250000,)\n",
      "Gradient Descent(1876/2999): loss=0.3899173818128227, w0=0.00963439276286442, w1=-0.25473067097853985\n",
      "(250000,)\n",
      "Gradient Descent(1877/2999): loss=0.3899173818121384, w0=0.00963439221082816, w1=-0.2547306696298868\n",
      "(250000,)\n",
      "Gradient Descent(1878/2999): loss=0.389917381811458, w0=0.009634391660365633, w1=-0.25473066828507845\n",
      "(250000,)\n",
      "Gradient Descent(1879/2999): loss=0.38991738181078167, w0=0.009634391111472346, w1=-0.25473066694410396\n",
      "(250000,)\n",
      "Gradient Descent(1880/2999): loss=0.389917381810109, w0=0.009634390564143837, w1=-0.2547306656069524\n",
      "(250000,)\n",
      "Gradient Descent(1881/2999): loss=0.3899173818094402, w0=0.009634390018375653, w1=-0.2547306642736128\n",
      "(250000,)\n",
      "Gradient Descent(1882/2999): loss=0.38991738180877517, w0=0.009634389474163339, w1=-0.25473066294407437\n",
      "(250000,)\n",
      "Gradient Descent(1883/2999): loss=0.38991738180811397, w0=0.00963438893150246, w1=-0.2547306616183262\n",
      "(250000,)\n",
      "Gradient Descent(1884/2999): loss=0.3899173818074565, w0=0.009634388390388597, w1=-0.25473066029635755\n",
      "(250000,)\n",
      "Gradient Descent(1885/2999): loss=0.3899173818068028, w0=0.009634387850817322, w1=-0.2547306589781576\n",
      "(250000,)\n",
      "Gradient Descent(1886/2999): loss=0.3899173818061529, w0=0.009634387312784258, w1=-0.2547306576637156\n",
      "(250000,)\n",
      "Gradient Descent(1887/2999): loss=0.3899173818055065, w0=0.009634386776285008, w1=-0.2547306563530209\n",
      "(250000,)\n",
      "Gradient Descent(1888/2999): loss=0.38991738180486385, w0=0.009634386241315199, w1=-0.25473065504606274\n",
      "(250000,)\n",
      "Gradient Descent(1889/2999): loss=0.38991738180422497, w0=0.009634385707870476, w1=-0.2547306537428305\n",
      "(250000,)\n",
      "Gradient Descent(1890/2999): loss=0.38991738180358965, w0=0.009634385175946496, w1=-0.25473065244331355\n",
      "(250000,)\n",
      "Gradient Descent(1891/2999): loss=0.38991738180295793, w0=0.009634384645538922, w1=-0.2547306511475014\n",
      "(250000,)\n",
      "Gradient Descent(1892/2999): loss=0.38991738180232993, w0=0.009634384116643429, w1=-0.2547306498553833\n",
      "(250000,)\n",
      "Gradient Descent(1893/2999): loss=0.3899173818017053, w0=0.009634383589255712, w1=-0.2547306485669488\n",
      "(250000,)\n",
      "Gradient Descent(1894/2999): loss=0.3899173818010845, w0=0.009634383063371462, w1=-0.2547306472821875\n",
      "(250000,)\n",
      "Gradient Descent(1895/2999): loss=0.389917381800467, w0=0.009634382538986394, w1=-0.2547306460010888\n",
      "(250000,)\n",
      "Gradient Descent(1896/2999): loss=0.38991738179985314, w0=0.00963438201609624, w1=-0.2547306447236423\n",
      "(250000,)\n",
      "Gradient Descent(1897/2999): loss=0.38991738179924273, w0=0.009634381494696736, w1=-0.2547306434498376\n",
      "(250000,)\n",
      "Gradient Descent(1898/2999): loss=0.3899173817986357, w0=0.009634380974783632, w1=-0.2547306421796643\n",
      "(250000,)\n",
      "Gradient Descent(1899/2999): loss=0.38991738179803226, w0=0.009634380456352697, w1=-0.25473064091311204\n",
      "(250000,)\n",
      "Gradient Descent(1900/2999): loss=0.3899173817974323, w0=0.009634379939399702, w1=-0.2547306396501705\n",
      "(250000,)\n",
      "Gradient Descent(1901/2999): loss=0.3899173817968356, w0=0.009634379423920432, w1=-0.25473063839082943\n",
      "(250000,)\n",
      "Gradient Descent(1902/2999): loss=0.38991738179624236, w0=0.009634378909910682, w1=-0.2547306371350785\n",
      "(250000,)\n",
      "Gradient Descent(1903/2999): loss=0.38991738179565255, w0=0.009634378397366262, w1=-0.2547306358829075\n",
      "(250000,)\n",
      "Gradient Descent(1904/2999): loss=0.389917381795066, w0=0.009634377886283, w1=-0.25473063463430623\n",
      "(250000,)\n",
      "Gradient Descent(1905/2999): loss=0.3899173817944829, w0=0.009634377376656731, w1=-0.25473063338926455\n",
      "(250000,)\n",
      "Gradient Descent(1906/2999): loss=0.389917381793903, w0=0.009634376868483297, w1=-0.25473063214777225\n",
      "(250000,)\n",
      "Gradient Descent(1907/2999): loss=0.3899173817933265, w0=0.009634376361758564, w1=-0.25473063090981923\n",
      "(250000,)\n",
      "Gradient Descent(1908/2999): loss=0.3899173817927532, w0=0.009634375856478403, w1=-0.2547306296753954\n",
      "(250000,)\n",
      "Gradient Descent(1909/2999): loss=0.3899173817921833, w0=0.009634375352638686, w1=-0.25473062844449074\n",
      "(250000,)\n",
      "Gradient Descent(1910/2999): loss=0.3899173817916165, w0=0.009634374850235314, w1=-0.25473062721709516\n",
      "(250000,)\n",
      "Gradient Descent(1911/2999): loss=0.38991738179105295, w0=0.009634374349264195, w1=-0.2547306259931987\n",
      "(250000,)\n",
      "Gradient Descent(1912/2999): loss=0.38991738179049273, w0=0.00963437384972124, w1=-0.2547306247727913\n",
      "(250000,)\n",
      "Gradient Descent(1913/2999): loss=0.38991738178993557, w0=0.009634373351602377, w1=-0.2547306235558631\n",
      "(250000,)\n",
      "Gradient Descent(1914/2999): loss=0.38991738178938157, w0=0.009634372854903548, w1=-0.2547306223424042\n",
      "(250000,)\n",
      "Gradient Descent(1915/2999): loss=0.3899173817888309, w0=0.009634372359620705, w1=-0.25473062113240463\n",
      "(250000,)\n",
      "Gradient Descent(1916/2999): loss=0.3899173817882832, w0=0.00963437186574982, w1=-0.25473061992585455\n",
      "(250000,)\n",
      "Gradient Descent(1917/2999): loss=0.38991738178773855, w0=0.009634371373286862, w1=-0.25473061872274416\n",
      "(250000,)\n",
      "Gradient Descent(1918/2999): loss=0.3899173817871973, w0=0.009634370882227808, w1=-0.25473061752306364\n",
      "(250000,)\n",
      "Gradient Descent(1919/2999): loss=0.3899173817866588, w0=0.009634370392568658, w1=-0.2547306163268032\n",
      "(250000,)\n",
      "Gradient Descent(1920/2999): loss=0.38991738178612356, w0=0.009634369904305431, w1=-0.2547306151339531\n",
      "(250000,)\n",
      "Gradient Descent(1921/2999): loss=0.38991738178559127, w0=0.009634369417434143, w1=-0.25473061394450364\n",
      "(250000,)\n",
      "Gradient Descent(1922/2999): loss=0.38991738178506213, w0=0.009634368931950832, w1=-0.25473061275844505\n",
      "(250000,)\n",
      "Gradient Descent(1923/2999): loss=0.38991738178453594, w0=0.009634368447851533, w1=-0.2547306115757677\n",
      "(250000,)\n",
      "Gradient Descent(1924/2999): loss=0.38991738178401264, w0=0.009634367965132304, w1=-0.254730610396462\n",
      "(250000,)\n",
      "Gradient Descent(1925/2999): loss=0.3899173817834924, w0=0.009634367483789208, w1=-0.25473060922051827\n",
      "(250000,)\n",
      "Gradient Descent(1926/2999): loss=0.3899173817829752, w0=0.009634367003818322, w1=-0.25473060804792697\n",
      "(250000,)\n",
      "Gradient Descent(1927/2999): loss=0.3899173817824608, w0=0.009634366525215732, w1=-0.2547306068786785\n",
      "(250000,)\n",
      "Gradient Descent(1928/2999): loss=0.3899173817819494, w0=0.009634366047977542, w1=-0.25473060571276335\n",
      "(250000,)\n",
      "Gradient Descent(1929/2999): loss=0.38991738178144103, w0=0.00963436557209986, w1=-0.25473060455017205\n",
      "(250000,)\n",
      "Gradient Descent(1930/2999): loss=0.38991738178093543, w0=0.009634365097578809, w1=-0.2547306033908951\n",
      "(250000,)\n",
      "Gradient Descent(1931/2999): loss=0.38991738178043267, w0=0.009634364624410523, w1=-0.25473060223492305\n",
      "(250000,)\n",
      "Gradient Descent(1932/2999): loss=0.38991738177993285, w0=0.009634364152591151, w1=-0.2547306010822465\n",
      "(250000,)\n",
      "Gradient Descent(1933/2999): loss=0.38991738177943575, w0=0.009634363682116849, w1=-0.254730599932856\n",
      "(250000,)\n",
      "Gradient Descent(1934/2999): loss=0.3899173817789417, w0=0.009634363212983764, w1=-0.25473059878674226\n",
      "(250000,)\n",
      "Gradient Descent(1935/2999): loss=0.38991738177845037, w0=0.009634362745188087, w1=-0.25473059764389583\n",
      "(250000,)\n",
      "Gradient Descent(1936/2999): loss=0.3899173817779618, w0=0.009634362278725994, w1=-0.25473059650430746\n",
      "(250000,)\n",
      "Gradient Descent(1937/2999): loss=0.389917381777476, w0=0.009634361813593692, w1=-0.2547305953679679\n",
      "(250000,)\n",
      "Gradient Descent(1938/2999): loss=0.38991738177699303, w0=0.009634361349787391, w1=-0.2547305942348678\n",
      "(250000,)\n",
      "Gradient Descent(1939/2999): loss=0.3899173817765127, w0=0.009634360887303305, w1=-0.254730593104998\n",
      "(250000,)\n",
      "Gradient Descent(1940/2999): loss=0.38991738177603524, w0=0.009634360426137674, w1=-0.2547305919783493\n",
      "(250000,)\n",
      "Gradient Descent(1941/2999): loss=0.38991738177556035, w0=0.009634359966286733, w1=-0.25473059085491245\n",
      "(250000,)\n",
      "Gradient Descent(1942/2999): loss=0.3899173817750883, w0=0.009634359507746737, w1=-0.2547305897346783\n",
      "(250000,)\n",
      "Gradient Descent(1943/2999): loss=0.3899173817746189, w0=0.009634359050513944, w1=-0.2547305886176378\n",
      "(250000,)\n",
      "Gradient Descent(1944/2999): loss=0.38991738177415214, w0=0.009634358594584632, w1=-0.25473058750378175\n",
      "(250000,)\n",
      "Gradient Descent(1945/2999): loss=0.389917381773688, w0=0.00963435813995508, w1=-0.2547305863931011\n",
      "(250000,)\n",
      "Gradient Descent(1946/2999): loss=0.3899173817732266, w0=0.009634357686621587, w1=-0.2547305852855868\n",
      "(250000,)\n",
      "Gradient Descent(1947/2999): loss=0.38991738177276786, w0=0.009634357234580457, w1=-0.25473058418122985\n",
      "(250000,)\n",
      "Gradient Descent(1948/2999): loss=0.3899173817723116, w0=0.009634356783828006, w1=-0.2547305830800212\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1949/2999): loss=0.3899173817718581, w0=0.009634356334360564, w1=-0.254730581981952\n",
      "(250000,)\n",
      "Gradient Descent(1950/2999): loss=0.38991738177140706, w0=0.009634355886174463, w1=-0.2547305808870131\n",
      "(250000,)\n",
      "Gradient Descent(1951/2999): loss=0.3899173817709585, w0=0.00963435543926605, w1=-0.25473057979519576\n",
      "(250000,)\n",
      "Gradient Descent(1952/2999): loss=0.3899173817705126, w0=0.009634354993631687, w1=-0.25473057870649096\n",
      "(250000,)\n",
      "Gradient Descent(1953/2999): loss=0.3899173817700693, w0=0.009634354549267733, w1=-0.25473057762088985\n",
      "(250000,)\n",
      "Gradient Descent(1954/2999): loss=0.3899173817696284, w0=0.009634354106170572, w1=-0.2547305765383836\n",
      "(250000,)\n",
      "Gradient Descent(1955/2999): loss=0.38991738176919, w0=0.0096343536643366, w1=-0.2547305754589634\n",
      "(250000,)\n",
      "Gradient Descent(1956/2999): loss=0.38991738176875423, w0=0.009634353223762207, w1=-0.2547305743826204\n",
      "(250000,)\n",
      "Gradient Descent(1957/2999): loss=0.3899173817683209, w0=0.0096343527844438, w1=-0.25473057330934595\n",
      "(250000,)\n",
      "Gradient Descent(1958/2999): loss=0.38991738176789004, w0=0.009634352346377797, w1=-0.25473057223913115\n",
      "(250000,)\n",
      "Gradient Descent(1959/2999): loss=0.38991738176746155, w0=0.009634351909560638, w1=-0.25473057117196735\n",
      "(250000,)\n",
      "Gradient Descent(1960/2999): loss=0.3899173817670356, w0=0.00963435147398876, w1=-0.2547305701078459\n",
      "(250000,)\n",
      "Gradient Descent(1961/2999): loss=0.38991738176661206, w0=0.009634351039658618, w1=-0.254730569046758\n",
      "(250000,)\n",
      "Gradient Descent(1962/2999): loss=0.3899173817661908, w0=0.009634350606566656, w1=-0.25473056798869514\n",
      "(250000,)\n",
      "Gradient Descent(1963/2999): loss=0.3899173817657721, w0=0.009634350174709363, w1=-0.2547305669336486\n",
      "(250000,)\n",
      "Gradient Descent(1964/2999): loss=0.3899173817653557, w0=0.00963434974408321, w1=-0.2547305658816098\n",
      "(250000,)\n",
      "Gradient Descent(1965/2999): loss=0.38991738176494173, w0=0.009634349314684688, w1=-0.2547305648325702\n",
      "(250000,)\n",
      "Gradient Descent(1966/2999): loss=0.38991738176453017, w0=0.0096343488865103, w1=-0.25473056378652126\n",
      "(250000,)\n",
      "Gradient Descent(1967/2999): loss=0.38991738176412083, w0=0.009634348459556557, w1=-0.25473056274345435\n",
      "(250000,)\n",
      "Gradient Descent(1968/2999): loss=0.3899173817637138, w0=0.009634348033819969, w1=-0.25473056170336106\n",
      "(250000,)\n",
      "Gradient Descent(1969/2999): loss=0.38991738176330915, w0=0.009634347609297069, w1=-0.2547305606662329\n",
      "(250000,)\n",
      "Gradient Descent(1970/2999): loss=0.3899173817629068, w0=0.009634347185984396, w1=-0.2547305596320614\n",
      "(250000,)\n",
      "Gradient Descent(1971/2999): loss=0.3899173817625067, w0=0.009634346763878513, w1=-0.25473055860083815\n",
      "(250000,)\n",
      "Gradient Descent(1972/2999): loss=0.389917381762109, w0=0.009634346342975966, w1=-0.25473055757255475\n",
      "(250000,)\n",
      "Gradient Descent(1973/2999): loss=0.3899173817617134, w0=0.009634345923273334, w1=-0.2547305565472028\n",
      "(250000,)\n",
      "Gradient Descent(1974/2999): loss=0.38991738176132024, w0=0.009634345504767189, w1=-0.2547305555247739\n",
      "(250000,)\n",
      "Gradient Descent(1975/2999): loss=0.3899173817609292, w0=0.009634345087454125, w1=-0.25473055450525983\n",
      "(250000,)\n",
      "Gradient Descent(1976/2999): loss=0.38991738176054036, w0=0.009634344671330734, w1=-0.2547305534886522\n",
      "(250000,)\n",
      "Gradient Descent(1977/2999): loss=0.3899173817601537, w0=0.009634344256393637, w1=-0.25473055247494275\n",
      "(250000,)\n",
      "Gradient Descent(1978/2999): loss=0.3899173817597693, w0=0.009634343842639444, w1=-0.2547305514641232\n",
      "(250000,)\n",
      "Gradient Descent(1979/2999): loss=0.38991738175938717, w0=0.009634343430064776, w1=-0.2547305504561853\n",
      "(250000,)\n",
      "Gradient Descent(1980/2999): loss=0.3899173817590071, w0=0.009634343018666275, w1=-0.2547305494511209\n",
      "(250000,)\n",
      "Gradient Descent(1981/2999): loss=0.38991738175862933, w0=0.009634342608440597, w1=-0.25473054844892173\n",
      "(250000,)\n",
      "Gradient Descent(1982/2999): loss=0.3899173817582535, w0=0.009634342199384393, w1=-0.2547305474495796\n",
      "(250000,)\n",
      "Gradient Descent(1983/2999): loss=0.38991738175788, w0=0.009634341791494326, w1=-0.25473054645308646\n",
      "(250000,)\n",
      "Gradient Descent(1984/2999): loss=0.38991738175750856, w0=0.00963434138476707, w1=-0.25473054545943413\n",
      "(250000,)\n",
      "Gradient Descent(1985/2999): loss=0.38991738175713925, w0=0.009634340979199319, w1=-0.25473054446861454\n",
      "(250000,)\n",
      "Gradient Descent(1986/2999): loss=0.389917381756772, w0=0.00963434057478777, w1=-0.2547305434806196\n",
      "(250000,)\n",
      "Gradient Descent(1987/2999): loss=0.38991738175640683, w0=0.00963434017152911, w1=-0.25473054249544125\n",
      "(250000,)\n",
      "Gradient Descent(1988/2999): loss=0.38991738175604385, w0=0.009634339769420066, w1=-0.2547305415130715\n",
      "(250000,)\n",
      "Gradient Descent(1989/2999): loss=0.38991738175568286, w0=0.009634339368457359, w1=-0.2547305405335023\n",
      "(250000,)\n",
      "Gradient Descent(1990/2999): loss=0.38991738175532387, w0=0.00963433896863772, w1=-0.2547305395567257\n",
      "(250000,)\n",
      "Gradient Descent(1991/2999): loss=0.38991738175496715, w0=0.00963433856995789, w1=-0.2547305385827337\n",
      "(250000,)\n",
      "Gradient Descent(1992/2999): loss=0.38991738175461216, w0=0.009634338172414618, w1=-0.25473053761151837\n",
      "(250000,)\n",
      "Gradient Descent(1993/2999): loss=0.38991738175425933, w0=0.009634337776004667, w1=-0.25473053664307177\n",
      "(250000,)\n",
      "Gradient Descent(1994/2999): loss=0.3899173817539085, w0=0.009634337380724809, w1=-0.25473053567738607\n",
      "(250000,)\n",
      "Gradient Descent(1995/2999): loss=0.38991738175355967, w0=0.009634336986571816, w1=-0.25473053471445334\n",
      "(250000,)\n",
      "Gradient Descent(1996/2999): loss=0.38991738175321283, w0=0.009634336593542476, w1=-0.2547305337542658\n",
      "(250000,)\n",
      "Gradient Descent(1997/2999): loss=0.389917381752868, w0=0.009634336201633587, w1=-0.25473053279681557\n",
      "(250000,)\n",
      "Gradient Descent(1998/2999): loss=0.38991738175252505, w0=0.009634335810841952, w1=-0.2547305318420949\n",
      "(250000,)\n",
      "Gradient Descent(1999/2999): loss=0.3899173817521841, w0=0.009634335421164392, w1=-0.2547305308900959\n",
      "(250000,)\n",
      "Gradient Descent(2000/2999): loss=0.3899173817518451, w0=0.009634335032597726, w1=-0.2547305299408109\n",
      "(250000,)\n",
      "Gradient Descent(2001/2999): loss=0.3899173817515081, w0=0.009634334645138779, w1=-0.25473052899423215\n",
      "(250000,)\n",
      "Gradient Descent(2002/2999): loss=0.3899173817511728, w0=0.009634334258784398, w1=-0.2547305280503519\n",
      "(250000,)\n",
      "Gradient Descent(2003/2999): loss=0.38991738175083956, w0=0.009634333873531437, w1=-0.2547305271091625\n",
      "(250000,)\n",
      "Gradient Descent(2004/2999): loss=0.3899173817505083, w0=0.009634333489376765, w1=-0.2547305261706563\n",
      "(250000,)\n",
      "Gradient Descent(2005/2999): loss=0.38991738175017876, w0=0.009634333106317245, w1=-0.25473052523482553\n",
      "(250000,)\n",
      "Gradient Descent(2006/2999): loss=0.38991738174985113, w0=0.009634332724349758, w1=-0.2547305243016627\n",
      "(250000,)\n",
      "Gradient Descent(2007/2999): loss=0.3899173817495254, w0=0.009634332343471191, w1=-0.25473052337116014\n",
      "(250000,)\n",
      "Gradient Descent(2008/2999): loss=0.3899173817492016, w0=0.009634331963678435, w1=-0.2547305224433103\n",
      "(250000,)\n",
      "Gradient Descent(2009/2999): loss=0.3899173817488795, w0=0.009634331584968393, w1=-0.25473052151810555\n",
      "(250000,)\n",
      "Gradient Descent(2010/2999): loss=0.38991738174855933, w0=0.009634331207337978, w1=-0.2547305205955384\n",
      "(250000,)\n",
      "Gradient Descent(2011/2999): loss=0.3899173817482409, w0=0.009634330830784117, w1=-0.25473051967560134\n",
      "(250000,)\n",
      "Gradient Descent(2012/2999): loss=0.38991738174792445, w0=0.00963433045530373, w1=-0.2547305187582869\n",
      "(250000,)\n",
      "Gradient Descent(2013/2999): loss=0.38991738174760965, w0=0.009634330080893773, w1=-0.2547305178435875\n",
      "(250000,)\n",
      "Gradient Descent(2014/2999): loss=0.3899173817472966, w0=0.009634329707551185, w1=-0.2547305169314958\n",
      "(250000,)\n",
      "Gradient Descent(2015/2999): loss=0.38991738174698554, w0=0.009634329335272925, w1=-0.25473051602200425\n",
      "(250000,)\n",
      "Gradient Descent(2016/2999): loss=0.38991738174667606, w0=0.009634328964055956, w1=-0.2547305151151055\n",
      "(250000,)\n",
      "Gradient Descent(2017/2999): loss=0.3899173817463684, w0=0.009634328593897266, w1=-0.2547305142107922\n",
      "(250000,)\n",
      "Gradient Descent(2018/2999): loss=0.38991738174606255, w0=0.009634328224793822, w1=-0.2547305133090569\n",
      "(250000,)\n",
      "Gradient Descent(2019/2999): loss=0.38991738174575835, w0=0.009634327856742628, w1=-0.2547305124098923\n",
      "(250000,)\n",
      "Gradient Descent(2020/2999): loss=0.3899173817454559, w0=0.009634327489740673, w1=-0.25473051151329107\n",
      "(250000,)\n",
      "Gradient Descent(2021/2999): loss=0.3899173817451552, w0=0.009634327123784973, w1=-0.2547305106192459\n",
      "(250000,)\n",
      "Gradient Descent(2022/2999): loss=0.3899173817448562, w0=0.00963432675887254, w1=-0.25473050972774947\n",
      "(250000,)\n",
      "Gradient Descent(2023/2999): loss=0.3899173817445591, w0=0.009634326395000406, w1=-0.25473050883879456\n",
      "(250000,)\n",
      "Gradient Descent(2024/2999): loss=0.3899173817442633, w0=0.009634326032165604, w1=-0.25473050795237384\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2025/2999): loss=0.3899173817439695, w0=0.009634325670365162, w1=-0.25473050706848016\n",
      "(250000,)\n",
      "Gradient Descent(2026/2999): loss=0.3899173817436772, w0=0.009634325309596146, w1=-0.2547305061871063\n",
      "(250000,)\n",
      "Gradient Descent(2027/2999): loss=0.3899173817433865, w0=0.009634324949855614, w1=-0.2547305053082451\n",
      "(250000,)\n",
      "Gradient Descent(2028/2999): loss=0.3899173817430977, w0=0.009634324591140634, w1=-0.2547305044318894\n",
      "(250000,)\n",
      "Gradient Descent(2029/2999): loss=0.3899173817428105, w0=0.009634324233448288, w1=-0.254730503558032\n",
      "(250000,)\n",
      "Gradient Descent(2030/2999): loss=0.3899173817425248, w0=0.009634323876775652, w1=-0.2547305026866658\n",
      "(250000,)\n",
      "Gradient Descent(2031/2999): loss=0.3899173817422407, w0=0.009634323521119823, w1=-0.2547305018177837\n",
      "(250000,)\n",
      "Gradient Descent(2032/2999): loss=0.38991738174195834, w0=0.009634323166477901, w1=-0.25473050095137867\n",
      "(250000,)\n",
      "Gradient Descent(2033/2999): loss=0.3899173817416776, w0=0.009634322812846992, w1=-0.2547305000874436\n",
      "(250000,)\n",
      "Gradient Descent(2034/2999): loss=0.3899173817413984, w0=0.009634322460224215, w1=-0.2547304992259714\n",
      "(250000,)\n",
      "Gradient Descent(2035/2999): loss=0.3899173817411207, w0=0.0096343221086067, w1=-0.25473049836695516\n",
      "(250000,)\n",
      "Gradient Descent(2036/2999): loss=0.3899173817408447, w0=0.009634321757991577, w1=-0.2547304975103878\n",
      "(250000,)\n",
      "Gradient Descent(2037/2999): loss=0.38991738174057033, w0=0.009634321408375994, w1=-0.25473049665626235\n",
      "(250000,)\n",
      "Gradient Descent(2038/2999): loss=0.38991738174029744, w0=0.009634321059757091, w1=-0.25473049580457185\n",
      "(250000,)\n",
      "Gradient Descent(2039/2999): loss=0.38991738174002605, w0=0.009634320712132041, w1=-0.2547304949553094\n",
      "(250000,)\n",
      "Gradient Descent(2040/2999): loss=0.3899173817397563, w0=0.009634320365498003, w1=-0.25473049410846804\n",
      "(250000,)\n",
      "Gradient Descent(2041/2999): loss=0.389917381739488, w0=0.009634320019852158, w1=-0.25473049326404085\n",
      "(250000,)\n",
      "Gradient Descent(2042/2999): loss=0.38991738173922136, w0=0.009634319675191683, w1=-0.254730492422021\n",
      "(250000,)\n",
      "Gradient Descent(2043/2999): loss=0.38991738173895607, w0=0.009634319331513765, w1=-0.2547304915824016\n",
      "(250000,)\n",
      "Gradient Descent(2044/2999): loss=0.3899173817386923, w0=0.009634318988815614, w1=-0.25473049074517573\n",
      "(250000,)\n",
      "Gradient Descent(2045/2999): loss=0.38991738173843016, w0=0.009634318647094424, w1=-0.2547304899103367\n",
      "(250000,)\n",
      "Gradient Descent(2046/2999): loss=0.3899173817381696, w0=0.00963431830634742, w1=-0.25473048907787765\n",
      "(250000,)\n",
      "Gradient Descent(2047/2999): loss=0.38991738173791024, w0=0.00963431796657183, w1=-0.25473048824779176\n",
      "(250000,)\n",
      "Gradient Descent(2048/2999): loss=0.3899173817376525, w0=0.009634317627764866, w1=-0.2547304874200723\n",
      "(250000,)\n",
      "Gradient Descent(2049/2999): loss=0.3899173817373962, w0=0.009634317289923778, w1=-0.25473048659471254\n",
      "(250000,)\n",
      "Gradient Descent(2050/2999): loss=0.3899173817371414, w0=0.009634316953045815, w1=-0.2547304857717057\n",
      "(250000,)\n",
      "Gradient Descent(2051/2999): loss=0.38991738173688806, w0=0.009634316617128224, w1=-0.2547304849510451\n",
      "(250000,)\n",
      "Gradient Descent(2052/2999): loss=0.38991738173663615, w0=0.00963431628216827, w1=-0.2547304841327241\n",
      "(250000,)\n",
      "Gradient Descent(2053/2999): loss=0.3899173817363857, w0=0.009634315948163216, w1=-0.254730483316736\n",
      "(250000,)\n",
      "Gradient Descent(2054/2999): loss=0.3899173817361366, w0=0.009634315615110354, w1=-0.25473048250307406\n",
      "(250000,)\n",
      "Gradient Descent(2055/2999): loss=0.38991738173588886, w0=0.009634315283006964, w1=-0.2547304816917318\n",
      "(250000,)\n",
      "Gradient Descent(2056/2999): loss=0.38991738173564267, w0=0.00963431495185034, w1=-0.2547304808827025\n",
      "(250000,)\n",
      "Gradient Descent(2057/2999): loss=0.38991738173539797, w0=0.009634314621637783, w1=-0.2547304800759796\n",
      "(250000,)\n",
      "Gradient Descent(2058/2999): loss=0.3899173817351544, w0=0.009634314292366592, w1=-0.2547304792715565\n",
      "(250000,)\n",
      "Gradient Descent(2059/2999): loss=0.38991738173491236, w0=0.009634313964034096, w1=-0.25473047846942665\n",
      "(250000,)\n",
      "Gradient Descent(2060/2999): loss=0.3899173817346716, w0=0.009634313636637611, w1=-0.25473047766958357\n",
      "(250000,)\n",
      "Gradient Descent(2061/2999): loss=0.3899173817344324, w0=0.009634313310174468, w1=-0.25473047687202066\n",
      "(250000,)\n",
      "Gradient Descent(2062/2999): loss=0.3899173817341944, w0=0.00963431298464201, w1=-0.2547304760767315\n",
      "(250000,)\n",
      "Gradient Descent(2063/2999): loss=0.3899173817339578, w0=0.009634312660037588, w1=-0.2547304752837095\n",
      "(250000,)\n",
      "Gradient Descent(2064/2999): loss=0.3899173817337225, w0=0.009634312336358548, w1=-0.25473047449294833\n",
      "(250000,)\n",
      "Gradient Descent(2065/2999): loss=0.3899173817334886, w0=0.009634312013602262, w1=-0.25473047370444146\n",
      "(250000,)\n",
      "Gradient Descent(2066/2999): loss=0.38991738173325613, w0=0.009634311691766092, w1=-0.25473047291818246\n",
      "(250000,)\n",
      "Gradient Descent(2067/2999): loss=0.3899173817330248, w0=0.009634311370847411, w1=-0.25473047213416494\n",
      "(250000,)\n",
      "Gradient Descent(2068/2999): loss=0.3899173817327949, w0=0.009634311050843612, w1=-0.2547304713523825\n",
      "(250000,)\n",
      "Gradient Descent(2069/2999): loss=0.38991738173256635, w0=0.009634310731752083, w1=-0.2547304705728288\n",
      "(250000,)\n",
      "Gradient Descent(2070/2999): loss=0.3899173817323389, w0=0.00963431041357022, w1=-0.25473046979549746\n",
      "(250000,)\n",
      "Gradient Descent(2071/2999): loss=0.38991738173211293, w0=0.009634310096295437, w1=-0.25473046902038216\n",
      "(250000,)\n",
      "Gradient Descent(2072/2999): loss=0.38991738173188834, w0=0.009634309779925144, w1=-0.2547304682474766\n",
      "(250000,)\n",
      "Gradient Descent(2073/2999): loss=0.38991738173166485, w0=0.009634309464456767, w1=-0.2547304674767744\n",
      "(250000,)\n",
      "Gradient Descent(2074/2999): loss=0.38991738173144264, w0=0.009634309149887726, w1=-0.2547304667082694\n",
      "(250000,)\n",
      "Gradient Descent(2075/2999): loss=0.3899173817312217, w0=0.009634308836215463, w1=-0.2547304659419552\n",
      "(250000,)\n",
      "Gradient Descent(2076/2999): loss=0.389917381731002, w0=0.009634308523437425, w1=-0.25473046517782566\n",
      "(250000,)\n",
      "Gradient Descent(2077/2999): loss=0.3899173817307836, w0=0.009634308211551056, w1=-0.2547304644158745\n",
      "(250000,)\n",
      "Gradient Descent(2078/2999): loss=0.38991738173056645, w0=0.009634307900553815, w1=-0.25473046365609553\n",
      "(250000,)\n",
      "Gradient Descent(2079/2999): loss=0.38991738173035045, w0=0.009634307590443167, w1=-0.25473046289848256\n",
      "(250000,)\n",
      "Gradient Descent(2080/2999): loss=0.3899173817301357, w0=0.009634307281216586, w1=-0.25473046214302936\n",
      "(250000,)\n",
      "Gradient Descent(2081/2999): loss=0.3899173817299223, w0=0.009634306972871553, w1=-0.2547304613897298\n",
      "(250000,)\n",
      "Gradient Descent(2082/2999): loss=0.38991738172970997, w0=0.009634306665405558, w1=-0.25473046063857785\n",
      "(250000,)\n",
      "Gradient Descent(2083/2999): loss=0.389917381729499, w0=0.009634306358816083, w1=-0.2547304598895673\n",
      "(250000,)\n",
      "Gradient Descent(2084/2999): loss=0.389917381729289, w0=0.009634306053100644, w1=-0.254730459142692\n",
      "(250000,)\n",
      "Gradient Descent(2085/2999): loss=0.3899173817290805, w0=0.009634305748256739, w1=-0.2547304583979459\n",
      "(250000,)\n",
      "Gradient Descent(2086/2999): loss=0.3899173817288729, w0=0.009634305444281881, w1=-0.25473045765532293\n",
      "(250000,)\n",
      "Gradient Descent(2087/2999): loss=0.3899173817286666, w0=0.009634305141173595, w1=-0.25473045691481705\n",
      "(250000,)\n",
      "Gradient Descent(2088/2999): loss=0.3899173817284616, w0=0.00963430483892942, w1=-0.2547304561764222\n",
      "(250000,)\n",
      "Gradient Descent(2089/2999): loss=0.3899173817282576, w0=0.009634304537546885, w1=-0.2547304554401324\n",
      "(250000,)\n",
      "Gradient Descent(2090/2999): loss=0.3899173817280548, w0=0.009634304237023546, w1=-0.25473045470594163\n",
      "(250000,)\n",
      "Gradient Descent(2091/2999): loss=0.3899173817278532, w0=0.009634303937356942, w1=-0.2547304539738439\n",
      "(250000,)\n",
      "Gradient Descent(2092/2999): loss=0.38991738172765267, w0=0.009634303638544629, w1=-0.25473045324383325\n",
      "(250000,)\n",
      "Gradient Descent(2093/2999): loss=0.3899173817274533, w0=0.009634303340584177, w1=-0.2547304525159037\n",
      "(250000,)\n",
      "Gradient Descent(2094/2999): loss=0.38991738172725515, w0=0.009634303043473149, w1=-0.2547304517900494\n",
      "(250000,)\n",
      "Gradient Descent(2095/2999): loss=0.3899173817270581, w0=0.009634302747209132, w1=-0.25473045106626435\n",
      "(250000,)\n",
      "Gradient Descent(2096/2999): loss=0.3899173817268621, w0=0.00963430245178971, w1=-0.25473045034454267\n",
      "(250000,)\n",
      "Gradient Descent(2097/2999): loss=0.3899173817266673, w0=0.009634302157212466, w1=-0.2547304496248785\n",
      "(250000,)\n",
      "Gradient Descent(2098/2999): loss=0.3899173817264735, w0=0.009634301863475005, w1=-0.254730448907266\n",
      "(250000,)\n",
      "Gradient Descent(2099/2999): loss=0.3899173817262809, w0=0.009634301570574932, w1=-0.2547304481916992\n",
      "(250000,)\n",
      "Gradient Descent(2100/2999): loss=0.38991738172608925, w0=0.009634301278509864, w1=-0.2547304474781724\n",
      "(250000,)\n",
      "Gradient Descent(2101/2999): loss=0.38991738172589885, w0=0.009634300987277427, w1=-0.2547304467666797\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2102/2999): loss=0.3899173817257095, w0=0.009634300696875235, w1=-0.2547304460572154\n",
      "(250000,)\n",
      "Gradient Descent(2103/2999): loss=0.38991738172552126, w0=0.009634300407300919, w1=-0.2547304453497736\n",
      "(250000,)\n",
      "Gradient Descent(2104/2999): loss=0.3899173817253341, w0=0.009634300118552123, w1=-0.25473044464434863\n",
      "(250000,)\n",
      "Gradient Descent(2105/2999): loss=0.389917381725148, w0=0.00963429983062649, w1=-0.25473044394093464\n",
      "(250000,)\n",
      "Gradient Descent(2106/2999): loss=0.38991738172496276, w0=0.009634299543521688, w1=-0.254730443239526\n",
      "(250000,)\n",
      "Gradient Descent(2107/2999): loss=0.3899173817247789, w0=0.009634299257235366, w1=-0.2547304425401169\n",
      "(250000,)\n",
      "Gradient Descent(2108/2999): loss=0.3899173817245958, w0=0.009634298971765192, w1=-0.2547304418427017\n",
      "(250000,)\n",
      "Gradient Descent(2109/2999): loss=0.3899173817244139, w0=0.009634298687108848, w1=-0.2547304411472747\n",
      "(250000,)\n",
      "Gradient Descent(2110/2999): loss=0.3899173817242329, w0=0.009634298403264003, w1=-0.2547304404538303\n",
      "(250000,)\n",
      "Gradient Descent(2111/2999): loss=0.38991738172405305, w0=0.00963429812022835, w1=-0.25473043976236276\n",
      "(250000,)\n",
      "Gradient Descent(2112/2999): loss=0.3899173817238742, w0=0.009634297837999576, w1=-0.25473043907286647\n",
      "(250000,)\n",
      "Gradient Descent(2113/2999): loss=0.38991738172369633, w0=0.009634297556575386, w1=-0.25473043838533577\n",
      "(250000,)\n",
      "Gradient Descent(2114/2999): loss=0.3899173817235196, w0=0.009634297275953482, w1=-0.2547304376997651\n",
      "(250000,)\n",
      "Gradient Descent(2115/2999): loss=0.3899173817233438, w0=0.009634296996131572, w1=-0.2547304370161489\n",
      "(250000,)\n",
      "Gradient Descent(2116/2999): loss=0.3899173817231689, w0=0.00963429671710738, w1=-0.25473043633448156\n",
      "(250000,)\n",
      "Gradient Descent(2117/2999): loss=0.3899173817229951, w0=0.009634296438878637, w1=-0.2547304356547575\n",
      "(250000,)\n",
      "Gradient Descent(2118/2999): loss=0.3899173817228223, w0=0.009634296161443075, w1=-0.25473043497697123\n",
      "(250000,)\n",
      "Gradient Descent(2119/2999): loss=0.38991738172265056, w0=0.009634295884798433, w1=-0.2547304343011172\n",
      "(250000,)\n",
      "Gradient Descent(2120/2999): loss=0.3899173817224795, w0=0.009634295608942453, w1=-0.2547304336271899\n",
      "(250000,)\n",
      "Gradient Descent(2121/2999): loss=0.3899173817223097, w0=0.009634295333872887, w1=-0.2547304329551838\n",
      "(250000,)\n",
      "Gradient Descent(2122/2999): loss=0.3899173817221408, w0=0.009634295059587485, w1=-0.25473043228509357\n",
      "(250000,)\n",
      "Gradient Descent(2123/2999): loss=0.38991738172197277, w0=0.009634294786084026, w1=-0.2547304316169136\n",
      "(250000,)\n",
      "Gradient Descent(2124/2999): loss=0.38991738172180584, w0=0.00963429451336028, w1=-0.25473043095063846\n",
      "(250000,)\n",
      "Gradient Descent(2125/2999): loss=0.38991738172163976, w0=0.00963429424141401, w1=-0.25473043028626274\n",
      "(250000,)\n",
      "Gradient Descent(2126/2999): loss=0.38991738172147455, w0=0.00963429397024301, w1=-0.25473042962378106\n",
      "(250000,)\n",
      "Gradient Descent(2127/2999): loss=0.38991738172131046, w0=0.00963429369984507, w1=-0.254730428963188\n",
      "(250000,)\n",
      "Gradient Descent(2128/2999): loss=0.3899173817211472, w0=0.009634293430217974, w1=-0.2547304283044782\n",
      "(250000,)\n",
      "Gradient Descent(2129/2999): loss=0.389917381720985, w0=0.009634293161359548, w1=-0.2547304276476462\n",
      "(250000,)\n",
      "Gradient Descent(2130/2999): loss=0.3899173817208236, w0=0.009634292893267588, w1=-0.2547304269926867\n",
      "(250000,)\n",
      "Gradient Descent(2131/2999): loss=0.38991738172066315, w0=0.009634292625939907, w1=-0.2547304263395944\n",
      "(250000,)\n",
      "Gradient Descent(2132/2999): loss=0.38991738172050355, w0=0.009634292359374328, w1=-0.25473042568836396\n",
      "(250000,)\n",
      "Gradient Descent(2133/2999): loss=0.38991738172034485, w0=0.009634292093568671, w1=-0.25473042503899007\n",
      "(250000,)\n",
      "Gradient Descent(2134/2999): loss=0.3899173817201872, w0=0.009634291828520781, w1=-0.2547304243914674\n",
      "(250000,)\n",
      "Gradient Descent(2135/2999): loss=0.3899173817200303, w0=0.009634291564228497, w1=-0.25473042374579075\n",
      "(250000,)\n",
      "Gradient Descent(2136/2999): loss=0.3899173817198744, w0=0.009634291300689664, w1=-0.25473042310195476\n",
      "(250000,)\n",
      "Gradient Descent(2137/2999): loss=0.38991738171971935, w0=0.009634291037902127, w1=-0.2547304224599542\n",
      "(250000,)\n",
      "Gradient Descent(2138/2999): loss=0.389917381719565, w0=0.009634290775863742, w1=-0.2547304218197839\n",
      "(250000,)\n",
      "Gradient Descent(2139/2999): loss=0.3899173817194117, w0=0.009634290514572376, w1=-0.25473042118143857\n",
      "(250000,)\n",
      "Gradient Descent(2140/2999): loss=0.38991738171925944, w0=0.009634290254025903, w1=-0.25473042054491307\n",
      "(250000,)\n",
      "Gradient Descent(2141/2999): loss=0.38991738171910784, w0=0.009634289994222204, w1=-0.2547304199102022\n",
      "(250000,)\n",
      "Gradient Descent(2142/2999): loss=0.3899173817189572, w0=0.009634289735159157, w1=-0.2547304192773007\n",
      "(250000,)\n",
      "Gradient Descent(2143/2999): loss=0.3899173817188073, w0=0.009634289476834647, w1=-0.2547304186462036\n",
      "(250000,)\n",
      "Gradient Descent(2144/2999): loss=0.38991738171865825, w0=0.009634289219246571, w1=-0.2547304180169056\n",
      "(250000,)\n",
      "Gradient Descent(2145/2999): loss=0.38991738171851026, w0=0.009634288962392835, w1=-0.2547304173894016\n",
      "(250000,)\n",
      "Gradient Descent(2146/2999): loss=0.3899173817183629, w0=0.00963428870627134, w1=-0.2547304167636865\n",
      "(250000,)\n",
      "Gradient Descent(2147/2999): loss=0.3899173817182164, w0=0.009634288450879996, w1=-0.2547304161397552\n",
      "(250000,)\n",
      "Gradient Descent(2148/2999): loss=0.3899173817180709, w0=0.009634288196216718, w1=-0.25473041551760267\n",
      "(250000,)\n",
      "Gradient Descent(2149/2999): loss=0.389917381717926, w0=0.00963428794227944, w1=-0.25473041489722376\n",
      "(250000,)\n",
      "Gradient Descent(2150/2999): loss=0.38991738171778206, w0=0.009634287689066098, w1=-0.2547304142786134\n",
      "(250000,)\n",
      "Gradient Descent(2151/2999): loss=0.3899173817176389, w0=0.00963428743657462, w1=-0.25473041366176663\n",
      "(250000,)\n",
      "Gradient Descent(2152/2999): loss=0.38991738171749657, w0=0.009634287184802949, w1=-0.2547304130466783\n",
      "(250000,)\n",
      "Gradient Descent(2153/2999): loss=0.38991738171735507, w0=0.009634286933749028, w1=-0.25473041243334354\n",
      "(250000,)\n",
      "Gradient Descent(2154/2999): loss=0.3899173817172143, w0=0.009634286683410825, w1=-0.25473041182175726\n",
      "(250000,)\n",
      "Gradient Descent(2155/2999): loss=0.38991738171707435, w0=0.009634286433786287, w1=-0.2547304112119145\n",
      "(250000,)\n",
      "Gradient Descent(2156/2999): loss=0.3899173817169353, w0=0.009634286184873374, w1=-0.25473041060381024\n",
      "(250000,)\n",
      "Gradient Descent(2157/2999): loss=0.38991738171679685, w0=0.00963428593667007, w1=-0.2547304099974396\n",
      "(250000,)\n",
      "Gradient Descent(2158/2999): loss=0.3899173817166594, w0=0.00963428568917435, w1=-0.2547304093927977\n",
      "(250000,)\n",
      "Gradient Descent(2159/2999): loss=0.3899173817165226, w0=0.009634285442384193, w1=-0.25473040878987946\n",
      "(250000,)\n",
      "Gradient Descent(2160/2999): loss=0.3899173817163866, w0=0.009634285196297585, w1=-0.25473040818868004\n",
      "(250000,)\n",
      "Gradient Descent(2161/2999): loss=0.38991738171625145, w0=0.009634284950912522, w1=-0.2547304075891945\n",
      "(250000,)\n",
      "Gradient Descent(2162/2999): loss=0.38991738171611706, w0=0.009634284706227007, w1=-0.254730406991418\n",
      "(250000,)\n",
      "Gradient Descent(2163/2999): loss=0.38991738171598334, w0=0.009634284462239046, w1=-0.25473040639534567\n",
      "(250000,)\n",
      "Gradient Descent(2164/2999): loss=0.38991738171585044, w0=0.009634284218946647, w1=-0.2547304058009726\n",
      "(250000,)\n",
      "Gradient Descent(2165/2999): loss=0.3899173817157182, w0=0.00963428397634783, w1=-0.254730405208294\n",
      "(250000,)\n",
      "Gradient Descent(2166/2999): loss=0.3899173817155869, w0=0.00963428373444062, w1=-0.254730404617305\n",
      "(250000,)\n",
      "Gradient Descent(2167/2999): loss=0.38991738171545626, w0=0.009634283493223046, w1=-0.2547304040280008\n",
      "(250000,)\n",
      "Gradient Descent(2168/2999): loss=0.3899173817153263, w0=0.009634283252693132, w1=-0.2547304034403766\n",
      "(250000,)\n",
      "Gradient Descent(2169/2999): loss=0.38991738171519724, w0=0.009634283012848927, w1=-0.25473040285442766\n",
      "(250000,)\n",
      "Gradient Descent(2170/2999): loss=0.3899173817150687, w0=0.009634282773688479, w1=-0.2547304022701491\n",
      "(250000,)\n",
      "Gradient Descent(2171/2999): loss=0.389917381714941, w0=0.009634282535209829, w1=-0.2547304016875362\n",
      "(250000,)\n",
      "Gradient Descent(2172/2999): loss=0.389917381714814, w0=0.009634282297411036, w1=-0.25473040110658424\n",
      "(250000,)\n",
      "Gradient Descent(2173/2999): loss=0.38991738171468776, w0=0.009634282060290166, w1=-0.25473040052728846\n",
      "(250000,)\n",
      "Gradient Descent(2174/2999): loss=0.3899173817145623, w0=0.009634281823845285, w1=-0.2547303999496442\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2175/2999): loss=0.38991738171443735, w0=0.009634281588074469, w1=-0.25473039937364667\n",
      "(250000,)\n",
      "Gradient Descent(2176/2999): loss=0.3899173817143134, w0=0.009634281352975787, w1=-0.2547303987992912\n",
      "(250000,)\n",
      "Gradient Descent(2177/2999): loss=0.38991738171419, w0=0.009634281118547318, w1=-0.2547303982265731\n",
      "(250000,)\n",
      "Gradient Descent(2178/2999): loss=0.38991738171406726, w0=0.009634280884787166, w1=-0.2547303976554877\n",
      "(250000,)\n",
      "Gradient Descent(2179/2999): loss=0.38991738171394524, w0=0.009634280651693423, w1=-0.2547303970860304\n",
      "(250000,)\n",
      "Gradient Descent(2180/2999): loss=0.389917381713824, w0=0.009634280419264193, w1=-0.2547303965181965\n",
      "(250000,)\n",
      "Gradient Descent(2181/2999): loss=0.3899173817137033, w0=0.009634280187497572, w1=-0.2547303959519814\n",
      "(250000,)\n",
      "Gradient Descent(2182/2999): loss=0.3899173817135835, w0=0.009634279956391677, w1=-0.25473039538738046\n",
      "(250000,)\n",
      "Gradient Descent(2183/2999): loss=0.38991738171346413, w0=0.009634279725944616, w1=-0.2547303948243891\n",
      "(250000,)\n",
      "Gradient Descent(2184/2999): loss=0.38991738171334556, w0=0.009634279496154516, w1=-0.2547303942630027\n",
      "(250000,)\n",
      "Gradient Descent(2185/2999): loss=0.3899173817132277, w0=0.009634279267019506, w1=-0.2547303937032167\n",
      "(250000,)\n",
      "Gradient Descent(2186/2999): loss=0.3899173817131105, w0=0.009634279038537712, w1=-0.2547303931450266\n",
      "(250000,)\n",
      "Gradient Descent(2187/2999): loss=0.38991738171299395, w0=0.00963427881070728, w1=-0.2547303925884278\n",
      "(250000,)\n",
      "Gradient Descent(2188/2999): loss=0.38991738171287804, w0=0.00963427858352635, w1=-0.2547303920334157\n",
      "(250000,)\n",
      "Gradient Descent(2189/2999): loss=0.3899173817127629, w0=0.009634278356993068, w1=-0.2547303914799859\n",
      "(250000,)\n",
      "Gradient Descent(2190/2999): loss=0.3899173817126482, w0=0.009634278131105595, w1=-0.25473039092813377\n",
      "(250000,)\n",
      "Gradient Descent(2191/2999): loss=0.38991738171253437, w0=0.009634277905862085, w1=-0.2547303903778549\n",
      "(250000,)\n",
      "Gradient Descent(2192/2999): loss=0.389917381712421, w0=0.009634277681260702, w1=-0.25473038982914475\n",
      "(250000,)\n",
      "Gradient Descent(2193/2999): loss=0.38991738171230844, w0=0.009634277457299617, w1=-0.25473038928199887\n",
      "(250000,)\n",
      "Gradient Descent(2194/2999): loss=0.3899173817121965, w0=0.009634277233977005, w1=-0.25473038873641285\n",
      "(250000,)\n",
      "Gradient Descent(2195/2999): loss=0.38991738171208506, w0=0.009634277011291041, w1=-0.2547303881923822\n",
      "(250000,)\n",
      "Gradient Descent(2196/2999): loss=0.3899173817119744, w0=0.009634276789239914, w1=-0.25473038764990247\n",
      "(250000,)\n",
      "Gradient Descent(2197/2999): loss=0.38991738171186424, w0=0.00963427656782181, w1=-0.2547303871089692\n",
      "(250000,)\n",
      "Gradient Descent(2198/2999): loss=0.38991738171175483, w0=0.009634276347034938, w1=-0.2547303865695781\n",
      "(250000,)\n",
      "Gradient Descent(2199/2999): loss=0.3899173817116459, w0=0.009634276126877492, w1=-0.25473038603172465\n",
      "(250000,)\n",
      "Gradient Descent(2200/2999): loss=0.3899173817115378, w0=0.00963427590734767, w1=-0.25473038549540455\n",
      "(250000,)\n",
      "Gradient Descent(2201/2999): loss=0.38991738171143014, w0=0.009634275688443689, w1=-0.2547303849606134\n",
      "(250000,)\n",
      "Gradient Descent(2202/2999): loss=0.3899173817113233, w0=0.009634275470163751, w1=-0.2547303844273468\n",
      "(250000,)\n",
      "Gradient Descent(2203/2999): loss=0.38991738171121687, w0=0.009634275252506094, w1=-0.2547303838956005\n",
      "(250000,)\n",
      "Gradient Descent(2204/2999): loss=0.389917381711111, w0=0.00963427503546894, w1=-0.25473038336537007\n",
      "(250000,)\n",
      "Gradient Descent(2205/2999): loss=0.3899173817110059, w0=0.00963427481905052, w1=-0.2547303828366512\n",
      "(250000,)\n",
      "Gradient Descent(2206/2999): loss=0.3899173817109013, w0=0.009634274603249067, w1=-0.2547303823094397\n",
      "(250000,)\n",
      "Gradient Descent(2207/2999): loss=0.38991738171079743, w0=0.00963427438806282, w1=-0.2547303817837311\n",
      "(250000,)\n",
      "Gradient Descent(2208/2999): loss=0.3899173817106939, w0=0.009634274173490028, w1=-0.25473038125952124\n",
      "(250000,)\n",
      "Gradient Descent(2209/2999): loss=0.38991738171059115, w0=0.009634273959528945, w1=-0.2547303807368058\n",
      "(250000,)\n",
      "Gradient Descent(2210/2999): loss=0.3899173817104889, w0=0.009634273746177825, w1=-0.25473038021558053\n",
      "(250000,)\n",
      "Gradient Descent(2211/2999): loss=0.38991738171038737, w0=0.009634273533434936, w1=-0.25473037969584117\n",
      "(250000,)\n",
      "Gradient Descent(2212/2999): loss=0.3899173817102862, w0=0.00963427332129853, w1=-0.2547303791775835\n",
      "(250000,)\n",
      "Gradient Descent(2213/2999): loss=0.3899173817101858, w0=0.009634273109766892, w1=-0.2547303786608033\n",
      "(250000,)\n",
      "Gradient Descent(2214/2999): loss=0.3899173817100859, w0=0.009634272898838286, w1=-0.2547303781454963\n",
      "(250000,)\n",
      "Gradient Descent(2215/2999): loss=0.3899173817099866, w0=0.009634272688511, w1=-0.2547303776316584\n",
      "(250000,)\n",
      "Gradient Descent(2216/2999): loss=0.38991738170988777, w0=0.00963427247878332, w1=-0.2547303771192853\n",
      "(250000,)\n",
      "Gradient Descent(2217/2999): loss=0.3899173817097897, w0=0.009634272269653528, w1=-0.25473037660837294\n",
      "(250000,)\n",
      "Gradient Descent(2218/2999): loss=0.3899173817096919, w0=0.009634272061119924, w1=-0.2547303760989171\n",
      "(250000,)\n",
      "Gradient Descent(2219/2999): loss=0.38991738170959495, w0=0.009634271853180814, w1=-0.25473037559091355\n",
      "(250000,)\n",
      "Gradient Descent(2220/2999): loss=0.38991738170949836, w0=0.0096342716458345, w1=-0.25473037508435825\n",
      "(250000,)\n",
      "Gradient Descent(2221/2999): loss=0.38991738170940227, w0=0.00963427143907929, w1=-0.2547303745792471\n",
      "(250000,)\n",
      "Gradient Descent(2222/2999): loss=0.3899173817093069, w0=0.009634271232913504, w1=-0.25473037407557586\n",
      "(250000,)\n",
      "Gradient Descent(2223/2999): loss=0.38991738170921203, w0=0.009634271027335462, w1=-0.25473037357334055\n",
      "(250000,)\n",
      "Gradient Descent(2224/2999): loss=0.38991738170911766, w0=0.009634270822343485, w1=-0.254730373072537\n",
      "(250000,)\n",
      "Gradient Descent(2225/2999): loss=0.3899173817090239, w0=0.0096342706179359, w1=-0.25473037257316117\n",
      "(250000,)\n",
      "Gradient Descent(2226/2999): loss=0.38991738170893053, w0=0.00963427041411104, w1=-0.25473037207520893\n",
      "(250000,)\n",
      "Gradient Descent(2227/2999): loss=0.3899173817088377, w0=0.00963427021086725, w1=-0.2547303715786763\n",
      "(250000,)\n",
      "Gradient Descent(2228/2999): loss=0.38991738170874546, w0=0.009634270008202864, w1=-0.25473037108355917\n",
      "(250000,)\n",
      "Gradient Descent(2229/2999): loss=0.38991738170865387, w0=0.009634269806116242, w1=-0.25473037058985354\n",
      "(250000,)\n",
      "Gradient Descent(2230/2999): loss=0.3899173817085626, w0=0.009634269604605741, w1=-0.25473037009755534\n",
      "(250000,)\n",
      "Gradient Descent(2231/2999): loss=0.38991738170847196, w0=0.009634269403669697, w1=-0.2547303696066606\n",
      "(250000,)\n",
      "Gradient Descent(2232/2999): loss=0.38991738170838186, w0=0.009634269203306494, w1=-0.25473036911716535\n",
      "(250000,)\n",
      "Gradient Descent(2233/2999): loss=0.38991738170829227, w0=0.009634269003514486, w1=-0.2547303686290655\n",
      "(250000,)\n",
      "Gradient Descent(2234/2999): loss=0.3899173817082031, w0=0.00963426880429205, w1=-0.25473036814235717\n",
      "(250000,)\n",
      "Gradient Descent(2235/2999): loss=0.3899173817081146, w0=0.009634268605637559, w1=-0.2547303676570364\n",
      "(250000,)\n",
      "Gradient Descent(2236/2999): loss=0.38991738170802637, w0=0.009634268407549398, w1=-0.2547303671730991\n",
      "(250000,)\n",
      "Gradient Descent(2237/2999): loss=0.38991738170793877, w0=0.009634268210025939, w1=-0.25473036669054144\n",
      "(250000,)\n",
      "Gradient Descent(2238/2999): loss=0.3899173817078516, w0=0.009634268013065588, w1=-0.2547303662093595\n",
      "(250000,)\n",
      "Gradient Descent(2239/2999): loss=0.389917381707765, w0=0.009634267816666734, w1=-0.2547303657295493\n",
      "(250000,)\n",
      "Gradient Descent(2240/2999): loss=0.38991738170767887, w0=0.009634267620827782, w1=-0.254730365251107\n",
      "(250000,)\n",
      "Gradient Descent(2241/2999): loss=0.38991738170759327, w0=0.009634267425547129, w1=-0.2547303647740286\n",
      "(250000,)\n",
      "Gradient Descent(2242/2999): loss=0.3899173817075082, w0=0.009634267230823185, w1=-0.25473036429831025\n",
      "(250000,)\n",
      "Gradient Descent(2243/2999): loss=0.3899173817074235, w0=0.009634267036654367, w1=-0.2547303638239481\n",
      "(250000,)\n",
      "Gradient Descent(2244/2999): loss=0.3899173817073394, w0=0.009634266843039084, w1=-0.2547303633509383\n",
      "(250000,)\n",
      "Gradient Descent(2245/2999): loss=0.3899173817072556, w0=0.009634266649975761, w1=-0.2547303628792769\n",
      "(250000,)\n",
      "Gradient Descent(2246/2999): loss=0.3899173817071725, w0=0.009634266457462825, w1=-0.2547303624089602\n",
      "(250000,)\n",
      "Gradient Descent(2247/2999): loss=0.3899173817070897, w0=0.009634266265498705, w1=-0.2547303619399842\n",
      "(250000,)\n",
      "Gradient Descent(2248/2999): loss=0.38991738170700735, w0=0.009634266074081837, w1=-0.25473036147234523\n",
      "(250000,)\n",
      "Gradient Descent(2249/2999): loss=0.3899173817069256, w0=0.009634265883210666, w1=-0.2547303610060394\n",
      "(250000,)\n",
      "Gradient Descent(2250/2999): loss=0.3899173817068443, w0=0.009634265692883632, w1=-0.2547303605410629\n",
      "(250000,)\n",
      "Gradient Descent(2251/2999): loss=0.3899173817067634, w0=0.00963426550309919, w1=-0.254730360077412\n",
      "(250000,)\n",
      "Gradient Descent(2252/2999): loss=0.38991738170668294, w0=0.009634265313855788, w1=-0.25473035961508284\n",
      "(250000,)\n",
      "Gradient Descent(2253/2999): loss=0.38991738170660295, w0=0.009634265125151885, w1=-0.2547303591540717\n",
      "(250000,)\n",
      "Gradient Descent(2254/2999): loss=0.3899173817065235, w0=0.009634264936985937, w1=-0.2547303586943748\n",
      "(250000,)\n",
      "Gradient Descent(2255/2999): loss=0.38991738170644447, w0=0.009634264749356417, w1=-0.2547303582359885\n",
      "(250000,)\n",
      "Gradient Descent(2256/2999): loss=0.3899173817063658, w0=0.009634264562261796, w1=-0.2547303577789089\n",
      "(250000,)\n",
      "Gradient Descent(2257/2999): loss=0.3899173817062877, w0=0.00963426437570054, w1=-0.2547303573231324\n",
      "(250000,)\n",
      "Gradient Descent(2258/2999): loss=0.38991738170620993, w0=0.009634264189671138, w1=-0.2547303568686552\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2259/2999): loss=0.3899173817061327, w0=0.009634264004172072, w1=-0.25473035641547365\n",
      "(250000,)\n",
      "Gradient Descent(2260/2999): loss=0.38991738170605594, w0=0.009634263819201834, w1=-0.254730355963584\n",
      "(250000,)\n",
      "Gradient Descent(2261/2999): loss=0.3899173817059795, w0=0.009634263634758913, w1=-0.2547303555129826\n",
      "(250000,)\n",
      "Gradient Descent(2262/2999): loss=0.38991738170590357, w0=0.00963426345084181, w1=-0.2547303550636658\n",
      "(250000,)\n",
      "Gradient Descent(2263/2999): loss=0.38991738170582807, w0=0.009634263267449018, w1=-0.25473035461562993\n",
      "(250000,)\n",
      "Gradient Descent(2264/2999): loss=0.3899173817057529, w0=0.009634263084579037, w1=-0.2547303541688713\n",
      "(250000,)\n",
      "Gradient Descent(2265/2999): loss=0.3899173817056783, w0=0.009634262902230382, w1=-0.2547303537233863\n",
      "(250000,)\n",
      "Gradient Descent(2266/2999): loss=0.3899173817056041, w0=0.00963426272040158, w1=-0.2547303532791713\n",
      "(250000,)\n",
      "Gradient Descent(2267/2999): loss=0.3899173817055302, w0=0.00963426253909113, w1=-0.2547303528362227\n",
      "(250000,)\n",
      "Gradient Descent(2268/2999): loss=0.3899173817054567, w0=0.009634262358297566, w1=-0.25473035239453684\n",
      "(250000,)\n",
      "Gradient Descent(2269/2999): loss=0.38991738170538376, w0=0.009634262178019415, w1=-0.25473035195411015\n",
      "(250000,)\n",
      "Gradient Descent(2270/2999): loss=0.3899173817053112, w0=0.009634261998255197, w1=-0.25473035151493906\n",
      "(250000,)\n",
      "Gradient Descent(2271/2999): loss=0.3899173817052391, w0=0.009634261819003453, w1=-0.25473035107701997\n",
      "(250000,)\n",
      "Gradient Descent(2272/2999): loss=0.3899173817051674, w0=0.009634261640262725, w1=-0.25473035064034927\n",
      "(250000,)\n",
      "Gradient Descent(2273/2999): loss=0.3899173817050961, w0=0.009634261462031554, w1=-0.25473035020492346\n",
      "(250000,)\n",
      "Gradient Descent(2274/2999): loss=0.3899173817050251, w0=0.009634261284308483, w1=-0.254730349770739\n",
      "(250000,)\n",
      "Gradient Descent(2275/2999): loss=0.3899173817049546, w0=0.009634261107092071, w1=-0.25473034933779226\n",
      "(250000,)\n",
      "Gradient Descent(2276/2999): loss=0.3899173817048845, w0=0.00963426093038087, w1=-0.2547303489060798\n",
      "(250000,)\n",
      "Gradient Descent(2277/2999): loss=0.38991738170481477, w0=0.00963426075417344, w1=-0.2547303484755981\n",
      "(250000,)\n",
      "Gradient Descent(2278/2999): loss=0.38991738170474544, w0=0.009634260578468347, w1=-0.2547303480463436\n",
      "(250000,)\n",
      "Gradient Descent(2279/2999): loss=0.38991738170467655, w0=0.00963426040326415, w1=-0.2547303476183128\n",
      "(250000,)\n",
      "Gradient Descent(2280/2999): loss=0.3899173817046079, w0=0.009634260228559434, w1=-0.2547303471915023\n",
      "(250000,)\n",
      "Gradient Descent(2281/2999): loss=0.38991738170453977, w0=0.009634260054352773, w1=-0.2547303467659085\n",
      "(250000,)\n",
      "Gradient Descent(2282/2999): loss=0.389917381704472, w0=0.009634259880642735, w1=-0.254730346341528\n",
      "(250000,)\n",
      "Gradient Descent(2283/2999): loss=0.3899173817044047, w0=0.00963425970742791, w1=-0.25473034591835736\n",
      "(250000,)\n",
      "Gradient Descent(2284/2999): loss=0.38991738170433776, w0=0.009634259534706896, w1=-0.25473034549639306\n",
      "(250000,)\n",
      "Gradient Descent(2285/2999): loss=0.38991738170427104, w0=0.009634259362478274, w1=-0.2547303450756317\n",
      "(250000,)\n",
      "Gradient Descent(2286/2999): loss=0.3899173817042048, w0=0.009634259190740636, w1=-0.25473034465606986\n",
      "(250000,)\n",
      "Gradient Descent(2287/2999): loss=0.389917381704139, w0=0.009634259019492598, w1=-0.25473034423770413\n",
      "(250000,)\n",
      "Gradient Descent(2288/2999): loss=0.38991738170407353, w0=0.00963425884873276, w1=-0.25473034382053106\n",
      "(250000,)\n",
      "Gradient Descent(2289/2999): loss=0.3899173817040084, w0=0.009634258678459733, w1=-0.2547303434045473\n",
      "(250000,)\n",
      "Gradient Descent(2290/2999): loss=0.38991738170394363, w0=0.00963425850867212, w1=-0.25473034298974945\n",
      "(250000,)\n",
      "Gradient Descent(2291/2999): loss=0.38991738170387924, w0=0.009634258339368542, w1=-0.2547303425761341\n",
      "(250000,)\n",
      "Gradient Descent(2292/2999): loss=0.38991738170381535, w0=0.009634258170547612, w1=-0.25473034216369783\n",
      "(250000,)\n",
      "Gradient Descent(2293/2999): loss=0.3899173817037516, w0=0.009634258002207968, w1=-0.2547303417524374\n",
      "(250000,)\n",
      "Gradient Descent(2294/2999): loss=0.38991738170368845, w0=0.009634257834348223, w1=-0.2547303413423494\n",
      "(250000,)\n",
      "Gradient Descent(2295/2999): loss=0.3899173817036255, w0=0.009634257666967018, w1=-0.25473034093343044\n",
      "(250000,)\n",
      "Gradient Descent(2296/2999): loss=0.38991738170356294, w0=0.009634257500062984, w1=-0.2547303405256773\n",
      "(250000,)\n",
      "Gradient Descent(2297/2999): loss=0.3899173817035007, w0=0.009634257333634762, w1=-0.2547303401190865\n",
      "(250000,)\n",
      "Gradient Descent(2298/2999): loss=0.3899173817034389, w0=0.009634257167680994, w1=-0.2547303397136549\n",
      "(250000,)\n",
      "Gradient Descent(2299/2999): loss=0.3899173817033775, w0=0.009634257002200334, w1=-0.25473033930937905\n",
      "(250000,)\n",
      "Gradient Descent(2300/2999): loss=0.3899173817033162, w0=0.009634256837191425, w1=-0.25473033890625574\n",
      "(250000,)\n",
      "Gradient Descent(2301/2999): loss=0.38991738170325546, w0=0.00963425667265292, w1=-0.2547303385042817\n",
      "(250000,)\n",
      "Gradient Descent(2302/2999): loss=0.389917381703195, w0=0.009634256508583483, w1=-0.2547303381034535\n",
      "(250000,)\n",
      "Gradient Descent(2303/2999): loss=0.38991738170313495, w0=0.00963425634498178, w1=-0.2547303377037681\n",
      "(250000,)\n",
      "Gradient Descent(2304/2999): loss=0.3899173817030751, w0=0.009634256181846478, w1=-0.25473033730522204\n",
      "(250000,)\n",
      "Gradient Descent(2305/2999): loss=0.38991738170301576, w0=0.009634256019176236, w1=-0.2547303369078122\n",
      "(250000,)\n",
      "Gradient Descent(2306/2999): loss=0.38991738170295664, w0=0.009634255856969746, w1=-0.2547303365115353\n",
      "(250000,)\n",
      "Gradient Descent(2307/2999): loss=0.3899173817028979, w0=0.009634255695225668, w1=-0.2547303361163881\n",
      "(250000,)\n",
      "Gradient Descent(2308/2999): loss=0.3899173817028395, w0=0.009634255533942699, w1=-0.2547303357223675\n",
      "(250000,)\n",
      "Gradient Descent(2309/2999): loss=0.3899173817027814, w0=0.009634255373119512, w1=-0.2547303353294701\n",
      "(250000,)\n",
      "Gradient Descent(2310/2999): loss=0.38991738170272366, w0=0.009634255212754806, w1=-0.25473033493769276\n",
      "(250000,)\n",
      "Gradient Descent(2311/2999): loss=0.38991738170266627, w0=0.009634255052847268, w1=-0.2547303345470323\n",
      "(250000,)\n",
      "Gradient Descent(2312/2999): loss=0.3899173817026092, w0=0.009634254893395597, w1=-0.2547303341574856\n",
      "(250000,)\n",
      "Gradient Descent(2313/2999): loss=0.38991738170255236, w0=0.009634254734398497, w1=-0.25473033376904936\n",
      "(250000,)\n",
      "Gradient Descent(2314/2999): loss=0.38991738170249607, w0=0.00963425457585467, w1=-0.2547303333817205\n",
      "(250000,)\n",
      "Gradient Descent(2315/2999): loss=0.38991738170243984, w0=0.009634254417762816, w1=-0.25473033299549586\n",
      "(250000,)\n",
      "Gradient Descent(2316/2999): loss=0.38991738170238405, w0=0.009634254260121653, w1=-0.25473033261037226\n",
      "(250000,)\n",
      "Gradient Descent(2317/2999): loss=0.3899173817023286, w0=0.009634254102929894, w1=-0.2547303322263466\n",
      "(250000,)\n",
      "Gradient Descent(2318/2999): loss=0.38991738170227336, w0=0.009634253946186264, w1=-0.2547303318434157\n",
      "(250000,)\n",
      "Gradient Descent(2319/2999): loss=0.3899173817022185, w0=0.009634253789889483, w1=-0.2547303314615765\n",
      "(250000,)\n",
      "Gradient Descent(2320/2999): loss=0.389917381702164, w0=0.009634253634038279, w1=-0.2547303310808258\n",
      "(250000,)\n",
      "Gradient Descent(2321/2999): loss=0.38991738170210977, w0=0.009634253478631368, w1=-0.2547303307011606\n",
      "(250000,)\n",
      "Gradient Descent(2322/2999): loss=0.3899173817020558, w0=0.009634253323667498, w1=-0.2547303303225777\n",
      "(250000,)\n",
      "Gradient Descent(2323/2999): loss=0.3899173817020022, w0=0.009634253169145396, w1=-0.2547303299450741\n",
      "(250000,)\n",
      "Gradient Descent(2324/2999): loss=0.3899173817019489, w0=0.009634253015063808, w1=-0.2547303295686467\n",
      "(250000,)\n",
      "Gradient Descent(2325/2999): loss=0.3899173817018959, w0=0.009634252861421486, w1=-0.25473032919329247\n",
      "(250000,)\n",
      "Gradient Descent(2326/2999): loss=0.3899173817018432, w0=0.00963425270821717, w1=-0.25473032881900826\n",
      "(250000,)\n",
      "Gradient Descent(2327/2999): loss=0.38991738170179074, w0=0.009634252555449608, w1=-0.2547303284457911\n",
      "(250000,)\n",
      "Gradient Descent(2328/2999): loss=0.3899173817017387, w0=0.009634252403117563, w1=-0.2547303280736379\n",
      "(250000,)\n",
      "Gradient Descent(2329/2999): loss=0.3899173817016869, w0=0.009634252251219796, w1=-0.2547303277025456\n",
      "(250000,)\n",
      "Gradient Descent(2330/2999): loss=0.3899173817016354, w0=0.009634252099755058, w1=-0.2547303273325112\n",
      "(250000,)\n",
      "Gradient Descent(2331/2999): loss=0.38991738170158413, w0=0.009634251948722111, w1=-0.25473032696353176\n",
      "(250000,)\n",
      "Gradient Descent(2332/2999): loss=0.38991738170153317, w0=0.009634251798119728, w1=-0.2547303265956042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(2333/2999): loss=0.3899173817014825, w0=0.009634251647946684, w1=-0.2547303262287255\n",
      "(250000,)\n",
      "Gradient Descent(2334/2999): loss=0.3899173817014322, w0=0.009634251498201763, w1=-0.2547303258628927\n",
      "(250000,)\n",
      "Gradient Descent(2335/2999): loss=0.3899173817013822, w0=0.009634251348883734, w1=-0.2547303254981028\n",
      "(250000,)\n",
      "Gradient Descent(2336/2999): loss=0.38991738170133233, w0=0.009634251199991383, w1=-0.2547303251343529\n",
      "(250000,)\n",
      "Gradient Descent(2337/2999): loss=0.3899173817012828, w0=0.009634251051523497, w1=-0.25473032477164\n",
      "(250000,)\n",
      "Gradient Descent(2338/2999): loss=0.3899173817012337, w0=0.009634250903478863, w1=-0.2547303244099611\n",
      "(250000,)\n",
      "Gradient Descent(2339/2999): loss=0.3899173817011847, w0=0.00963425075585628, w1=-0.2547303240493133\n",
      "(250000,)\n",
      "Gradient Descent(2340/2999): loss=0.389917381701136, w0=0.00963425060865454, w1=-0.2547303236896936\n",
      "(250000,)\n",
      "Gradient Descent(2341/2999): loss=0.38991738170108775, w0=0.009634250461872442, w1=-0.25473032333109913\n",
      "(250000,)\n",
      "Gradient Descent(2342/2999): loss=0.38991738170103957, w0=0.009634250315508793, w1=-0.25473032297352693\n",
      "(250000,)\n",
      "Gradient Descent(2343/2999): loss=0.38991738170099166, w0=0.009634250169562396, w1=-0.25473032261697415\n",
      "(250000,)\n",
      "Gradient Descent(2344/2999): loss=0.3899173817009441, w0=0.009634250024032074, w1=-0.2547303222614378\n",
      "(250000,)\n",
      "Gradient Descent(2345/2999): loss=0.3899173817008968, w0=0.009634249878916637, w1=-0.254730321906915\n",
      "(250000,)\n",
      "Gradient Descent(2346/2999): loss=0.38991738170084983, w0=0.009634249734214891, w1=-0.2547303215534029\n",
      "(250000,)\n",
      "Gradient Descent(2347/2999): loss=0.3899173817008031, w0=0.009634249589925658, w1=-0.2547303212008986\n",
      "(250000,)\n",
      "Gradient Descent(2348/2999): loss=0.38991738170075657, w0=0.009634249446047762, w1=-0.2547303208493992\n",
      "(250000,)\n",
      "Gradient Descent(2349/2999): loss=0.3899173817007104, w0=0.009634249302580035, w1=-0.25473032049890193\n",
      "(250000,)\n",
      "Gradient Descent(2350/2999): loss=0.3899173817006643, w0=0.009634249159521312, w1=-0.25473032014940383\n",
      "(250000,)\n",
      "Gradient Descent(2351/2999): loss=0.38991738170061874, w0=0.009634249016870416, w1=-0.2547303198009021\n",
      "(250000,)\n",
      "Gradient Descent(2352/2999): loss=0.3899173817005733, w0=0.009634248874626203, w1=-0.25473031945339386\n",
      "(250000,)\n",
      "Gradient Descent(2353/2999): loss=0.3899173817005281, w0=0.009634248732787497, w1=-0.2547303191068763\n",
      "(250000,)\n",
      "Gradient Descent(2354/2999): loss=0.3899173817004831, w0=0.00963424859135315, w1=-0.2547303187613466\n",
      "(250000,)\n",
      "Gradient Descent(2355/2999): loss=0.3899173817004385, w0=0.009634248450322017, w1=-0.25473031841680194\n",
      "(250000,)\n",
      "Gradient Descent(2356/2999): loss=0.38991738170039414, w0=0.009634248309692929, w1=-0.25473031807323954\n",
      "(250000,)\n",
      "Gradient Descent(2357/2999): loss=0.38991738170034995, w0=0.009634248169464744, w1=-0.2547303177306566\n",
      "(250000,)\n",
      "Gradient Descent(2358/2999): loss=0.38991738170030604, w0=0.009634248029636326, w1=-0.2547303173890502\n",
      "(250000,)\n",
      "Gradient Descent(2359/2999): loss=0.38991738170026236, w0=0.009634247890206536, w1=-0.2547303170484177\n",
      "(250000,)\n",
      "Gradient Descent(2360/2999): loss=0.38991738170021906, w0=0.009634247751174232, w1=-0.2547303167087563\n",
      "(250000,)\n",
      "Gradient Descent(2361/2999): loss=0.3899173817001758, w0=0.009634247612538286, w1=-0.25473031637006316\n",
      "(250000,)\n",
      "Gradient Descent(2362/2999): loss=0.38991738170013296, w0=0.009634247474297565, w1=-0.2547303160323356\n",
      "(250000,)\n",
      "Gradient Descent(2363/2999): loss=0.38991738170009027, w0=0.009634247336450944, w1=-0.25473031569557086\n",
      "(250000,)\n",
      "Gradient Descent(2364/2999): loss=0.3899173817000478, w0=0.009634247198997302, w1=-0.2547303153597662\n",
      "(250000,)\n",
      "Gradient Descent(2365/2999): loss=0.3899173817000056, w0=0.009634247061935513, w1=-0.25473031502491883\n",
      "(250000,)\n",
      "Gradient Descent(2366/2999): loss=0.38991738169996376, w0=0.009634246925264461, w1=-0.254730314691026\n",
      "(250000,)\n",
      "Gradient Descent(2367/2999): loss=0.38991738169992196, w0=0.009634246788983027, w1=-0.2547303143580851\n",
      "(250000,)\n",
      "Gradient Descent(2368/2999): loss=0.3899173816998805, w0=0.00963424665309011, w1=-0.2547303140260933\n",
      "(250000,)\n",
      "Gradient Descent(2369/2999): loss=0.38991738169983936, w0=0.009634246517584603, w1=-0.254730313695048\n",
      "(250000,)\n",
      "Gradient Descent(2370/2999): loss=0.3899173816997982, w0=0.00963424638246539, w1=-0.25473031336494645\n",
      "(250000,)\n",
      "Gradient Descent(2371/2999): loss=0.38991738169975754, w0=0.009634246247731374, w1=-0.2547303130357859\n",
      "(250000,)\n",
      "Gradient Descent(2372/2999): loss=0.389917381699717, w0=0.009634246113381461, w1=-0.2547303127075638\n",
      "(250000,)\n",
      "Gradient Descent(2373/2999): loss=0.38991738169967666, w0=0.00963424597941456, w1=-0.2547303123802774\n",
      "(250000,)\n",
      "Gradient Descent(2374/2999): loss=0.38991738169963663, w0=0.009634245845829578, w1=-0.25473031205392394\n",
      "(250000,)\n",
      "Gradient Descent(2375/2999): loss=0.38991738169959683, w0=0.009634245712625427, w1=-0.2547303117285009\n",
      "(250000,)\n",
      "Gradient Descent(2376/2999): loss=0.38991738169955714, w0=0.009634245579801014, w1=-0.2547303114040056\n",
      "(250000,)\n",
      "Gradient Descent(2377/2999): loss=0.3899173816995177, w0=0.009634245447355257, w1=-0.25473031108043537\n",
      "(250000,)\n",
      "Gradient Descent(2378/2999): loss=0.3899173816994786, w0=0.009634245315287076, w1=-0.25473031075778757\n",
      "(250000,)\n",
      "Gradient Descent(2379/2999): loss=0.3899173816994396, w0=0.009634245183595399, w1=-0.2547303104360596\n",
      "(250000,)\n",
      "Gradient Descent(2380/2999): loss=0.3899173816994009, w0=0.009634245052279153, w1=-0.25473031011524877\n",
      "(250000,)\n",
      "Gradient Descent(2381/2999): loss=0.3899173816993624, w0=0.009634244921337255, w1=-0.25473030979535255\n",
      "(250000,)\n",
      "Gradient Descent(2382/2999): loss=0.3899173816993241, w0=0.009634244790768649, w1=-0.25473030947636827\n",
      "(250000,)\n",
      "Gradient Descent(2383/2999): loss=0.3899173816992861, w0=0.00963424466057227, w1=-0.25473030915829337\n",
      "(250000,)\n",
      "Gradient Descent(2384/2999): loss=0.3899173816992482, w0=0.009634244530747058, w1=-0.25473030884112524\n",
      "(250000,)\n",
      "Gradient Descent(2385/2999): loss=0.38991738169921064, w0=0.009634244401291951, w1=-0.2547303085248613\n",
      "(250000,)\n",
      "Gradient Descent(2386/2999): loss=0.3899173816991731, w0=0.009634244272205902, w1=-0.25473030820949893\n",
      "(250000,)\n",
      "Gradient Descent(2387/2999): loss=0.3899173816991359, w0=0.009634244143487854, w1=-0.25473030789503565\n",
      "(250000,)\n",
      "Gradient Descent(2388/2999): loss=0.38991738169909895, w0=0.009634244015136758, w1=-0.25473030758146886\n",
      "(250000,)\n",
      "Gradient Descent(2389/2999): loss=0.3899173816990621, w0=0.009634243887151565, w1=-0.254730307268796\n",
      "(250000,)\n",
      "Gradient Descent(2390/2999): loss=0.3899173816990256, w0=0.00963424375953124, w1=-0.25473030695701443\n",
      "(250000,)\n",
      "Gradient Descent(2391/2999): loss=0.3899173816989891, w0=0.00963424363227473, w1=-0.25473030664612173\n",
      "(250000,)\n",
      "Gradient Descent(2392/2999): loss=0.38991738169895307, w0=0.009634243505381003, w1=-0.2547303063361153\n",
      "(250000,)\n",
      "Gradient Descent(2393/2999): loss=0.3899173816989171, w0=0.009634243378849027, w1=-0.2547303060269927\n",
      "(250000,)\n",
      "Gradient Descent(2394/2999): loss=0.38991738169888124, w0=0.00963424325267777, w1=-0.2547303057187513\n",
      "(250000,)\n",
      "Gradient Descent(2395/2999): loss=0.3899173816988458, w0=0.009634243126866206, w1=-0.2547303054113887\n",
      "(250000,)\n",
      "Gradient Descent(2396/2999): loss=0.38991738169881046, w0=0.009634243001413306, w1=-0.2547303051049023\n",
      "(250000,)\n",
      "Gradient Descent(2397/2999): loss=0.3899173816987753, w0=0.009634242876318049, w1=-0.25473030479928965\n",
      "(250000,)\n",
      "Gradient Descent(2398/2999): loss=0.3899173816987404, w0=0.009634242751579416, w1=-0.25473030449454825\n",
      "(250000,)\n",
      "Gradient Descent(2399/2999): loss=0.3899173816987056, w0=0.009634242627196386, w1=-0.2547303041906756\n",
      "(250000,)\n",
      "Gradient Descent(2400/2999): loss=0.38991738169867113, w0=0.00963424250316795, w1=-0.25473030388766926\n",
      "(250000,)\n",
      "Gradient Descent(2401/2999): loss=0.38991738169863677, w0=0.009634242379493098, w1=-0.25473030358552673\n",
      "(250000,)\n",
      "Gradient Descent(2402/2999): loss=0.38991738169860257, w0=0.009634242256170817, w1=-0.2547303032842455\n",
      "(250000,)\n",
      "Gradient Descent(2403/2999): loss=0.3899173816985686, w0=0.009634242133200119, w1=-0.2547303029838232\n",
      "(250000,)\n",
      "Gradient Descent(2404/2999): loss=0.38991738169853485, w0=0.009634242010579979, w1=-0.25473030268425734\n",
      "(250000,)\n",
      "Gradient Descent(2405/2999): loss=0.38991738169850126, w0=0.00963424188830941, w1=-0.2547303023855455\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2406/2999): loss=0.38991738169846785, w0=0.00963424176638741, w1=-0.2547303020876852\n",
      "(250000,)\n",
      "Gradient Descent(2407/2999): loss=0.3899173816984347, w0=0.009634241644812986, w1=-0.2547303017906741\n",
      "(250000,)\n",
      "Gradient Descent(2408/2999): loss=0.3899173816984017, w0=0.009634241523585144, w1=-0.2547303014945097\n",
      "(250000,)\n",
      "Gradient Descent(2409/2999): loss=0.3899173816983689, w0=0.009634241402702897, w1=-0.2547303011991896\n",
      "(250000,)\n",
      "Gradient Descent(2410/2999): loss=0.38991738169833623, w0=0.009634241282165265, w1=-0.25473030090471144\n",
      "(250000,)\n",
      "Gradient Descent(2411/2999): loss=0.38991738169830376, w0=0.009634241161971266, w1=-0.25473030061107277\n",
      "(250000,)\n",
      "Gradient Descent(2412/2999): loss=0.38991738169827145, w0=0.009634241042119924, w1=-0.2547303003182712\n",
      "(250000,)\n",
      "Gradient Descent(2413/2999): loss=0.3899173816982394, w0=0.009634240922610253, w1=-0.25473030002630437\n",
      "(250000,)\n",
      "Gradient Descent(2414/2999): loss=0.38991738169820755, w0=0.009634240803441276, w1=-0.25473029973516986\n",
      "(250000,)\n",
      "Gradient Descent(2415/2999): loss=0.3899173816981759, w0=0.009634240684612038, w1=-0.2547302994448653\n",
      "(250000,)\n",
      "Gradient Descent(2416/2999): loss=0.3899173816981444, w0=0.009634240566121555, w1=-0.25473029915538836\n",
      "(250000,)\n",
      "Gradient Descent(2417/2999): loss=0.38991738169811296, w0=0.009634240447968873, w1=-0.25473029886673665\n",
      "(250000,)\n",
      "Gradient Descent(2418/2999): loss=0.3899173816980818, w0=0.009634240330153017, w1=-0.25473029857890783\n",
      "(250000,)\n",
      "Gradient Descent(2419/2999): loss=0.38991738169805085, w0=0.009634240212673036, w1=-0.2547302982918996\n",
      "(250000,)\n",
      "Gradient Descent(2420/2999): loss=0.38991738169802004, w0=0.009634240095527965, w1=-0.25473029800570957\n",
      "(250000,)\n",
      "Gradient Descent(2421/2999): loss=0.38991738169798934, w0=0.00963423997871685, w1=-0.2547302977203354\n",
      "(250000,)\n",
      "Gradient Descent(2422/2999): loss=0.389917381697959, w0=0.009634239862238754, w1=-0.2547302974357748\n",
      "(250000,)\n",
      "Gradient Descent(2423/2999): loss=0.38991738169792867, w0=0.009634239746092708, w1=-0.2547302971520254\n",
      "(250000,)\n",
      "Gradient Descent(2424/2999): loss=0.3899173816978984, w0=0.009634239630277769, w1=-0.25473029686908494\n",
      "(250000,)\n",
      "Gradient Descent(2425/2999): loss=0.3899173816978685, w0=0.009634239514792996, w1=-0.25473029658695107\n",
      "(250000,)\n",
      "Gradient Descent(2426/2999): loss=0.38991738169783874, w0=0.00963423939963745, w1=-0.2547302963056215\n",
      "(250000,)\n",
      "Gradient Descent(2427/2999): loss=0.38991738169780915, w0=0.009634239284810185, w1=-0.25473029602509395\n",
      "(250000,)\n",
      "Gradient Descent(2428/2999): loss=0.3899173816977797, w0=0.00963423917031028, w1=-0.25473029574536615\n",
      "(250000,)\n",
      "Gradient Descent(2429/2999): loss=0.3899173816977505, w0=0.009634239056136794, w1=-0.25473029546643583\n",
      "(250000,)\n",
      "Gradient Descent(2430/2999): loss=0.38991738169772133, w0=0.009634238942288795, w1=-0.25473029518830065\n",
      "(250000,)\n",
      "Gradient Descent(2431/2999): loss=0.3899173816976924, w0=0.009634238828765353, w1=-0.2547302949109584\n",
      "(250000,)\n",
      "Gradient Descent(2432/2999): loss=0.38991738169766366, w0=0.009634238715565553, w1=-0.2547302946344068\n",
      "(250000,)\n",
      "Gradient Descent(2433/2999): loss=0.389917381697635, w0=0.009634238602688456, w1=-0.2547302943586436\n",
      "(250000,)\n",
      "Gradient Descent(2434/2999): loss=0.3899173816976066, w0=0.009634238490133154, w1=-0.25473029408366654\n",
      "(250000,)\n",
      "Gradient Descent(2435/2999): loss=0.3899173816975782, w0=0.00963423837789872, w1=-0.2547302938094734\n",
      "(250000,)\n",
      "Gradient Descent(2436/2999): loss=0.3899173816975502, w0=0.009634238265984255, w1=-0.2547302935360619\n",
      "(250000,)\n",
      "Gradient Descent(2437/2999): loss=0.3899173816975221, w0=0.009634238154388838, w1=-0.2547302932634299\n",
      "(250000,)\n",
      "Gradient Descent(2438/2999): loss=0.3899173816974943, w0=0.009634238043111561, w1=-0.25473029299157507\n",
      "(250000,)\n",
      "Gradient Descent(2439/2999): loss=0.38991738169746665, w0=0.00963423793215151, w1=-0.25473029272049524\n",
      "(250000,)\n",
      "Gradient Descent(2440/2999): loss=0.3899173816974392, w0=0.00963423782150779, w1=-0.25473029245018824\n",
      "(250000,)\n",
      "Gradient Descent(2441/2999): loss=0.38991738169741186, w0=0.009634237711179494, w1=-0.25473029218065185\n",
      "(250000,)\n",
      "Gradient Descent(2442/2999): loss=0.3899173816973847, w0=0.009634237601165717, w1=-0.25473029191188384\n",
      "(250000,)\n",
      "Gradient Descent(2443/2999): loss=0.3899173816973577, w0=0.009634237491465567, w1=-0.25473029164388206\n",
      "(250000,)\n",
      "Gradient Descent(2444/2999): loss=0.38991738169733076, w0=0.00963423738207815, w1=-0.25473029137664427\n",
      "(250000,)\n",
      "Gradient Descent(2445/2999): loss=0.38991738169730406, w0=0.009634237273002586, w1=-0.2547302911101683\n",
      "(250000,)\n",
      "Gradient Descent(2446/2999): loss=0.38991738169727747, w0=0.009634237164237981, w1=-0.2547302908444521\n",
      "(250000,)\n",
      "Gradient Descent(2447/2999): loss=0.389917381697251, w0=0.009634237055783437, w1=-0.25473029057949337\n",
      "(250000,)\n",
      "Gradient Descent(2448/2999): loss=0.3899173816972248, w0=0.009634236947638075, w1=-0.25473029031529\n",
      "(250000,)\n",
      "Gradient Descent(2449/2999): loss=0.38991738169719875, w0=0.009634236839801018, w1=-0.2547302900518398\n",
      "(250000,)\n",
      "Gradient Descent(2450/2999): loss=0.3899173816971727, w0=0.009634236732271385, w1=-0.25473028978914064\n",
      "(250000,)\n",
      "Gradient Descent(2451/2999): loss=0.3899173816971469, w0=0.009634236625048297, w1=-0.2547302895271904\n",
      "(250000,)\n",
      "Gradient Descent(2452/2999): loss=0.38991738169712126, w0=0.00963423651813088, w1=-0.2547302892659869\n",
      "(250000,)\n",
      "Gradient Descent(2453/2999): loss=0.38991738169709567, w0=0.009634236411518267, w1=-0.2547302890055281\n",
      "(250000,)\n",
      "Gradient Descent(2454/2999): loss=0.3899173816970703, w0=0.009634236305209582, w1=-0.2547302887458118\n",
      "(250000,)\n",
      "Gradient Descent(2455/2999): loss=0.389917381697045, w0=0.009634236199203963, w1=-0.2547302884868359\n",
      "(250000,)\n",
      "Gradient Descent(2456/2999): loss=0.38991738169702, w0=0.009634236093500545, w1=-0.2547302882285982\n",
      "(250000,)\n",
      "Gradient Descent(2457/2999): loss=0.38991738169699514, w0=0.009634235988098466, w1=-0.2547302879710968\n",
      "(250000,)\n",
      "Gradient Descent(2458/2999): loss=0.3899173816969703, w0=0.009634235882996877, w1=-0.2547302877143295\n",
      "(250000,)\n",
      "Gradient Descent(2459/2999): loss=0.38991738169694556, w0=0.009634235778194916, w1=-0.2547302874582941\n",
      "(250000,)\n",
      "Gradient Descent(2460/2999): loss=0.3899173816969211, w0=0.009634235673691725, w1=-0.2547302872029887\n",
      "(250000,)\n",
      "Gradient Descent(2461/2999): loss=0.3899173816968967, w0=0.009634235569486449, w1=-0.25473028694841104\n",
      "(250000,)\n",
      "Gradient Descent(2462/2999): loss=0.38991738169687246, w0=0.009634235465578248, w1=-0.25473028669455916\n",
      "(250000,)\n",
      "Gradient Descent(2463/2999): loss=0.3899173816968483, w0=0.009634235361966266, w1=-0.254730286441431\n",
      "(250000,)\n",
      "Gradient Descent(2464/2999): loss=0.38991738169682433, w0=0.00963423525864966, w1=-0.25473028618902444\n",
      "(250000,)\n",
      "Gradient Descent(2465/2999): loss=0.38991738169680046, w0=0.009634235155627594, w1=-0.25473028593733743\n",
      "(250000,)\n",
      "Gradient Descent(2466/2999): loss=0.3899173816967768, w0=0.009634235052899225, w1=-0.25473028568636796\n",
      "(250000,)\n",
      "Gradient Descent(2467/2999): loss=0.3899173816967532, w0=0.009634234950463718, w1=-0.254730285436114\n",
      "(250000,)\n",
      "Gradient Descent(2468/2999): loss=0.38991738169672974, w0=0.009634234848320242, w1=-0.2547302851865734\n",
      "(250000,)\n",
      "Gradient Descent(2469/2999): loss=0.3899173816967064, w0=0.009634234746467948, w1=-0.25473028493774424\n",
      "(250000,)\n",
      "Gradient Descent(2470/2999): loss=0.38991738169668333, w0=0.009634234644906021, w1=-0.25473028468962444\n",
      "(250000,)\n",
      "Gradient Descent(2471/2999): loss=0.38991738169666024, w0=0.009634234543633633, w1=-0.25473028444221196\n",
      "(250000,)\n",
      "Gradient Descent(2472/2999): loss=0.38991738169663737, w0=0.00963423444264995, w1=-0.2547302841955048\n",
      "(250000,)\n",
      "Gradient Descent(2473/2999): loss=0.3899173816966146, w0=0.009634234341954151, w1=-0.254730283949501\n",
      "(250000,)\n",
      "Gradient Descent(2474/2999): loss=0.38991738169659196, w0=0.009634234241545414, w1=-0.2547302837041985\n",
      "(250000,)\n",
      "Gradient Descent(2475/2999): loss=0.3899173816965695, w0=0.009634234141422927, w1=-0.25473028345959525\n",
      "(250000,)\n",
      "Gradient Descent(2476/2999): loss=0.38991738169654705, w0=0.009634234041585869, w1=-0.25473028321568936\n",
      "(250000,)\n",
      "Gradient Descent(2477/2999): loss=0.3899173816965248, w0=0.009634233942033426, w1=-0.2547302829724788\n",
      "(250000,)\n",
      "Gradient Descent(2478/2999): loss=0.3899173816965027, w0=0.009634233842764784, w1=-0.25473028272996157\n",
      "(250000,)\n",
      "Gradient Descent(2479/2999): loss=0.38991738169648077, w0=0.009634233743779145, w1=-0.25473028248813573\n",
      "(250000,)\n",
      "Gradient Descent(2480/2999): loss=0.3899173816964588, w0=0.009634233645075693, w1=-0.2547302822469993\n",
      "(250000,)\n",
      "Gradient Descent(2481/2999): loss=0.3899173816964371, w0=0.00963423354665363, w1=-0.2547302820065503\n",
      "(250000,)\n",
      "Gradient Descent(2482/2999): loss=0.38991738169641543, w0=0.009634233448512145, w1=-0.2547302817667868\n",
      "(250000,)\n",
      "Gradient Descent(2483/2999): loss=0.38991738169639395, w0=0.009634233350650445, w1=-0.2547302815277068\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2484/2999): loss=0.3899173816963725, w0=0.009634233253067734, w1=-0.2547302812893084\n",
      "(250000,)\n",
      "Gradient Descent(2485/2999): loss=0.3899173816963512, w0=0.009634233155763219, w1=-0.2547302810515896\n",
      "(250000,)\n",
      "Gradient Descent(2486/2999): loss=0.38991738169633006, w0=0.009634233058736102, w1=-0.2547302808145485\n",
      "(250000,)\n",
      "Gradient Descent(2487/2999): loss=0.3899173816963091, w0=0.009634232961985581, w1=-0.25473028057818314\n",
      "(250000,)\n",
      "Gradient Descent(2488/2999): loss=0.38991738169628815, w0=0.00963423286551088, w1=-0.2547302803424916\n",
      "(250000,)\n",
      "Gradient Descent(2489/2999): loss=0.3899173816962674, w0=0.00963423276931121, w1=-0.254730280107472\n",
      "(250000,)\n",
      "Gradient Descent(2490/2999): loss=0.3899173816962468, w0=0.00963423267338579, w1=-0.2547302798731224\n",
      "(250000,)\n",
      "Gradient Descent(2491/2999): loss=0.3899173816962262, w0=0.009634232577733838, w1=-0.2547302796394409\n",
      "(250000,)\n",
      "Gradient Descent(2492/2999): loss=0.38991738169620577, w0=0.009634232482354569, w1=-0.25473027940642556\n",
      "(250000,)\n",
      "Gradient Descent(2493/2999): loss=0.3899173816961855, w0=0.009634232387247215, w1=-0.25473027917407454\n",
      "(250000,)\n",
      "Gradient Descent(2494/2999): loss=0.38991738169616524, w0=0.009634232292410987, w1=-0.25473027894238587\n",
      "(250000,)\n",
      "Gradient Descent(2495/2999): loss=0.38991738169614515, w0=0.009634232197845123, w1=-0.2547302787113577\n",
      "(250000,)\n",
      "Gradient Descent(2496/2999): loss=0.3899173816961251, w0=0.00963423210354885, w1=-0.25473027848098817\n",
      "(250000,)\n",
      "Gradient Descent(2497/2999): loss=0.38991738169610535, w0=0.009634232009521399, w1=-0.25473027825127537\n",
      "(250000,)\n",
      "Gradient Descent(2498/2999): loss=0.38991738169608553, w0=0.009634231915761994, w1=-0.2547302780222174\n",
      "(250000,)\n",
      "Gradient Descent(2499/2999): loss=0.3899173816960659, w0=0.00963423182226988, w1=-0.2547302777938125\n",
      "(250000,)\n",
      "Gradient Descent(2500/2999): loss=0.3899173816960464, w0=0.009634231729044294, w1=-0.25473027756605876\n",
      "(250000,)\n",
      "Gradient Descent(2501/2999): loss=0.3899173816960269, w0=0.009634231636084475, w1=-0.25473027733895426\n",
      "(250000,)\n",
      "Gradient Descent(2502/2999): loss=0.3899173816960077, w0=0.00963423154338967, w1=-0.2547302771124972\n",
      "(250000,)\n",
      "Gradient Descent(2503/2999): loss=0.38991738169598844, w0=0.009634231450959119, w1=-0.2547302768866857\n",
      "(250000,)\n",
      "Gradient Descent(2504/2999): loss=0.3899173816959694, w0=0.009634231358792078, w1=-0.254730276661518\n",
      "(250000,)\n",
      "Gradient Descent(2505/2999): loss=0.38991738169595047, w0=0.009634231266887786, w1=-0.25473027643699214\n",
      "(250000,)\n",
      "Gradient Descent(2506/2999): loss=0.3899173816959316, w0=0.009634231175245495, w1=-0.2547302762131064\n",
      "(250000,)\n",
      "Gradient Descent(2507/2999): loss=0.38991738169591283, w0=0.009634231083864457, w1=-0.2547302759898589\n",
      "(250000,)\n",
      "Gradient Descent(2508/2999): loss=0.3899173816958943, w0=0.009634230992743938, w1=-0.2547302757672479\n",
      "(250000,)\n",
      "Gradient Descent(2509/2999): loss=0.38991738169587564, w0=0.009634230901883178, w1=-0.25473027554527145\n",
      "(250000,)\n",
      "Gradient Descent(2510/2999): loss=0.3899173816958572, w0=0.009634230811281447, w1=-0.25473027532392784\n",
      "(250000,)\n",
      "Gradient Descent(2511/2999): loss=0.3899173816958389, w0=0.009634230720938008, w1=-0.2547302751032152\n",
      "(250000,)\n",
      "Gradient Descent(2512/2999): loss=0.3899173816958207, w0=0.009634230630852118, w1=-0.2547302748831318\n",
      "(250000,)\n",
      "Gradient Descent(2513/2999): loss=0.38991738169580253, w0=0.00963423054102305, w1=-0.25473027466367587\n",
      "(250000,)\n",
      "Gradient Descent(2514/2999): loss=0.38991738169578455, w0=0.009634230451450076, w1=-0.25473027444484553\n",
      "(250000,)\n",
      "Gradient Descent(2515/2999): loss=0.38991738169576656, w0=0.00963423036213245, w1=-0.254730274226639\n",
      "(250000,)\n",
      "Gradient Descent(2516/2999): loss=0.38991738169574874, w0=0.009634230273069455, w1=-0.25473027400905457\n",
      "(250000,)\n",
      "Gradient Descent(2517/2999): loss=0.38991738169573104, w0=0.009634230184260354, w1=-0.25473027379209046\n",
      "(250000,)\n",
      "Gradient Descent(2518/2999): loss=0.3899173816957135, w0=0.009634230095704435, w1=-0.25473027357574485\n",
      "(250000,)\n",
      "Gradient Descent(2519/2999): loss=0.3899173816956959, w0=0.009634230007400975, w1=-0.25473027336001597\n",
      "(250000,)\n",
      "Gradient Descent(2520/2999): loss=0.38991738169567863, w0=0.009634229919349249, w1=-0.25473027314490215\n",
      "(250000,)\n",
      "Gradient Descent(2521/2999): loss=0.38991738169566126, w0=0.00963422983154854, w1=-0.25473027293040157\n",
      "(250000,)\n",
      "Gradient Descent(2522/2999): loss=0.389917381695644, w0=0.009634229743998144, w1=-0.25473027271651244\n",
      "(250000,)\n",
      "Gradient Descent(2523/2999): loss=0.38991738169562695, w0=0.00963422965669733, w1=-0.2547302725032331\n",
      "(250000,)\n",
      "Gradient Descent(2524/2999): loss=0.3899173816956098, w0=0.009634229569645396, w1=-0.2547302722905618\n",
      "(250000,)\n",
      "Gradient Descent(2525/2999): loss=0.38991738169559287, w0=0.009634229482841627, w1=-0.25473027207849674\n",
      "(250000,)\n",
      "Gradient Descent(2526/2999): loss=0.389917381695576, w0=0.00963422939628532, w1=-0.2547302718670363\n",
      "(250000,)\n",
      "Gradient Descent(2527/2999): loss=0.3899173816955594, w0=0.009634229309975772, w1=-0.2547302716561786\n",
      "(250000,)\n",
      "Gradient Descent(2528/2999): loss=0.3899173816955428, w0=0.009634229223912274, w1=-0.2547302714459221\n",
      "(250000,)\n",
      "Gradient Descent(2529/2999): loss=0.3899173816955262, w0=0.009634229138094133, w1=-0.25473027123626496\n",
      "(250000,)\n",
      "Gradient Descent(2530/2999): loss=0.38991738169550977, w0=0.009634229052520645, w1=-0.2547302710272055\n",
      "(250000,)\n",
      "Gradient Descent(2531/2999): loss=0.3899173816954934, w0=0.009634228967191105, w1=-0.2547302708187421\n",
      "(250000,)\n",
      "Gradient Descent(2532/2999): loss=0.38991738169547707, w0=0.009634228882104832, w1=-0.25473027061087294\n",
      "(250000,)\n",
      "Gradient Descent(2533/2999): loss=0.389917381695461, w0=0.00963422879726112, w1=-0.2547302704035964\n",
      "(250000,)\n",
      "Gradient Descent(2534/2999): loss=0.38991738169544493, w0=0.009634228712659286, w1=-0.25473027019691075\n",
      "(250000,)\n",
      "Gradient Descent(2535/2999): loss=0.38991738169542883, w0=0.009634228628298633, w1=-0.25473026999081433\n",
      "(250000,)\n",
      "Gradient Descent(2536/2999): loss=0.389917381695413, w0=0.009634228544178482, w1=-0.25473026978530544\n",
      "(250000,)\n",
      "Gradient Descent(2537/2999): loss=0.3899173816953973, w0=0.009634228460298133, w1=-0.2547302695803824\n",
      "(250000,)\n",
      "Gradient Descent(2538/2999): loss=0.38991738169538154, w0=0.009634228376656912, w1=-0.2547302693760436\n",
      "(250000,)\n",
      "Gradient Descent(2539/2999): loss=0.3899173816953659, w0=0.009634228293254138, w1=-0.2547302691722873\n",
      "(250000,)\n",
      "Gradient Descent(2540/2999): loss=0.3899173816953503, w0=0.00963422821008913, w1=-0.2547302689691119\n",
      "(250000,)\n",
      "Gradient Descent(2541/2999): loss=0.38991738169533485, w0=0.009634228127161207, w1=-0.2547302687665157\n",
      "(250000,)\n",
      "Gradient Descent(2542/2999): loss=0.3899173816953195, w0=0.0096342280444697, w1=-0.2547302685644971\n",
      "(250000,)\n",
      "Gradient Descent(2543/2999): loss=0.3899173816953042, w0=0.009634227962013936, w1=-0.2547302683630544\n",
      "(250000,)\n",
      "Gradient Descent(2544/2999): loss=0.38991738169528906, w0=0.00963422787979324, w1=-0.25473026816218597\n",
      "(250000,)\n",
      "Gradient Descent(2545/2999): loss=0.38991738169527396, w0=0.00963422779780694, w1=-0.25473026796189013\n",
      "(250000,)\n",
      "Gradient Descent(2546/2999): loss=0.38991738169525897, w0=0.009634227716054366, w1=-0.25473026776216534\n",
      "(250000,)\n",
      "Gradient Descent(2547/2999): loss=0.3899173816952441, w0=0.009634227634534849, w1=-0.2547302675630099\n",
      "(250000,)\n",
      "Gradient Descent(2548/2999): loss=0.3899173816952292, w0=0.00963422755324773, w1=-0.2547302673644223\n",
      "(250000,)\n",
      "Gradient Descent(2549/2999): loss=0.38991738169521445, w0=0.009634227472192345, w1=-0.2547302671664008\n",
      "(250000,)\n",
      "Gradient Descent(2550/2999): loss=0.3899173816951997, w0=0.009634227391368029, w1=-0.2547302669689438\n",
      "(250000,)\n",
      "Gradient Descent(2551/2999): loss=0.38991738169518514, w0=0.009634227310774133, w1=-0.25473026677204974\n",
      "(250000,)\n",
      "Gradient Descent(2552/2999): loss=0.38991738169517065, w0=0.009634227230409994, w1=-0.25473026657571696\n",
      "(250000,)\n",
      "Gradient Descent(2553/2999): loss=0.3899173816951562, w0=0.009634227150274953, w1=-0.2547302663799439\n",
      "(250000,)\n",
      "Gradient Descent(2554/2999): loss=0.38991738169514195, w0=0.00963422707036836, w1=-0.25473026618472894\n",
      "(250000,)\n",
      "Gradient Descent(2555/2999): loss=0.38991738169512763, w0=0.009634226990689583, w1=-0.2547302659900705\n",
      "(250000,)\n",
      "Gradient Descent(2556/2999): loss=0.3899173816951135, w0=0.009634226911237949, w1=-0.25473026579596697\n",
      "(250000,)\n",
      "Gradient Descent(2557/2999): loss=0.3899173816950993, w0=0.009634226832012809, w1=-0.25473026560241685\n",
      "(250000,)\n",
      "Gradient Descent(2558/2999): loss=0.38991738169508533, w0=0.009634226753013534, w1=-0.25473026540941845\n",
      "(250000,)\n",
      "Gradient Descent(2559/2999): loss=0.3899173816950714, w0=0.00963422667423947, w1=-0.2547302652169703\n",
      "(250000,)\n",
      "Gradient Descent(2560/2999): loss=0.3899173816950575, w0=0.009634226595689972, w1=-0.25473026502507073\n",
      "(250000,)\n",
      "Gradient Descent(2561/2999): loss=0.3899173816950437, w0=0.009634226517364406, w1=-0.2547302648337183\n",
      "(250000,)\n",
      "Gradient Descent(2562/2999): loss=0.38991738169503, w0=0.009634226439262131, w1=-0.2547302646429114\n",
      "(250000,)\n",
      "Gradient Descent(2563/2999): loss=0.38991738169501644, w0=0.00963422636138251, w1=-0.2547302644526484\n",
      "(250000,)\n",
      "Gradient Descent(2564/2999): loss=0.38991738169500284, w0=0.009634226283724909, w1=-0.25473026426292783\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2565/2999): loss=0.38991738169498946, w0=0.009634226206288694, w1=-0.2547302640737481\n",
      "(250000,)\n",
      "Gradient Descent(2566/2999): loss=0.38991738169497603, w0=0.009634226129073232, w1=-0.25473026388510767\n",
      "(250000,)\n",
      "Gradient Descent(2567/2999): loss=0.3899173816949627, w0=0.009634226052077897, w1=-0.254730263697005\n",
      "(250000,)\n",
      "Gradient Descent(2568/2999): loss=0.3899173816949494, w0=0.009634225975302071, w1=-0.2547302635094386\n",
      "(250000,)\n",
      "Gradient Descent(2569/2999): loss=0.38991738169493634, w0=0.009634225898745123, w1=-0.25473026332240695\n",
      "(250000,)\n",
      "Gradient Descent(2570/2999): loss=0.3899173816949232, w0=0.009634225822406419, w1=-0.25473026313590846\n",
      "(250000,)\n",
      "Gradient Descent(2571/2999): loss=0.3899173816949102, w0=0.009634225746285339, w1=-0.25473026294994167\n",
      "(250000,)\n",
      "Gradient Descent(2572/2999): loss=0.38991738169489726, w0=0.009634225670381268, w1=-0.254730262764505\n",
      "(250000,)\n",
      "Gradient Descent(2573/2999): loss=0.38991738169488444, w0=0.009634225594693584, w1=-0.25473026257959697\n",
      "(250000,)\n",
      "Gradient Descent(2574/2999): loss=0.3899173816948715, w0=0.009634225519221669, w1=-0.2547302623952161\n",
      "(250000,)\n",
      "Gradient Descent(2575/2999): loss=0.38991738169485873, w0=0.009634225443964911, w1=-0.2547302622113609\n",
      "(250000,)\n",
      "Gradient Descent(2576/2999): loss=0.3899173816948462, w0=0.009634225368922701, w1=-0.2547302620280298\n",
      "(250000,)\n",
      "Gradient Descent(2577/2999): loss=0.3899173816948337, w0=0.009634225294094422, w1=-0.25473026184522135\n",
      "(250000,)\n",
      "Gradient Descent(2578/2999): loss=0.38991738169482115, w0=0.009634225219479464, w1=-0.25473026166293405\n",
      "(250000,)\n",
      "Gradient Descent(2579/2999): loss=0.38991738169480866, w0=0.00963422514507722, w1=-0.2547302614811664\n",
      "(250000,)\n",
      "Gradient Descent(2580/2999): loss=0.3899173816947963, w0=0.009634225070887084, w1=-0.25473026129991694\n",
      "(250000,)\n",
      "Gradient Descent(2581/2999): loss=0.38991738169478396, w0=0.009634224996908452, w1=-0.2547302611191842\n",
      "(250000,)\n",
      "Gradient Descent(2582/2999): loss=0.3899173816947718, w0=0.009634224923140717, w1=-0.25473026093896667\n",
      "(250000,)\n",
      "Gradient Descent(2583/2999): loss=0.3899173816947596, w0=0.009634224849583282, w1=-0.2547302607592629\n",
      "(250000,)\n",
      "Gradient Descent(2584/2999): loss=0.3899173816947476, w0=0.00963422477623555, w1=-0.2547302605800715\n",
      "(250000,)\n",
      "Gradient Descent(2585/2999): loss=0.3899173816947355, w0=0.00963422470309691, w1=-0.2547302604013909\n",
      "(250000,)\n",
      "Gradient Descent(2586/2999): loss=0.3899173816947236, w0=0.00963422463016678, w1=-0.2547302602232197\n",
      "(250000,)\n",
      "Gradient Descent(2587/2999): loss=0.38991738169471174, w0=0.009634224557444563, w1=-0.25473026004555643\n",
      "(250000,)\n",
      "Gradient Descent(2588/2999): loss=0.38991738169469997, w0=0.00963422448492966, w1=-0.25473025986839964\n",
      "(250000,)\n",
      "Gradient Descent(2589/2999): loss=0.38991738169468815, w0=0.009634224412621486, w1=-0.2547302596917479\n",
      "(250000,)\n",
      "Gradient Descent(2590/2999): loss=0.38991738169467655, w0=0.009634224340519446, w1=-0.25473025951559974\n",
      "(250000,)\n",
      "Gradient Descent(2591/2999): loss=0.3899173816946649, w0=0.009634224268622956, w1=-0.25473025933995375\n",
      "(250000,)\n",
      "Gradient Descent(2592/2999): loss=0.3899173816946533, w0=0.009634224196931435, w1=-0.2547302591648085\n",
      "(250000,)\n",
      "Gradient Descent(2593/2999): loss=0.38991738169464185, w0=0.009634224125444287, w1=-0.2547302589901626\n",
      "(250000,)\n",
      "Gradient Descent(2594/2999): loss=0.3899173816946304, w0=0.009634224054160931, w1=-0.2547302588160146\n",
      "(250000,)\n",
      "Gradient Descent(2595/2999): loss=0.389917381694619, w0=0.009634223983080795, w1=-0.254730258642363\n",
      "(250000,)\n",
      "Gradient Descent(2596/2999): loss=0.38991738169460777, w0=0.009634223912203298, w1=-0.25473025846920644\n",
      "(250000,)\n",
      "Gradient Descent(2597/2999): loss=0.38991738169459655, w0=0.009634223841527859, w1=-0.25473025829654355\n",
      "(250000,)\n",
      "Gradient Descent(2598/2999): loss=0.3899173816945854, w0=0.0096342237710539, w1=-0.2547302581243729\n",
      "(250000,)\n",
      "Gradient Descent(2599/2999): loss=0.3899173816945743, w0=0.00963422370078085, w1=-0.25473025795269305\n",
      "(250000,)\n",
      "Gradient Descent(2600/2999): loss=0.38991738169456325, w0=0.009634223630708138, w1=-0.2547302577815026\n",
      "(250000,)\n",
      "Gradient Descent(2601/2999): loss=0.3899173816945523, w0=0.009634223560835186, w1=-0.2547302576108002\n",
      "(250000,)\n",
      "Gradient Descent(2602/2999): loss=0.38991738169454143, w0=0.00963422349116143, w1=-0.2547302574405845\n",
      "(250000,)\n",
      "Gradient Descent(2603/2999): loss=0.3899173816945306, w0=0.009634223421686303, w1=-0.25473025727085397\n",
      "(250000,)\n",
      "Gradient Descent(2604/2999): loss=0.38991738169451984, w0=0.00963422335240924, w1=-0.25473025710160735\n",
      "(250000,)\n",
      "Gradient Descent(2605/2999): loss=0.38991738169450907, w0=0.00963422328332967, w1=-0.25473025693284324\n",
      "(250000,)\n",
      "Gradient Descent(2606/2999): loss=0.3899173816944984, w0=0.009634223214447032, w1=-0.25473025676456024\n",
      "(250000,)\n",
      "Gradient Descent(2607/2999): loss=0.38991738169448786, w0=0.009634223145760765, w1=-0.25473025659675697\n",
      "(250000,)\n",
      "Gradient Descent(2608/2999): loss=0.3899173816944772, w0=0.009634223077270314, w1=-0.25473025642943203\n",
      "(250000,)\n",
      "Gradient Descent(2609/2999): loss=0.38991738169446677, w0=0.009634223008975111, w1=-0.25473025626258416\n",
      "(250000,)\n",
      "Gradient Descent(2610/2999): loss=0.38991738169445633, w0=0.009634222940874609, w1=-0.2547302560962119\n",
      "(250000,)\n",
      "Gradient Descent(2611/2999): loss=0.38991738169444606, w0=0.009634222872968246, w1=-0.25473025593031395\n",
      "(250000,)\n",
      "Gradient Descent(2612/2999): loss=0.38991738169443585, w0=0.009634222805255468, w1=-0.25473025576488895\n",
      "(250000,)\n",
      "Gradient Descent(2613/2999): loss=0.38991738169442547, w0=0.009634222737735728, w1=-0.25473025559993556\n",
      "(250000,)\n",
      "Gradient Descent(2614/2999): loss=0.3899173816944152, w0=0.009634222670408487, w1=-0.2547302554354524\n",
      "(250000,)\n",
      "Gradient Descent(2615/2999): loss=0.3899173816944052, w0=0.009634222603273183, w1=-0.25473025527143817\n",
      "(250000,)\n",
      "Gradient Descent(2616/2999): loss=0.3899173816943951, w0=0.009634222536329264, w1=-0.2547302551078915\n",
      "(250000,)\n",
      "Gradient Descent(2617/2999): loss=0.389917381694385, w0=0.009634222469576185, w1=-0.25473025494481105\n",
      "(250000,)\n",
      "Gradient Descent(2618/2999): loss=0.38991738169437506, w0=0.00963422240301341, w1=-0.2547302547821956\n",
      "(250000,)\n",
      "Gradient Descent(2619/2999): loss=0.3899173816943653, w0=0.009634222336640392, w1=-0.2547302546200437\n",
      "(250000,)\n",
      "Gradient Descent(2620/2999): loss=0.38991738169435536, w0=0.009634222270456592, w1=-0.254730254458354\n",
      "(250000,)\n",
      "Gradient Descent(2621/2999): loss=0.3899173816943456, w0=0.009634222204461473, w1=-0.2547302542971253\n",
      "(250000,)\n",
      "Gradient Descent(2622/2999): loss=0.3899173816943358, w0=0.009634222138654494, w1=-0.25473025413635625\n",
      "(250000,)\n",
      "Gradient Descent(2623/2999): loss=0.38991738169432616, w0=0.009634222073035117, w1=-0.2547302539760455\n",
      "(250000,)\n",
      "Gradient Descent(2624/2999): loss=0.3899173816943166, w0=0.009634222007602812, w1=-0.25473025381619174\n",
      "(250000,)\n",
      "Gradient Descent(2625/2999): loss=0.389917381694307, w0=0.009634221942357038, w1=-0.25473025365679375\n",
      "(250000,)\n",
      "Gradient Descent(2626/2999): loss=0.38991738169429746, w0=0.009634221877297268, w1=-0.2547302534978501\n",
      "(250000,)\n",
      "Gradient Descent(2627/2999): loss=0.389917381694288, w0=0.009634221812422973, w1=-0.2547302533393596\n",
      "(250000,)\n",
      "Gradient Descent(2628/2999): loss=0.3899173816942787, w0=0.00963422174773362, w1=-0.254730253181321\n",
      "(250000,)\n",
      "Gradient Descent(2629/2999): loss=0.3899173816942693, w0=0.009634221683228682, w1=-0.25473025302373287\n",
      "(250000,)\n",
      "Gradient Descent(2630/2999): loss=0.38991738169426, w0=0.009634221618907642, w1=-0.254730252866594\n",
      "(250000,)\n",
      "Gradient Descent(2631/2999): loss=0.3899173816942507, w0=0.00963422155476997, w1=-0.2547302527099031\n",
      "(250000,)\n",
      "Gradient Descent(2632/2999): loss=0.38991738169424156, w0=0.009634221490815146, w1=-0.25473025255365894\n",
      "(250000,)\n",
      "Gradient Descent(2633/2999): loss=0.38991738169423246, w0=0.00963422142704264, w1=-0.2547302523978602\n",
      "(250000,)\n",
      "Gradient Descent(2634/2999): loss=0.38991738169422335, w0=0.009634221363451939, w1=-0.25473025224250556\n",
      "(250000,)\n",
      "Gradient Descent(2635/2999): loss=0.38991738169421436, w0=0.009634221300042524, w1=-0.2547302520875938\n",
      "(250000,)\n",
      "Gradient Descent(2636/2999): loss=0.38991738169420526, w0=0.009634221236813878, w1=-0.2547302519331237\n",
      "(250000,)\n",
      "Gradient Descent(2637/2999): loss=0.3899173816941964, w0=0.009634221173765486, w1=-0.254730251779094\n",
      "(250000,)\n",
      "Gradient Descent(2638/2999): loss=0.3899173816941875, w0=0.009634221110896828, w1=-0.25473025162550333\n",
      "(250000,)\n",
      "Gradient Descent(2639/2999): loss=0.38991738169417867, w0=0.009634221048207398, w1=-0.25473025147235057\n",
      "(250000,)\n",
      "Gradient Descent(2640/2999): loss=0.38991738169416995, w0=0.009634220985696676, w1=-0.2547302513196344\n",
      "(250000,)\n",
      "Gradient Descent(2641/2999): loss=0.3899173816941612, w0=0.009634220923364167, w1=-0.2547302511673536\n",
      "(250000,)\n",
      "Gradient Descent(2642/2999): loss=0.3899173816941526, w0=0.009634220861209356, w1=-0.2547302510155069\n",
      "(250000,)\n",
      "Gradient Descent(2643/2999): loss=0.3899173816941439, w0=0.009634220799231734, w1=-0.2547302508640931\n",
      "(250000,)\n",
      "Gradient Descent(2644/2999): loss=0.3899173816941353, w0=0.009634220737430798, w1=-0.254730250713111\n",
      "(250000,)\n",
      "Gradient Descent(2645/2999): loss=0.38991738169412676, w0=0.009634220675806045, w1=-0.25473025056255927\n",
      "(250000,)\n",
      "Gradient Descent(2646/2999): loss=0.3899173816941182, w0=0.009634220614356976, w1=-0.25473025041243674\n",
      "(250000,)\n",
      "Gradient Descent(2647/2999): loss=0.38991738169410983, w0=0.009634220553083087, w1=-0.2547302502627422\n",
      "(250000,)\n",
      "Gradient Descent(2648/2999): loss=0.3899173816941014, w0=0.009634220491983885, w1=-0.2547302501134744\n",
      "(250000,)\n",
      "Gradient Descent(2649/2999): loss=0.3899173816940931, w0=0.009634220431058854, w1=-0.2547302499646321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(2650/2999): loss=0.3899173816940848, w0=0.00963422037030752, w1=-0.25473024981621417\n",
      "(250000,)\n",
      "Gradient Descent(2651/2999): loss=0.3899173816940766, w0=0.009634220309729371, w1=-0.25473024966821933\n",
      "(250000,)\n",
      "Gradient Descent(2652/2999): loss=0.3899173816940684, w0=0.009634220249323915, w1=-0.25473024952064643\n",
      "(250000,)\n",
      "Gradient Descent(2653/2999): loss=0.38991738169406015, w0=0.009634220189090662, w1=-0.2547302493734942\n",
      "(250000,)\n",
      "Gradient Descent(2654/2999): loss=0.389917381694052, w0=0.009634220129029127, w1=-0.2547302492267615\n",
      "(250000,)\n",
      "Gradient Descent(2655/2999): loss=0.38991738169404405, w0=0.009634220069138812, w1=-0.2547302490804471\n",
      "(250000,)\n",
      "Gradient Descent(2656/2999): loss=0.38991738169403606, w0=0.009634220009419233, w1=-0.2547302489345498\n",
      "(250000,)\n",
      "Gradient Descent(2657/2999): loss=0.389917381694028, w0=0.009634219949869903, w1=-0.25473024878906847\n",
      "(250000,)\n",
      "Gradient Descent(2658/2999): loss=0.3899173816940202, w0=0.009634219890490339, w1=-0.25473024864400184\n",
      "(250000,)\n",
      "Gradient Descent(2659/2999): loss=0.3899173816940123, w0=0.009634219831280053, w1=-0.2547302484993488\n",
      "(250000,)\n",
      "Gradient Descent(2660/2999): loss=0.38991738169400453, w0=0.009634219772238562, w1=-0.2547302483551081\n",
      "(250000,)\n",
      "Gradient Descent(2661/2999): loss=0.38991738169399665, w0=0.009634219713365394, w1=-0.2547302482112786\n",
      "(250000,)\n",
      "Gradient Descent(2662/2999): loss=0.389917381693989, w0=0.009634219654660065, w1=-0.25473024806785916\n",
      "(250000,)\n",
      "Gradient Descent(2663/2999): loss=0.38991738169398116, w0=0.009634219596122097, w1=-0.25473024792484855\n",
      "(250000,)\n",
      "Gradient Descent(2664/2999): loss=0.3899173816939736, w0=0.009634219537751001, w1=-0.2547302477822457\n",
      "(250000,)\n",
      "Gradient Descent(2665/2999): loss=0.3899173816939659, w0=0.009634219479546317, w1=-0.2547302476400493\n",
      "(250000,)\n",
      "Gradient Descent(2666/2999): loss=0.38991738169395834, w0=0.009634219421507561, w1=-0.25473024749825834\n",
      "(250000,)\n",
      "Gradient Descent(2667/2999): loss=0.38991738169395085, w0=0.009634219363634275, w1=-0.25473024735687155\n",
      "(250000,)\n",
      "Gradient Descent(2668/2999): loss=0.38991738169394347, w0=0.00963421930592596, w1=-0.2547302472158879\n",
      "(250000,)\n",
      "Gradient Descent(2669/2999): loss=0.3899173816939359, w0=0.009634219248382165, w1=-0.2547302470753061\n",
      "(250000,)\n",
      "Gradient Descent(2670/2999): loss=0.3899173816939285, w0=0.009634219191002415, w1=-0.25473024693512514\n",
      "(250000,)\n",
      "Gradient Descent(2671/2999): loss=0.38991738169392115, w0=0.009634219133786244, w1=-0.2547302467953438\n",
      "(250000,)\n",
      "Gradient Descent(2672/2999): loss=0.38991738169391377, w0=0.009634219076733196, w1=-0.2547302466559609\n",
      "(250000,)\n",
      "Gradient Descent(2673/2999): loss=0.3899173816939066, w0=0.009634219019842795, w1=-0.25473024651697534\n",
      "(250000,)\n",
      "Gradient Descent(2674/2999): loss=0.3899173816938994, w0=0.009634218963114576, w1=-0.25473024637838604\n",
      "(250000,)\n",
      "Gradient Descent(2675/2999): loss=0.3899173816938921, w0=0.009634218906548076, w1=-0.2547302462401918\n",
      "(250000,)\n",
      "Gradient Descent(2676/2999): loss=0.38991738169388507, w0=0.009634218850142838, w1=-0.2547302461023916\n",
      "(250000,)\n",
      "Gradient Descent(2677/2999): loss=0.38991738169387774, w0=0.0096342187938984, w1=-0.25473024596498417\n",
      "(250000,)\n",
      "Gradient Descent(2678/2999): loss=0.3899173816938708, w0=0.009634218737814303, w1=-0.2547302458279685\n",
      "(250000,)\n",
      "Gradient Descent(2679/2999): loss=0.38991738169386375, w0=0.00963421868189009, w1=-0.2547302456913434\n",
      "(250000,)\n",
      "Gradient Descent(2680/2999): loss=0.38991738169385676, w0=0.009634218626125308, w1=-0.2547302455551078\n",
      "(250000,)\n",
      "Gradient Descent(2681/2999): loss=0.3899173816938499, w0=0.009634218570519496, w1=-0.25473024541926065\n",
      "(250000,)\n",
      "Gradient Descent(2682/2999): loss=0.38991738169384293, w0=0.009634218515072207, w1=-0.2547302452838007\n",
      "(250000,)\n",
      "Gradient Descent(2683/2999): loss=0.38991738169383605, w0=0.009634218459782995, w1=-0.254730245148727\n",
      "(250000,)\n",
      "Gradient Descent(2684/2999): loss=0.3899173816938293, w0=0.009634218404651396, w1=-0.2547302450140383\n",
      "(250000,)\n",
      "Gradient Descent(2685/2999): loss=0.3899173816938225, w0=0.009634218349676975, w1=-0.25473024487973356\n",
      "(250000,)\n",
      "Gradient Descent(2686/2999): loss=0.3899173816938156, w0=0.009634218294859274, w1=-0.25473024474581174\n",
      "(250000,)\n",
      "Gradient Descent(2687/2999): loss=0.38991738169380896, w0=0.00963421824019785, w1=-0.2547302446122717\n",
      "(250000,)\n",
      "Gradient Descent(2688/2999): loss=0.3899173816938023, w0=0.009634218185692255, w1=-0.25473024447911236\n",
      "(250000,)\n",
      "Gradient Descent(2689/2999): loss=0.38991738169379564, w0=0.009634218131342043, w1=-0.2547302443463326\n",
      "(250000,)\n",
      "Gradient Descent(2690/2999): loss=0.389917381693789, w0=0.00963421807714678, w1=-0.2547302442139314\n",
      "(250000,)\n",
      "Gradient Descent(2691/2999): loss=0.38991738169378254, w0=0.009634218023106014, w1=-0.2547302440819077\n",
      "(250000,)\n",
      "Gradient Descent(2692/2999): loss=0.38991738169377604, w0=0.009634217969219308, w1=-0.25473024395026034\n",
      "(250000,)\n",
      "Gradient Descent(2693/2999): loss=0.38991738169376944, w0=0.009634217915486219, w1=-0.25473024381898823\n",
      "(250000,)\n",
      "Gradient Descent(2694/2999): loss=0.389917381693763, w0=0.009634217861906312, w1=-0.2547302436880904\n",
      "(250000,)\n",
      "Gradient Descent(2695/2999): loss=0.38991738169375667, w0=0.009634217808479156, w1=-0.2547302435575657\n",
      "(250000,)\n",
      "Gradient Descent(2696/2999): loss=0.38991738169375023, w0=0.009634217755204313, w1=-0.2547302434274131\n",
      "(250000,)\n",
      "Gradient Descent(2697/2999): loss=0.38991738169374385, w0=0.009634217702081341, w1=-0.2547302432976316\n",
      "(250000,)\n",
      "Gradient Descent(2698/2999): loss=0.38991738169373763, w0=0.009634217649109815, w1=-0.25473024316822007\n",
      "(250000,)\n",
      "Gradient Descent(2699/2999): loss=0.38991738169373136, w0=0.009634217596289303, w1=-0.25473024303917746\n",
      "(250000,)\n",
      "Gradient Descent(2700/2999): loss=0.3899173816937251, w0=0.009634217543619374, w1=-0.2547302429105027\n",
      "(250000,)\n",
      "Gradient Descent(2701/2999): loss=0.3899173816937189, w0=0.009634217491099597, w1=-0.2547302427821948\n",
      "(250000,)\n",
      "Gradient Descent(2702/2999): loss=0.3899173816937127, w0=0.009634217438729538, w1=-0.25473024265425265\n",
      "(250000,)\n",
      "Gradient Descent(2703/2999): loss=0.38991738169370666, w0=0.009634217386508772, w1=-0.25473024252667525\n",
      "(250000,)\n",
      "Gradient Descent(2704/2999): loss=0.3899173816937005, w0=0.00963421733443688, w1=-0.25473024239946157\n",
      "(250000,)\n",
      "Gradient Descent(2705/2999): loss=0.3899173816936944, w0=0.009634217282513442, w1=-0.25473024227261054\n",
      "(250000,)\n",
      "Gradient Descent(2706/2999): loss=0.3899173816936884, w0=0.009634217230738025, w1=-0.2547302421461211\n",
      "(250000,)\n",
      "Gradient Descent(2707/2999): loss=0.38991738169368245, w0=0.00963421717911022, w1=-0.2547302420199923\n",
      "(250000,)\n",
      "Gradient Descent(2708/2999): loss=0.38991738169367657, w0=0.009634217127629588, w1=-0.25473024189422305\n",
      "(250000,)\n",
      "Gradient Descent(2709/2999): loss=0.3899173816936706, w0=0.009634217076295713, w1=-0.25473024176881237\n",
      "(250000,)\n",
      "Gradient Descent(2710/2999): loss=0.38991738169366463, w0=0.009634217025108187, w1=-0.2547302416437592\n",
      "(250000,)\n",
      "Gradient Descent(2711/2999): loss=0.38991738169365886, w0=0.009634216974066589, w1=-0.25473024151906254\n",
      "(250000,)\n",
      "Gradient Descent(2712/2999): loss=0.3899173816936529, w0=0.009634216923170498, w1=-0.25473024139472134\n",
      "(250000,)\n",
      "Gradient Descent(2713/2999): loss=0.3899173816936472, w0=0.009634216872419499, w1=-0.2547302412707346\n",
      "(250000,)\n",
      "Gradient Descent(2714/2999): loss=0.38991738169364143, w0=0.00963421682181318, w1=-0.25473024114710136\n",
      "(250000,)\n",
      "Gradient Descent(2715/2999): loss=0.3899173816936357, w0=0.009634216771351132, w1=-0.2547302410238206\n",
      "(250000,)\n",
      "Gradient Descent(2716/2999): loss=0.38991738169363, w0=0.009634216721032946, w1=-0.25473024090089125\n",
      "(250000,)\n",
      "Gradient Descent(2717/2999): loss=0.38991738169362433, w0=0.009634216670858214, w1=-0.25473024077831236\n",
      "(250000,)\n",
      "Gradient Descent(2718/2999): loss=0.3899173816936188, w0=0.009634216620826518, w1=-0.2547302406560829\n",
      "(250000,)\n",
      "Gradient Descent(2719/2999): loss=0.3899173816936131, w0=0.009634216570937451, w1=-0.2547302405342019\n",
      "(250000,)\n",
      "Gradient Descent(2720/2999): loss=0.38991738169360757, w0=0.009634216521190607, w1=-0.2547302404126684\n",
      "(250000,)\n",
      "Gradient Descent(2721/2999): loss=0.38991738169360207, w0=0.00963421647158558, w1=-0.25473024029148134\n",
      "(250000,)\n",
      "Gradient Descent(2722/2999): loss=0.3899173816935966, w0=0.009634216422121969, w1=-0.2547302401706398\n",
      "(250000,)\n",
      "Gradient Descent(2723/2999): loss=0.3899173816935911, w0=0.009634216372799369, w1=-0.2547302400501427\n",
      "(250000,)\n",
      "Gradient Descent(2724/2999): loss=0.38991738169358564, w0=0.00963421632361738, w1=-0.25473023992998917\n",
      "(250000,)\n",
      "Gradient Descent(2725/2999): loss=0.3899173816935803, w0=0.009634216274575603, w1=-0.2547302398101781\n",
      "(250000,)\n",
      "Gradient Descent(2726/2999): loss=0.3899173816935748, w0=0.009634216225673632, w1=-0.2547302396907086\n",
      "(250000,)\n",
      "Gradient Descent(2727/2999): loss=0.3899173816935696, w0=0.009634216176911072, w1=-0.25473023957157975\n",
      "(250000,)\n",
      "Gradient Descent(2728/2999): loss=0.38991738169356416, w0=0.00963421612828753, w1=-0.2547302394527905\n",
      "(250000,)\n",
      "Gradient Descent(2729/2999): loss=0.389917381693559, w0=0.009634216079802601, w1=-0.25473023933433986\n",
      "(250000,)\n",
      "Gradient Descent(2730/2999): loss=0.38991738169355367, w0=0.009634216031455894, w1=-0.2547302392162269\n",
      "(250000,)\n",
      "Gradient Descent(2731/2999): loss=0.38991738169354845, w0=0.009634215983247006, w1=-0.25473023909845066\n",
      "(250000,)\n",
      "Gradient Descent(2732/2999): loss=0.38991738169354323, w0=0.009634215935175558, w1=-0.2547302389810102\n",
      "(250000,)\n",
      "Gradient Descent(2733/2999): loss=0.38991738169353807, w0=0.009634215887241147, w1=-0.25473023886390456\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2734/2999): loss=0.38991738169353296, w0=0.00963421583944339, w1=-0.25473023874713274\n",
      "(250000,)\n",
      "Gradient Descent(2735/2999): loss=0.38991738169352785, w0=0.009634215791781893, w1=-0.25473023863069383\n",
      "(250000,)\n",
      "Gradient Descent(2736/2999): loss=0.3899173816935228, w0=0.009634215744256278, w1=-0.25473023851458687\n",
      "(250000,)\n",
      "Gradient Descent(2737/2999): loss=0.3899173816935177, w0=0.009634215696866153, w1=-0.25473023839881087\n",
      "(250000,)\n",
      "Gradient Descent(2738/2999): loss=0.3899173816935127, w0=0.009634215649611123, w1=-0.25473023828336494\n",
      "(250000,)\n",
      "Gradient Descent(2739/2999): loss=0.3899173816935077, w0=0.009634215602490805, w1=-0.25473023816824814\n",
      "(250000,)\n",
      "Gradient Descent(2740/2999): loss=0.3899173816935027, w0=0.009634215555504819, w1=-0.2547302380534595\n",
      "(250000,)\n",
      "Gradient Descent(2741/2999): loss=0.3899173816934979, w0=0.009634215508652781, w1=-0.2547302379389981\n",
      "(250000,)\n",
      "Gradient Descent(2742/2999): loss=0.3899173816934929, w0=0.009634215461934305, w1=-0.25473023782486304\n",
      "(250000,)\n",
      "Gradient Descent(2743/2999): loss=0.38991738169348805, w0=0.009634215415349015, w1=-0.25473023771105335\n",
      "(250000,)\n",
      "Gradient Descent(2744/2999): loss=0.3899173816934833, w0=0.009634215368896538, w1=-0.2547302375975681\n",
      "(250000,)\n",
      "Gradient Descent(2745/2999): loss=0.38991738169347845, w0=0.009634215322576495, w1=-0.2547302374844064\n",
      "(250000,)\n",
      "Gradient Descent(2746/2999): loss=0.3899173816934735, w0=0.0096342152763885, w1=-0.25473023737156725\n",
      "(250000,)\n",
      "Gradient Descent(2747/2999): loss=0.3899173816934687, w0=0.009634215230332177, w1=-0.2547302372590498\n",
      "(250000,)\n",
      "Gradient Descent(2748/2999): loss=0.3899173816934641, w0=0.009634215184407149, w1=-0.2547302371468531\n",
      "(250000,)\n",
      "Gradient Descent(2749/2999): loss=0.38991738169345935, w0=0.009634215138613046, w1=-0.25473023703497627\n",
      "(250000,)\n",
      "Gradient Descent(2750/2999): loss=0.3899173816934547, w0=0.009634215092949492, w1=-0.2547302369234184\n",
      "(250000,)\n",
      "Gradient Descent(2751/2999): loss=0.38991738169345, w0=0.009634215047416114, w1=-0.25473023681217855\n",
      "(250000,)\n",
      "Gradient Descent(2752/2999): loss=0.38991738169344536, w0=0.009634215002012543, w1=-0.25473023670125583\n",
      "(250000,)\n",
      "Gradient Descent(2753/2999): loss=0.3899173816934407, w0=0.009634214956738416, w1=-0.2547302365906493\n",
      "(250000,)\n",
      "Gradient Descent(2754/2999): loss=0.38991738169343615, w0=0.009634214911593348, w1=-0.25473023648035814\n",
      "(250000,)\n",
      "Gradient Descent(2755/2999): loss=0.3899173816934315, w0=0.009634214866576988, w1=-0.2547302363703814\n",
      "(250000,)\n",
      "Gradient Descent(2756/2999): loss=0.389917381693427, w0=0.009634214821688957, w1=-0.2547302362607181\n",
      "(250000,)\n",
      "Gradient Descent(2757/2999): loss=0.3899173816934226, w0=0.009634214776928897, w1=-0.2547302361513675\n",
      "(250000,)\n",
      "Gradient Descent(2758/2999): loss=0.389917381693418, w0=0.00963421473229643, w1=-0.2547302360423286\n",
      "(250000,)\n",
      "Gradient Descent(2759/2999): loss=0.38991738169341356, w0=0.009634214687791205, w1=-0.25473023593360056\n",
      "(250000,)\n",
      "Gradient Descent(2760/2999): loss=0.3899173816934091, w0=0.009634214643412851, w1=-0.2547302358251825\n",
      "(250000,)\n",
      "Gradient Descent(2761/2999): loss=0.3899173816934048, w0=0.009634214599161021, w1=-0.25473023571707354\n",
      "(250000,)\n",
      "Gradient Descent(2762/2999): loss=0.38991738169340046, w0=0.009634214555035353, w1=-0.25473023560927277\n",
      "(250000,)\n",
      "Gradient Descent(2763/2999): loss=0.3899173816933961, w0=0.009634214511035475, w1=-0.2547302355017793\n",
      "(250000,)\n",
      "Gradient Descent(2764/2999): loss=0.38991738169339174, w0=0.009634214467161025, w1=-0.2547302353945923\n",
      "(250000,)\n",
      "Gradient Descent(2765/2999): loss=0.38991738169338747, w0=0.009634214423411653, w1=-0.2547302352877108\n",
      "(250000,)\n",
      "Gradient Descent(2766/2999): loss=0.3899173816933832, w0=0.009634214379787004, w1=-0.2547302351811341\n",
      "(250000,)\n",
      "Gradient Descent(2767/2999): loss=0.38991738169337886, w0=0.009634214336286724, w1=-0.25473023507486114\n",
      "(250000,)\n",
      "Gradient Descent(2768/2999): loss=0.38991738169337464, w0=0.00963421429291045, w1=-0.2547302349688912\n",
      "(250000,)\n",
      "Gradient Descent(2769/2999): loss=0.38991738169337037, w0=0.009634214249657838, w1=-0.2547302348632233\n",
      "(250000,)\n",
      "Gradient Descent(2770/2999): loss=0.38991738169336626, w0=0.009634214206528529, w1=-0.2547302347578567\n",
      "(250000,)\n",
      "Gradient Descent(2771/2999): loss=0.3899173816933621, w0=0.009634214163522174, w1=-0.2547302346527905\n",
      "(250000,)\n",
      "Gradient Descent(2772/2999): loss=0.389917381693358, w0=0.009634214120638428, w1=-0.2547302345480238\n",
      "(250000,)\n",
      "Gradient Descent(2773/2999): loss=0.3899173816933538, w0=0.009634214077876943, w1=-0.25473023444355575\n",
      "(250000,)\n",
      "Gradient Descent(2774/2999): loss=0.38991738169334983, w0=0.009634214035237355, w1=-0.2547302343393855\n",
      "(250000,)\n",
      "Gradient Descent(2775/2999): loss=0.3899173816933457, w0=0.00963421399271933, w1=-0.2547302342355123\n",
      "(250000,)\n",
      "Gradient Descent(2776/2999): loss=0.3899173816933417, w0=0.009634213950322509, w1=-0.25473023413193513\n",
      "(250000,)\n",
      "Gradient Descent(2777/2999): loss=0.3899173816933377, w0=0.009634213908046557, w1=-0.2547302340286533\n",
      "(250000,)\n",
      "Gradient Descent(2778/2999): loss=0.3899173816933336, w0=0.009634213865891125, w1=-0.2547302339256659\n",
      "(250000,)\n",
      "Gradient Descent(2779/2999): loss=0.3899173816933297, w0=0.009634213823855875, w1=-0.2547302338229721\n",
      "(250000,)\n",
      "Gradient Descent(2780/2999): loss=0.38991738169332574, w0=0.00963421378194046, w1=-0.254730233720571\n",
      "(250000,)\n",
      "Gradient Descent(2781/2999): loss=0.38991738169332185, w0=0.009634213740144535, w1=-0.2547302336184619\n",
      "(250000,)\n",
      "Gradient Descent(2782/2999): loss=0.38991738169331785, w0=0.009634213698467757, w1=-0.25473023351664387\n",
      "(250000,)\n",
      "Gradient Descent(2783/2999): loss=0.389917381693314, w0=0.009634213656909791, w1=-0.2547302334151161\n",
      "(250000,)\n",
      "Gradient Descent(2784/2999): loss=0.3899173816933102, w0=0.009634213615470295, w1=-0.25473023331387773\n",
      "(250000,)\n",
      "Gradient Descent(2785/2999): loss=0.3899173816933063, w0=0.009634213574148945, w1=-0.25473023321292804\n",
      "(250000,)\n",
      "Gradient Descent(2786/2999): loss=0.38991738169330253, w0=0.00963421353294539, w1=-0.2547302331122661\n",
      "(250000,)\n",
      "Gradient Descent(2787/2999): loss=0.38991738169329876, w0=0.0096342134918593, w1=-0.2547302330118911\n",
      "(250000,)\n",
      "Gradient Descent(2788/2999): loss=0.38991738169329493, w0=0.009634213450890345, w1=-0.2547302329118023\n",
      "(250000,)\n",
      "Gradient Descent(2789/2999): loss=0.3899173816932912, w0=0.009634213410038181, w1=-0.25473023281199886\n",
      "(250000,)\n",
      "Gradient Descent(2790/2999): loss=0.3899173816932874, w0=0.009634213369302485, w1=-0.2547302327124799\n",
      "(250000,)\n",
      "Gradient Descent(2791/2999): loss=0.38991738169328377, w0=0.009634213328682915, w1=-0.2547302326132447\n",
      "(250000,)\n",
      "Gradient Descent(2792/2999): loss=0.38991738169328005, w0=0.009634213288179141, w1=-0.25473023251429233\n",
      "(250000,)\n",
      "Gradient Descent(2793/2999): loss=0.3899173816932764, w0=0.009634213247790832, w1=-0.2547302324156221\n",
      "(250000,)\n",
      "Gradient Descent(2794/2999): loss=0.3899173816932728, w0=0.009634213207517662, w1=-0.25473023231723313\n",
      "(250000,)\n",
      "Gradient Descent(2795/2999): loss=0.3899173816932691, w0=0.009634213167359304, w1=-0.25473023221912466\n",
      "(250000,)\n",
      "Gradient Descent(2796/2999): loss=0.3899173816932655, w0=0.009634213127315435, w1=-0.25473023212129586\n",
      "(250000,)\n",
      "Gradient Descent(2797/2999): loss=0.3899173816932619, w0=0.00963421308738572, w1=-0.25473023202374595\n",
      "(250000,)\n",
      "Gradient Descent(2798/2999): loss=0.38991738169325835, w0=0.009634213047569833, w1=-0.25473023192647415\n",
      "(250000,)\n",
      "Gradient Descent(2799/2999): loss=0.3899173816932548, w0=0.00963421300786746, w1=-0.2547302318294797\n",
      "(250000,)\n",
      "Gradient Descent(2800/2999): loss=0.3899173816932513, w0=0.009634212968278271, w1=-0.2547302317327617\n",
      "(250000,)\n",
      "Gradient Descent(2801/2999): loss=0.3899173816932478, w0=0.009634212928801943, w1=-0.25473023163631947\n",
      "(250000,)\n",
      "Gradient Descent(2802/2999): loss=0.38991738169324425, w0=0.009634212889438153, w1=-0.2547302315401522\n",
      "(250000,)\n",
      "Gradient Descent(2803/2999): loss=0.3899173816932408, w0=0.009634212850186586, w1=-0.254730231444259\n",
      "(250000,)\n",
      "Gradient Descent(2804/2999): loss=0.3899173816932374, w0=0.009634212811046922, w1=-0.2547302313486392\n",
      "(250000,)\n",
      "Gradient Descent(2805/2999): loss=0.389917381693234, w0=0.009634212772018827, w1=-0.254730231253292\n",
      "(250000,)\n",
      "Gradient Descent(2806/2999): loss=0.3899173816932306, w0=0.009634212733101994, w1=-0.25473023115821664\n",
      "(250000,)\n",
      "Gradient Descent(2807/2999): loss=0.3899173816932272, w0=0.009634212694296103, w1=-0.2547302310634123\n",
      "(250000,)\n",
      "Gradient Descent(2808/2999): loss=0.38991738169322376, w0=0.009634212655600848, w1=-0.25473023096887826\n",
      "(250000,)\n",
      "Gradient Descent(2809/2999): loss=0.3899173816932205, w0=0.009634212617015907, w1=-0.2547302308746137\n",
      "(250000,)\n",
      "Gradient Descent(2810/2999): loss=0.38991738169321705, w0=0.009634212578540956, w1=-0.2547302307806179\n",
      "(250000,)\n",
      "Gradient Descent(2811/2999): loss=0.3899173816932139, w0=0.009634212540175699, w1=-0.25473023068689005\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2812/2999): loss=0.38991738169321044, w0=0.009634212501919813, w1=-0.25473023059342936\n",
      "(250000,)\n",
      "Gradient Descent(2813/2999): loss=0.3899173816932073, w0=0.00963421246377299, w1=-0.25473023050023513\n",
      "(250000,)\n",
      "Gradient Descent(2814/2999): loss=0.3899173816932039, w0=0.009634212425734917, w1=-0.2547302304073066\n",
      "(250000,)\n",
      "Gradient Descent(2815/2999): loss=0.3899173816932007, w0=0.009634212387805279, w1=-0.254730230314643\n",
      "(250000,)\n",
      "Gradient Descent(2816/2999): loss=0.3899173816931975, w0=0.009634212349983778, w1=-0.2547302302222435\n",
      "(250000,)\n",
      "Gradient Descent(2817/2999): loss=0.3899173816931943, w0=0.009634212312270097, w1=-0.2547302301301075\n",
      "(250000,)\n",
      "Gradient Descent(2818/2999): loss=0.3899173816931911, w0=0.009634212274663935, w1=-0.2547302300382341\n",
      "(250000,)\n",
      "Gradient Descent(2819/2999): loss=0.3899173816931879, w0=0.009634212237164975, w1=-0.25473022994662264\n",
      "(250000,)\n",
      "Gradient Descent(2820/2999): loss=0.38991738169318474, w0=0.009634212199772915, w1=-0.2547302298552723\n",
      "(250000,)\n",
      "Gradient Descent(2821/2999): loss=0.3899173816931817, w0=0.009634212162487445, w1=-0.25473022976418247\n",
      "(250000,)\n",
      "Gradient Descent(2822/2999): loss=0.3899173816931786, w0=0.009634212125308279, w1=-0.2547302296733523\n",
      "(250000,)\n",
      "Gradient Descent(2823/2999): loss=0.3899173816931755, w0=0.009634212088235103, w1=-0.254730229582781\n",
      "(250000,)\n",
      "Gradient Descent(2824/2999): loss=0.3899173816931724, w0=0.009634212051267613, w1=-0.254730229492468\n",
      "(250000,)\n",
      "Gradient Descent(2825/2999): loss=0.3899173816931695, w0=0.009634212014405511, w1=-0.2547302294024124\n",
      "(250000,)\n",
      "Gradient Descent(2826/2999): loss=0.3899173816931663, w0=0.009634211977648494, w1=-0.2547302293126135\n",
      "(250000,)\n",
      "Gradient Descent(2827/2999): loss=0.3899173816931633, w0=0.00963421194099626, w1=-0.25473022922307065\n",
      "(250000,)\n",
      "Gradient Descent(2828/2999): loss=0.38991738169316026, w0=0.009634211904448515, w1=-0.25473022913378307\n",
      "(250000,)\n",
      "Gradient Descent(2829/2999): loss=0.3899173816931575, w0=0.009634211868004958, w1=-0.25473022904475\n",
      "(250000,)\n",
      "Gradient Descent(2830/2999): loss=0.38991738169315426, w0=0.0096342118316653, w1=-0.2547302289559708\n",
      "(250000,)\n",
      "Gradient Descent(2831/2999): loss=0.3899173816931514, w0=0.009634211795429237, w1=-0.2547302288674447\n",
      "(250000,)\n",
      "Gradient Descent(2832/2999): loss=0.3899173816931485, w0=0.009634211759296483, w1=-0.2547302287791709\n",
      "(250000,)\n",
      "Gradient Descent(2833/2999): loss=0.3899173816931455, w0=0.009634211723266744, w1=-0.2547302286911488\n",
      "(250000,)\n",
      "Gradient Descent(2834/2999): loss=0.38991738169314255, w0=0.009634211687339713, w1=-0.25473022860337763\n",
      "(250000,)\n",
      "Gradient Descent(2835/2999): loss=0.3899173816931397, w0=0.00963421165151511, w1=-0.2547302285158567\n",
      "(250000,)\n",
      "Gradient Descent(2836/2999): loss=0.38991738169313683, w0=0.00963421161579263, w1=-0.2547302284285852\n",
      "(250000,)\n",
      "Gradient Descent(2837/2999): loss=0.38991738169313395, w0=0.009634211580171982, w1=-0.25473022834156256\n",
      "(250000,)\n",
      "Gradient Descent(2838/2999): loss=0.38991738169313117, w0=0.009634211544652887, w1=-0.25473022825478797\n",
      "(250000,)\n",
      "Gradient Descent(2839/2999): loss=0.38991738169312834, w0=0.009634211509235057, w1=-0.2547302281682608\n",
      "(250000,)\n",
      "Gradient Descent(2840/2999): loss=0.38991738169312556, w0=0.009634211473918197, w1=-0.25473022808198026\n",
      "(250000,)\n",
      "Gradient Descent(2841/2999): loss=0.3899173816931228, w0=0.009634211438702016, w1=-0.2547302279959457\n",
      "(250000,)\n",
      "Gradient Descent(2842/2999): loss=0.38991738169311996, w0=0.009634211403586229, w1=-0.2547302279101564\n",
      "(250000,)\n",
      "Gradient Descent(2843/2999): loss=0.3899173816931172, w0=0.009634211368570553, w1=-0.25473022782461174\n",
      "(250000,)\n",
      "Gradient Descent(2844/2999): loss=0.38991738169311446, w0=0.009634211333654701, w1=-0.2547302277393109\n",
      "(250000,)\n",
      "Gradient Descent(2845/2999): loss=0.3899173816931118, w0=0.009634211298838388, w1=-0.25473022765425324\n",
      "(250000,)\n",
      "Gradient Descent(2846/2999): loss=0.389917381693109, w0=0.009634211264121331, w1=-0.2547302275694381\n",
      "(250000,)\n",
      "Gradient Descent(2847/2999): loss=0.3899173816931063, w0=0.00963421122950324, w1=-0.2547302274848647\n",
      "(250000,)\n",
      "Gradient Descent(2848/2999): loss=0.38991738169310364, w0=0.009634211194983843, w1=-0.2547302274005324\n",
      "(250000,)\n",
      "Gradient Descent(2849/2999): loss=0.38991738169310103, w0=0.009634211160562857, w1=-0.2547302273164405\n",
      "(250000,)\n",
      "Gradient Descent(2850/2999): loss=0.3899173816930983, w0=0.009634211126239993, w1=-0.25473022723258837\n",
      "(250000,)\n",
      "Gradient Descent(2851/2999): loss=0.38991738169309564, w0=0.00963421109201498, w1=-0.2547302271489753\n",
      "(250000,)\n",
      "Gradient Descent(2852/2999): loss=0.3899173816930931, w0=0.009634211057887535, w1=-0.25473022706560056\n",
      "(250000,)\n",
      "Gradient Descent(2853/2999): loss=0.38991738169309037, w0=0.009634211023857383, w1=-0.2547302269824635\n",
      "(250000,)\n",
      "Gradient Descent(2854/2999): loss=0.3899173816930878, w0=0.009634210989924235, w1=-0.2547302268995635\n",
      "(250000,)\n",
      "Gradient Descent(2855/2999): loss=0.3899173816930852, w0=0.009634210956087833, w1=-0.2547302268168998\n",
      "(250000,)\n",
      "Gradient Descent(2856/2999): loss=0.38991738169308265, w0=0.009634210922347885, w1=-0.25473022673447177\n",
      "(250000,)\n",
      "Gradient Descent(2857/2999): loss=0.38991738169308016, w0=0.009634210888704123, w1=-0.25473022665227874\n",
      "(250000,)\n",
      "Gradient Descent(2858/2999): loss=0.38991738169307766, w0=0.00963421085515628, w1=-0.25473022657032\n",
      "(250000,)\n",
      "Gradient Descent(2859/2999): loss=0.38991738169307516, w0=0.009634210821704077, w1=-0.25473022648859495\n",
      "(250000,)\n",
      "Gradient Descent(2860/2999): loss=0.38991738169307266, w0=0.009634210788347244, w1=-0.25473022640710286\n",
      "(250000,)\n",
      "Gradient Descent(2861/2999): loss=0.38991738169307016, w0=0.009634210755085497, w1=-0.2547302263258431\n",
      "(250000,)\n",
      "Gradient Descent(2862/2999): loss=0.3899173816930676, w0=0.009634210721918576, w1=-0.25473022624481495\n",
      "(250000,)\n",
      "Gradient Descent(2863/2999): loss=0.3899173816930652, w0=0.009634210688846206, w1=-0.2547302261640178\n",
      "(250000,)\n",
      "Gradient Descent(2864/2999): loss=0.3899173816930627, w0=0.009634210655868116, w1=-0.25473022608345103\n",
      "(250000,)\n",
      "Gradient Descent(2865/2999): loss=0.38991738169306045, w0=0.009634210622984045, w1=-0.2547302260031139\n",
      "(250000,)\n",
      "Gradient Descent(2866/2999): loss=0.389917381693058, w0=0.00963421059019372, w1=-0.2547302259230058\n",
      "(250000,)\n",
      "Gradient Descent(2867/2999): loss=0.38991738169305545, w0=0.009634210557496876, w1=-0.2547302258431261\n",
      "(250000,)\n",
      "Gradient Descent(2868/2999): loss=0.3899173816930531, w0=0.00963421052489324, w1=-0.2547302257634741\n",
      "(250000,)\n",
      "Gradient Descent(2869/2999): loss=0.3899173816930508, w0=0.009634210492382552, w1=-0.2547302256840492\n",
      "(250000,)\n",
      "Gradient Descent(2870/2999): loss=0.3899173816930484, w0=0.009634210459964541, w1=-0.2547302256048507\n",
      "(250000,)\n",
      "Gradient Descent(2871/2999): loss=0.38991738169304613, w0=0.00963421042763895, w1=-0.254730225525878\n",
      "(250000,)\n",
      "Gradient Descent(2872/2999): loss=0.38991738169304374, w0=0.009634210395405515, w1=-0.2547302254471304\n",
      "(250000,)\n",
      "Gradient Descent(2873/2999): loss=0.3899173816930414, w0=0.009634210363263972, w1=-0.25473022536860734\n",
      "(250000,)\n",
      "Gradient Descent(2874/2999): loss=0.389917381693039, w0=0.009634210331214055, w1=-0.2547302252903081\n",
      "(250000,)\n",
      "Gradient Descent(2875/2999): loss=0.38991738169303664, w0=0.009634210299255513, w1=-0.2547302252122321\n",
      "(250000,)\n",
      "Gradient Descent(2876/2999): loss=0.3899173816930345, w0=0.009634210267388072, w1=-0.25473022513437865\n",
      "(250000,)\n",
      "Gradient Descent(2877/2999): loss=0.38991738169303214, w0=0.009634210235611479, w1=-0.2547302250567472\n",
      "(250000,)\n",
      "Gradient Descent(2878/2999): loss=0.3899173816930299, w0=0.009634210203925471, w1=-0.254730224979337\n",
      "(250000,)\n",
      "Gradient Descent(2879/2999): loss=0.38991738169302764, w0=0.0096342101723298, w1=-0.25473022490214753\n",
      "(250000,)\n",
      "Gradient Descent(2880/2999): loss=0.3899173816930255, w0=0.009634210140824202, w1=-0.2547302248251781\n",
      "(250000,)\n",
      "Gradient Descent(2881/2999): loss=0.38991738169302315, w0=0.009634210109408427, w1=-0.2547302247484281\n",
      "(250000,)\n",
      "Gradient Descent(2882/2999): loss=0.3899173816930209, w0=0.00963421007808222, w1=-0.2547302246718969\n",
      "(250000,)\n",
      "Gradient Descent(2883/2999): loss=0.3899173816930188, w0=0.009634210046845315, w1=-0.25473022459558387\n",
      "(250000,)\n",
      "Gradient Descent(2884/2999): loss=0.38991738169301654, w0=0.009634210015697458, w1=-0.2547302245194884\n",
      "(250000,)\n",
      "Gradient Descent(2885/2999): loss=0.3899173816930144, w0=0.009634209984638402, w1=-0.2547302244436098\n",
      "(250000,)\n",
      "Gradient Descent(2886/2999): loss=0.3899173816930123, w0=0.009634209953667885, w1=-0.25473022436794757\n",
      "(250000,)\n",
      "Gradient Descent(2887/2999): loss=0.38991738169301016, w0=0.009634209922785655, w1=-0.25473022429250103\n",
      "(250000,)\n",
      "Gradient Descent(2888/2999): loss=0.389917381693008, w0=0.009634209891991456, w1=-0.2547302242172696\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2889/2999): loss=0.3899173816930058, w0=0.009634209861285057, w1=-0.2547302241422526\n",
      "(250000,)\n",
      "Gradient Descent(2890/2999): loss=0.3899173816930037, w0=0.009634209830666188, w1=-0.2547302240674495\n",
      "(250000,)\n",
      "Gradient Descent(2891/2999): loss=0.38991738169300166, w0=0.009634209800134609, w1=-0.25473022399285966\n",
      "(250000,)\n",
      "Gradient Descent(2892/2999): loss=0.3899173816929996, w0=0.009634209769690066, w1=-0.25473022391848243\n",
      "(250000,)\n",
      "Gradient Descent(2893/2999): loss=0.38991738169299756, w0=0.009634209739332317, w1=-0.25473022384431726\n",
      "(250000,)\n",
      "Gradient Descent(2894/2999): loss=0.38991738169299556, w0=0.00963420970906111, w1=-0.2547302237703635\n",
      "(250000,)\n",
      "Gradient Descent(2895/2999): loss=0.38991738169299345, w0=0.009634209678876203, w1=-0.2547302236966206\n",
      "(250000,)\n",
      "Gradient Descent(2896/2999): loss=0.3899173816929914, w0=0.009634209648777356, w1=-0.25473022362308784\n",
      "(250000,)\n",
      "Gradient Descent(2897/2999): loss=0.38991738169298934, w0=0.009634209618764314, w1=-0.25473022354976477\n",
      "(250000,)\n",
      "Gradient Descent(2898/2999): loss=0.38991738169298734, w0=0.009634209588836826, w1=-0.25473022347665075\n",
      "(250000,)\n",
      "Gradient Descent(2899/2999): loss=0.38991738169298534, w0=0.00963420955899466, w1=-0.2547302234037451\n",
      "(250000,)\n",
      "Gradient Descent(2900/2999): loss=0.3899173816929833, w0=0.009634209529237565, w1=-0.2547302233310474\n",
      "(250000,)\n",
      "Gradient Descent(2901/2999): loss=0.3899173816929814, w0=0.009634209499565307, w1=-0.2547302232585569\n",
      "(250000,)\n",
      "Gradient Descent(2902/2999): loss=0.3899173816929793, w0=0.009634209469977647, w1=-0.25473022318627303\n",
      "(250000,)\n",
      "Gradient Descent(2903/2999): loss=0.38991738169297735, w0=0.009634209440474334, w1=-0.25473022311419524\n",
      "(250000,)\n",
      "Gradient Descent(2904/2999): loss=0.3899173816929755, w0=0.00963420941105513, w1=-0.2547302230423229\n",
      "(250000,)\n",
      "Gradient Descent(2905/2999): loss=0.3899173816929735, w0=0.009634209381719792, w1=-0.25473022297065545\n",
      "(250000,)\n",
      "Gradient Descent(2906/2999): loss=0.38991738169297163, w0=0.009634209352468082, w1=-0.25473022289919234\n",
      "(250000,)\n",
      "Gradient Descent(2907/2999): loss=0.3899173816929697, w0=0.009634209323299758, w1=-0.25473022282793295\n",
      "(250000,)\n",
      "Gradient Descent(2908/2999): loss=0.3899173816929678, w0=0.009634209294214586, w1=-0.25473022275687673\n",
      "(250000,)\n",
      "Gradient Descent(2909/2999): loss=0.38991738169296597, w0=0.009634209265212336, w1=-0.2547302226860231\n",
      "(250000,)\n",
      "Gradient Descent(2910/2999): loss=0.3899173816929639, w0=0.009634209236292768, w1=-0.25473022261537137\n",
      "(250000,)\n",
      "Gradient Descent(2911/2999): loss=0.38991738169296214, w0=0.009634209207455648, w1=-0.2547302225449211\n",
      "(250000,)\n",
      "Gradient Descent(2912/2999): loss=0.3899173816929602, w0=0.009634209178700734, w1=-0.2547302224746717\n",
      "(250000,)\n",
      "Gradient Descent(2913/2999): loss=0.3899173816929584, w0=0.009634209150027792, w1=-0.25473022240462256\n",
      "(250000,)\n",
      "Gradient Descent(2914/2999): loss=0.38991738169295653, w0=0.0096342091214366, w1=-0.2547302223347731\n",
      "(250000,)\n",
      "Gradient Descent(2915/2999): loss=0.38991738169295465, w0=0.009634209092926912, w1=-0.25473022226512276\n",
      "(250000,)\n",
      "Gradient Descent(2916/2999): loss=0.3899173816929529, w0=0.009634209064498488, w1=-0.254730222195671\n",
      "(250000,)\n",
      "Gradient Descent(2917/2999): loss=0.389917381692951, w0=0.009634209036151102, w1=-0.2547302221264172\n",
      "(250000,)\n",
      "Gradient Descent(2918/2999): loss=0.3899173816929492, w0=0.009634209007884541, w1=-0.25473022205736084\n",
      "(250000,)\n",
      "Gradient Descent(2919/2999): loss=0.3899173816929476, w0=0.00963420897969856, w1=-0.25473022198850137\n",
      "(250000,)\n",
      "Gradient Descent(2920/2999): loss=0.3899173816929457, w0=0.009634208951592938, w1=-0.2547302219198382\n",
      "(250000,)\n",
      "Gradient Descent(2921/2999): loss=0.389917381692944, w0=0.009634208923567443, w1=-0.2547302218513708\n",
      "(250000,)\n",
      "Gradient Descent(2922/2999): loss=0.3899173816929422, w0=0.009634208895621842, w1=-0.25473022178309856\n",
      "(250000,)\n",
      "Gradient Descent(2923/2999): loss=0.3899173816929405, w0=0.009634208867755905, w1=-0.25473022171502097\n",
      "(250000,)\n",
      "Gradient Descent(2924/2999): loss=0.3899173816929387, w0=0.00963420883996941, w1=-0.25473022164713743\n",
      "(250000,)\n",
      "Gradient Descent(2925/2999): loss=0.389917381692937, w0=0.009634208812262127, w1=-0.2547302215794474\n",
      "(250000,)\n",
      "Gradient Descent(2926/2999): loss=0.38991738169293527, w0=0.009634208784633837, w1=-0.25473022151195035\n",
      "(250000,)\n",
      "Gradient Descent(2927/2999): loss=0.38991738169293355, w0=0.009634208757084308, w1=-0.25473022144464574\n",
      "(250000,)\n",
      "Gradient Descent(2928/2999): loss=0.38991738169293183, w0=0.00963420872961332, w1=-0.254730221377533\n",
      "(250000,)\n",
      "Gradient Descent(2929/2999): loss=0.3899173816929302, w0=0.009634208702220644, w1=-0.25473022131061157\n",
      "(250000,)\n",
      "Gradient Descent(2930/2999): loss=0.38991738169292856, w0=0.009634208674906056, w1=-0.25473022124388095\n",
      "(250000,)\n",
      "Gradient Descent(2931/2999): loss=0.3899173816929268, w0=0.009634208647669335, w1=-0.25473022117734057\n",
      "(250000,)\n",
      "Gradient Descent(2932/2999): loss=0.3899173816929251, w0=0.009634208620510263, w1=-0.25473022111098986\n",
      "(250000,)\n",
      "Gradient Descent(2933/2999): loss=0.38991738169292345, w0=0.009634208593428622, w1=-0.25473022104482834\n",
      "(250000,)\n",
      "Gradient Descent(2934/2999): loss=0.38991738169292184, w0=0.009634208566424184, w1=-0.2547302209788554\n",
      "(250000,)\n",
      "Gradient Descent(2935/2999): loss=0.38991738169292023, w0=0.009634208539496729, w1=-0.2547302209130705\n",
      "(250000,)\n",
      "Gradient Descent(2936/2999): loss=0.38991738169291856, w0=0.009634208512646037, w1=-0.2547302208474732\n",
      "(250000,)\n",
      "Gradient Descent(2937/2999): loss=0.389917381692917, w0=0.009634208485871891, w1=-0.2547302207820629\n",
      "(250000,)\n",
      "Gradient Descent(2938/2999): loss=0.38991738169291534, w0=0.009634208459174074, w1=-0.2547302207168391\n",
      "(250000,)\n",
      "Gradient Descent(2939/2999): loss=0.38991738169291373, w0=0.009634208432552365, w1=-0.2547302206518012\n",
      "(250000,)\n",
      "Gradient Descent(2940/2999): loss=0.3899173816929121, w0=0.009634208406006551, w1=-0.2547302205869487\n",
      "(250000,)\n",
      "Gradient Descent(2941/2999): loss=0.3899173816929106, w0=0.009634208379536421, w1=-0.2547302205222811\n",
      "(250000,)\n",
      "Gradient Descent(2942/2999): loss=0.38991738169290896, w0=0.009634208353141755, w1=-0.25473022045779786\n",
      "(250000,)\n",
      "Gradient Descent(2943/2999): loss=0.38991738169290746, w0=0.009634208326822328, w1=-0.25473022039349846\n",
      "(250000,)\n",
      "Gradient Descent(2944/2999): loss=0.3899173816929059, w0=0.009634208300577943, w1=-0.25473022032938236\n",
      "(250000,)\n",
      "Gradient Descent(2945/2999): loss=0.38991738169290435, w0=0.009634208274408368, w1=-0.254730220265449\n",
      "(250000,)\n",
      "Gradient Descent(2946/2999): loss=0.3899173816929028, w0=0.0096342082483134, w1=-0.25473022020169794\n",
      "(250000,)\n",
      "Gradient Descent(2947/2999): loss=0.38991738169290124, w0=0.009634208222292818, w1=-0.2547302201381286\n",
      "(250000,)\n",
      "Gradient Descent(2948/2999): loss=0.3899173816928998, w0=0.00963420819634642, w1=-0.2547302200747405\n",
      "(250000,)\n",
      "Gradient Descent(2949/2999): loss=0.38991738169289825, w0=0.009634208170473987, w1=-0.2547302200115331\n",
      "(250000,)\n",
      "Gradient Descent(2950/2999): loss=0.3899173816928967, w0=0.009634208144675317, w1=-0.2547302199485059\n",
      "(250000,)\n",
      "Gradient Descent(2951/2999): loss=0.38991738169289525, w0=0.009634208118950187, w1=-0.2547302198856584\n",
      "(250000,)\n",
      "Gradient Descent(2952/2999): loss=0.38991738169289386, w0=0.009634208093298399, w1=-0.25473021982299004\n",
      "(250000,)\n",
      "Gradient Descent(2953/2999): loss=0.38991738169289225, w0=0.009634208067719741, w1=-0.25473021976050036\n",
      "(250000,)\n",
      "Gradient Descent(2954/2999): loss=0.3899173816928908, w0=0.009634208042214, w1=-0.2547302196981888\n",
      "(250000,)\n",
      "Gradient Descent(2955/2999): loss=0.3899173816928895, w0=0.00963420801678097, w1=-0.2547302196360549\n",
      "(250000,)\n",
      "Gradient Descent(2956/2999): loss=0.389917381692888, w0=0.009634207991420448, w1=-0.25473021957409814\n",
      "(250000,)\n",
      "Gradient Descent(2957/2999): loss=0.3899173816928865, w0=0.009634207966132225, w1=-0.25473021951231795\n",
      "(250000,)\n",
      "Gradient Descent(2958/2999): loss=0.3899173816928851, w0=0.0096342079409161, w1=-0.2547302194507139\n",
      "(250000,)\n",
      "Gradient Descent(2959/2999): loss=0.38991738169288354, w0=0.009634207915771855, w1=-0.2547302193892855\n",
      "(250000,)\n",
      "Gradient Descent(2960/2999): loss=0.3899173816928822, w0=0.009634207890699294, w1=-0.2547302193280322\n",
      "(250000,)\n",
      "Gradient Descent(2961/2999): loss=0.3899173816928808, w0=0.009634207865698213, w1=-0.25473021926695355\n",
      "(250000,)\n",
      "Gradient Descent(2962/2999): loss=0.3899173816928795, w0=0.009634207840768402, w1=-0.254730219206049\n",
      "(250000,)\n",
      "Gradient Descent(2963/2999): loss=0.3899173816928781, w0=0.00963420781590966, w1=-0.25473021914531807\n",
      "(250000,)\n",
      "Gradient Descent(2964/2999): loss=0.38991738169287665, w0=0.009634207791121785, w1=-0.2547302190847603\n",
      "(250000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2965/2999): loss=0.3899173816928753, w0=0.009634207766404576, w1=-0.25473021902437515\n",
      "(250000,)\n",
      "Gradient Descent(2966/2999): loss=0.3899173816928738, w0=0.009634207741757833, w1=-0.25473021896416215\n",
      "(250000,)\n",
      "Gradient Descent(2967/2999): loss=0.3899173816928725, w0=0.009634207717181353, w1=-0.2547302189041208\n",
      "(250000,)\n",
      "Gradient Descent(2968/2999): loss=0.3899173816928712, w0=0.00963420769267493, w1=-0.2547302188442506\n",
      "(250000,)\n",
      "Gradient Descent(2969/2999): loss=0.3899173816928699, w0=0.009634207668238377, w1=-0.25473021878455115\n",
      "(250000,)\n",
      "Gradient Descent(2970/2999): loss=0.38991738169286855, w0=0.009634207643871493, w1=-0.25473021872502183\n",
      "(250000,)\n",
      "Gradient Descent(2971/2999): loss=0.38991738169286716, w0=0.009634207619574068, w1=-0.25473021866566226\n",
      "(250000,)\n",
      "Gradient Descent(2972/2999): loss=0.38991738169286577, w0=0.009634207595345905, w1=-0.2547302186064719\n",
      "(250000,)\n",
      "Gradient Descent(2973/2999): loss=0.3899173816928645, w0=0.009634207571186814, w1=-0.25473021854745026\n",
      "(250000,)\n",
      "Gradient Descent(2974/2999): loss=0.3899173816928631, w0=0.009634207547096598, w1=-0.2547302184885969\n",
      "(250000,)\n",
      "Gradient Descent(2975/2999): loss=0.38991738169286194, w0=0.009634207523075061, w1=-0.2547302184299113\n",
      "(250000,)\n",
      "Gradient Descent(2976/2999): loss=0.38991738169286055, w0=0.009634207499122003, w1=-0.254730218371393\n",
      "(250000,)\n",
      "Gradient Descent(2977/2999): loss=0.3899173816928593, w0=0.009634207475237239, w1=-0.2547302183130415\n",
      "(250000,)\n",
      "Gradient Descent(2978/2999): loss=0.3899173816928581, w0=0.009634207451420556, w1=-0.25473021825485637\n",
      "(250000,)\n",
      "Gradient Descent(2979/2999): loss=0.3899173816928568, w0=0.009634207427671772, w1=-0.2547302181968371\n",
      "(250000,)\n",
      "Gradient Descent(2980/2999): loss=0.3899173816928555, w0=0.009634207403990687, w1=-0.2547302181389833\n",
      "(250000,)\n",
      "Gradient Descent(2981/2999): loss=0.38991738169285434, w0=0.009634207380377116, w1=-0.25473021808129437\n",
      "(250000,)\n",
      "Gradient Descent(2982/2999): loss=0.389917381692853, w0=0.009634207356830866, w1=-0.25473021802376994\n",
      "(250000,)\n",
      "Gradient Descent(2983/2999): loss=0.38991738169285173, w0=0.00963420733335174, w1=-0.2547302179664095\n",
      "(250000,)\n",
      "Gradient Descent(2984/2999): loss=0.38991738169285056, w0=0.009634207309939554, w1=-0.2547302179092126\n",
      "(250000,)\n",
      "Gradient Descent(2985/2999): loss=0.3899173816928494, w0=0.009634207286594112, w1=-0.2547302178521787\n",
      "(250000,)\n",
      "Gradient Descent(2986/2999): loss=0.3899173816928481, w0=0.00963420726331522, w1=-0.2547302177953074\n",
      "(250000,)\n",
      "Gradient Descent(2987/2999): loss=0.38991738169284684, w0=0.00963420724010269, w1=-0.2547302177385983\n",
      "(250000,)\n",
      "Gradient Descent(2988/2999): loss=0.38991738169284573, w0=0.009634207216956327, w1=-0.2547302176820508\n",
      "(250000,)\n",
      "Gradient Descent(2989/2999): loss=0.3899173816928445, w0=0.009634207193875956, w1=-0.2547302176256645\n",
      "(250000,)\n",
      "Gradient Descent(2990/2999): loss=0.38991738169284323, w0=0.009634207170861378, w1=-0.254730217569439\n",
      "(250000,)\n",
      "Gradient Descent(2991/2999): loss=0.3899173816928421, w0=0.009634207147912415, w1=-0.2547302175133737\n",
      "(250000,)\n",
      "Gradient Descent(2992/2999): loss=0.38991738169284096, w0=0.009634207125028877, w1=-0.2547302174574683\n",
      "(250000,)\n",
      "Gradient Descent(2993/2999): loss=0.3899173816928398, w0=0.009634207102210584, w1=-0.2547302174017223\n",
      "(250000,)\n",
      "Gradient Descent(2994/2999): loss=0.38991738169283857, w0=0.009634207079457334, w1=-0.25473021734613516\n",
      "(250000,)\n",
      "Gradient Descent(2995/2999): loss=0.3899173816928374, w0=0.009634207056768948, w1=-0.2547302172907065\n",
      "(250000,)\n",
      "Gradient Descent(2996/2999): loss=0.38991738169283624, w0=0.009634207034145243, w1=-0.2547302172354359\n",
      "(250000,)\n",
      "Gradient Descent(2997/2999): loss=0.38991738169283513, w0=0.00963420701158603, w1=-0.2547302171803228\n",
      "(250000,)\n",
      "Gradient Descent(2998/2999): loss=0.38991738169283396, w0=0.009634206989091124, w1=-0.25473021712536686\n",
      "(250000,)\n",
      "Gradient Descent(2999/2999): loss=0.3899173816928328, w0=0.009634206966660357, w1=-0.2547302170705676\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import gradient_descent\n",
    "from costs import compute_loss\n",
    "initial_w=np.zeros((30,))\n",
    "_,w=gradient_descent(y, tX, initial_w, 3000,2*0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.01842209536994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tX.T@tX@w[999]-tX.T@y)@(tX.T@tX@w[999]-tX.T@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006108714831357409"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tX.T@tX@w[2999]-tX.T@y)@(tX.T@tX@w[1999]-tX.T@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a1a7380ac44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1499\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1499\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "(tX.T@tX@w[999]-tX.T@y)@(tX.T@tX@w[999]-tX.T@y)-(tX.T@tX@w[1499]-tX.T@y)@(tX.T@tX@w[1499]-tX.T@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": " not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c726fbe1599f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDATA_TEST_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;31m# TODO: download train data and supply path here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\ml-project-1-ml_pls2021\\project1\\scripts\\proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[1;34m(data_path, sub_sample)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1792\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m:  not found."
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
